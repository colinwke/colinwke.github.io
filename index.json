[{"categories":["tf","dl"],"content":"原理 dropout原理, 随机丢弃一些(输入)神经元, 防止参数过拟合 Applies Dropout to the input. Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. The units that are kept are scaled by 1 / (1 - rate), so that their sum is unchanged at training time and inference time. ","date":"2021-04-27","objectID":"/posts/2104271104-dropout-note.html:1:0","tags":["tf","dl"],"title":"dropout笔记","uri":"/posts/2104271104-dropout-note.html"},{"categories":["tf","dl"],"content":"核心实现 # tensorflow.python.ops.nn_ops.dropout_v2 noise_shape = _get_noise_shape(x, noise_shape) # Sample a uniform distribution on [0.0, 1.0) and select values larger than # rate. # # NOTE: Random uniform actually can only generate 2^23 floats on [1.0, 2.0) # and subtract 1.0. random_tensor = random_ops.random_uniform( noise_shape, seed=seed, dtype=x.dtype) keep_prob = 1 - rate scale = 1 / keep_prob # NOTE: if (1.0 + rate) - 1 is equal to rate, then we want to consider that # float to be selected, hence we use a \u003e= comparison. keep_mask = random_tensor \u003e= rate ret = x * scale * math_ops.cast(keep_mask, x.dtype) if not context.executing_eagerly(): ret.set_shape(x.get_shape()) return ret 这里并没有mode==training了, 返回到上上一层调用, 可以看到mode # tensorflow.python.keras.layers.core.Dropout.call def call(self, inputs, training=None): if training is None: training = K.learning_phase() def dropped_inputs(): return nn.dropout( inputs, noise_shape=self._get_noise_shape(inputs), seed=self.seed, rate=self.rate) output = tf_utils.smart_cond(training, dropped_inputs, lambda: array_ops.identity(inputs)) return output 综合上面来看, tensorflow dropout的处理方法是在训练的时候按照dropout(rate)丢弃一些神经元, 丢弃后, 然后在整体上, 又乘以一个scale( = 1 / keep_prob, keep_prob = 1 - rate), 所以在预估的时候是不需要做任何处理的. ","date":"2021-04-27","objectID":"/posts/2104271104-dropout-note.html:2:0","tags":["tf","dl"],"title":"dropout笔记","uri":"/posts/2104271104-dropout-note.html"},{"categories":["tf","dl"],"content":"问题思考 dropout是防止下层参数的过拟合还是上层参数的过拟合呢? 按照上面的分析, 可以理解为dropout随机丢弃一些输入, 因此在一定程度上防止下层参数的过拟合 每一层参数都需要dropout来防止过拟合吗? 换个角度就是每层都有可能过拟合吗? 这里需要回到dropout在集成学习上的原理, 通过随机丢弃神经元, dropout可以看做是多个网络模型的组合, 当有n个神经元的输入设置dropout=0.5时, 网络相当于有2^n种结构的集成, 因此, 当有dropout输入的神经元越多, 网络的集成度就约复杂, 越能防止过拟合, 这个还要根据实际数据情况设置 是否可以对参数进行dropout? 可以是可以, 但是对参数进行dropout后, 对于同一个batch数据, 缺失的特征列都是一样的了, 这样对模型的训练与预估势必会有一些影响 ","date":"2021-04-27","objectID":"/posts/2104271104-dropout-note.html:3:0","tags":["tf","dl"],"title":"dropout笔记","uri":"/posts/2104271104-dropout-note.html"},{"categories":["unsort"],"content":"相信未来, 拥抱未来! ","date":"2021-04-04","objectID":"/posts/2104040035-hello.html:0:0","tags":["unsort"],"title":"hello","uri":"/posts/2104040035-hello.html"},{"categories":["unsort"],"content":"多git有两种状态 多个git账号(user, email) 多个认证(identities) ","date":"2021-04-03","objectID":"/posts/old/2104031558-multi-git.html:0:0","tags":["unsort"],"title":"多git协作","uri":"/posts/old/2104031558-multi-git.html"},{"categories":["unsort"],"content":"设置多个git账号 条件: git版本号高于2.13(git –version) 编辑~/.gitconfig, 删除[user]配置块, 添加下面配置块, 注意: 路径后面要加斜杠/ [includeIf \"gitdir:~/这个路径下使用私人账号/\"] path = .gitconfig-personal [includeIf \"gitdir:~/这个路径下使用工作账号/\"] path = .gitconfig-working 然后配置刚才指定的两个配置文件 vim .gitconfg-personal [user] name = personal_name email = personal_email@personal_email.com vim .gitconfig-working [user] name = working_name email = working_name@working_name.com 然后在已配置路径下的~/这个路径下使用私人账号/或者~/这个路径下使用工作账号/执行命令, 可发现可以自动选择用户了 cd ~/这个路径下使用私人账号/ git config --get user.name git config --get user.email ","date":"2021-04-03","objectID":"/posts/old/2104031558-multi-git.html:1:0","tags":["unsort"],"title":"多git协作","uri":"/posts/old/2104031558-multi-git.html"},{"categories":["unsort"],"content":"refs https://gist.github.com/bgauduch/06a8c4ec2fec8fef6354afe94358c89e ","date":"2021-04-03","objectID":"/posts/old/2104031558-multi-git.html:2:0","tags":["unsort"],"title":"多git协作","uri":"/posts/old/2104031558-multi-git.html"},{"categories":["unsort"],"content":"用了一个很久的, 可以试一下 http://nekom.org/user 特点 简单(中文的, 有详细教程), 成本低(可以每月付, 最低10元) 点我的邀请链接会给我返利哟, 算对我的支持吧 [邀请链接] 另外一个, 有个朋友一直在用的 https://home.shadowsocks.ch/cart.php 特点 用的人挺多, 目前稳定性应该是非常好的 但是只能年付, 虽然算下来也是每月10元, 但是成本还是算高 还有一个, 另外一个朋友在用的, 需要自己搭, 我还没有试过, 后面换的话可能会试 自建ss服务器教程 https://zoomyale.com/2016/vultr_and_ss/ 科学上网-vultr-vps-搭建-shadowsocks-ss-教程 特点 设置较为复杂, 自己买服务器, 好像成本也挺高的, 但是流量较于前两个多 ","date":"2019-06-10","objectID":"/posts/old/1906101549-proxy.html:0:0","tags":["unsort"],"title":"proxy","uri":"/posts/old/1906101549-proxy.html"},{"categories":["unsort"],"content":"1 下载AutoHotKey 官网, 点我直接官网下载 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:1:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"2 编辑脚本 另存下面脚本为capslock_plus.ahk 注: 下面脚本只适用于AutoHotKey1.XXX, 不使用于2.XXX(V2和其他版本有区别!)直接下载 ; Autohotkey Capslock Remapping Script ; Colinwke ; More info at: TODO ; ; Functionality: ; - Deactivates capslock for normal (accidental) use. ; - Access the following functions when pressing Capslock: ; Cursor keys - J, K, L, I ; Home, PgDn, PgUp, End - U, O, P, ; ; Backspace and Del - Y, H ; ; Insert - [ ; Close tab, window - W, N ; Previous, next tab - M, , ; Undo, redo - ., / ; Menu - ' ; - Numpad at the right hand resting position when holding Ctrl+Shift+Alt (using keys m,.jkluio and spacebar) ; ; To use capslock as you normally would, you can press WinKey + Capslock ; This script update from Danik's work: https://gist.github.com/Danik/5808330 ;================================================================ #Persistent SetCapsLockState, AlwaysOff ;================================================================ ; Capslock + (ikjl) -\u003e (up, down, left, right) Capslock \u0026 i::Send {Blind}{Up DownTemp} Capslock \u0026 i up::Send {Blind}{Up Up} Capslock \u0026 k::Send {Blind}{Down DownTemp} Capslock \u0026 k up::Send {Blind}{Down Up} Capslock \u0026 j::Send {Blind}{Left DownTemp} Capslock \u0026 j up::Send {Blind}{Left Up} Capslock \u0026 l::Send {Blind}{Right DownTemp} Capslock \u0026 l up::Send {Blind}{Right Up} ;================================================================ ; Capslock + (uop;) -\u003e (home, end, pgup, pgdown) Capslock \u0026 u::SendInput {Blind}{Home Down} Capslock \u0026 u up::SendInput {Blind}{Home Up} Capslock \u0026 o::SendInput {Blind}{End Down} Capslock \u0026 o up::SendInput {Blind}{End Up} Capslock \u0026 `;::SendInput {Blind}{PgDn Down} Capslock \u0026 `; up::SendInput {Blind}{PgDn Up} Capslock \u0026 p::SendInput {Blind}{PgUp Down} Capslock \u0026 p up::SendInput {Blind}{PgUp Up} ;================================================================ ; Capslock + (nw) -\u003e (close tab|window) Capslock \u0026 n::SendInput {Ctrl down}{F4}{Ctrl up} Capslock \u0026 w::SendInput {Alt down}{F4}{Alt up} ;================================================================ ; Capslock + ([hy) -\u003e (insert, backspace, del) Capslock \u0026 [::SendInput {Blind}{Insert Down} Capslock \u0026 y::SendInput {Blind}{Del Down} Capslock \u0026 h::SendInput {Blind}{BS Down} Capslock \u0026 BS::SendInput {Blind}{BS Down} ;================================================================ ; Capslock + (m,) -\u003e (prev|next tab) Capslock \u0026 m::SendInput {Ctrl Down}{Tab Down} Capslock \u0026 m up::SendInput {Ctrl Up}{Tab Up} Capslock \u0026 ,::SendInput {Ctrl Down}{Shift Down}{Tab Down} Capslock \u0026 , up::SendInput {Ctrl Up}{Shift Up}{Tab Up} ;================================================================ ; Capslock + (./) -\u003e (undo, redo) Capslock \u0026 .::SendInput {Ctrl Down}{z Down} Capslock \u0026 . up::SendInput {Ctrl Up}{z Up} Capslock \u0026 /::SendInput {Ctrl Down}{y Down} Capslock \u0026 / up::SendInput {Ctrl Up}{y Up} ;================================================================ ; Capslock + ; -\u003e menu Capslock \u0026 '::Send {Blind}{AppsKey DownTemp} Capslock \u0026 ' up::Send {Blind}{AppsKey Up} ;================================================================ ; Numpad using Ctrl+Shift+Alt + m,.jkluio or space +^!Space:: SendInput {Numpad0} +^!m:: SendInput {Numpad1} +^!,:: SendInput {Numpad2} +^!.:: SendInput {Numpad3} +^!j:: SendInput {Numpad4} +^!k:: SendInput {Numpad5} +^!l:: SendInput {Numpad6} +^!u:: SendInput {Numpad7} +^!i:: SendInput {Numpad8} +^!o:: SendInput {Numpad9} ;================================================================ ; Make Capslock \u0026 Enter equivalent to Control+Enter Capslock \u0026 Enter::SendInput {Ctrl down}{Enter}{Ctrl up} ; Make Capslock \u0026 Alt Equivalent to Control+Alt !Capslock::SendInput {Ctrl down}{Alt Down} !Capslock up::SendInput {Ctrl up}{Alt up} ;================================================================ ; Make Win Key + Capslock work like Capslock (in case it's ever needed) #Capslock:: If GetKeyState(\"CapsLock\", \"T\") = 1 SetCapsLockState, AlwaysOff Else SetCapsLockState, AlwaysOn Return ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:2:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"3 修改打开方式 修改*.ahk的默认打开方式为AutoHotkeyU64.exe ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:3:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"4 运行 win+r 输入 shell:startup 回车, 把脚本拷贝至当前路径下, 使得自动运行生效 双击脚本立即生效 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:4:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"5 映射表 参考: https://gist.github.com/Danik/5808330 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:5:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"其他参考 capslock+ikjl 代替上下左右按键 ahk注释: 单行注释用分号;，多行同C语言用/* */括起。 按键列表 映射符号 如下输入 `;::LCtrl 遇到可能被误引用的字符时，前缀` 比如`\" `{ `% `` 比较好的教程, 另一个教程 另外一个神器capslock+ 初学者向导 指南和概述 脚本展示 自启动 在运行里面输入: shell:startup 打开之后把要启动的脚本放进去即可 热键, 组合键 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:6:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"几个capslock wsas代替上下左右 hjkl代替上下左右 ijkl代替上下左右, 推荐 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:7:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["reading"],"content":"2019-5-12 最近计划学习一下深度学习框架, kaggle是个不错的平台, 就找了其中的比赛Jigsaw Unintended Bias in Toxicity Classification. 在比赛的第四段, 描述了比赛的背景, 和技术中存在的问题: Here’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. “gay”), even when those comments were not actually toxic (such as “I am a gay woman”). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users. 谷歌翻译: 以下是背景：当对话AI团队首次构建毒性模型时，他们发现模型错误地学会将频繁攻击身份的名称与毒性联系起来。模型预测含有这些身份（例如“同性恋”）的评论很可能具有毒性，即使这些评论实际上并不具有毒性（例如“我是同性恋女性”）。发生这种情况是因为训练数据是从可用来源中提取的，不幸的是，某些身份以令人反感的方式被压倒性地提及。从具有这些不平衡的数据中训练模型可能会将这些偏差反映回用户。 简单说来就是大样本覆盖小样本的问题, 也是高频扰乱低频的问题, 这个问题在大多数场景都是存在的, 例如在淘宝购物中, 食品类的产品买了又买, 且一次也可以买多件, 但是对于电器类的产品, 一次一般只买一件, 且买了后很长一段时间都不会再买. 通常包含了这两个类品的数据构建模型时, 高频的数据就会对低频的数据造成影响. 这从另一个方面也反应了模型的偏差和方差问题, 也是过拟合和欠拟合问题, 对于\"电器\"和\"食品\"的分类, 可以考虑分开建模的方法, 即食品单独建一个模型, 电器单独建一个模型, 或者设置加权的损失函数. 似乎电器和食品的例子和这个比赛也有不同, 这个比赛是关于一个词语的偏差, 可能在比赛中还是更应该注意偏差和方差, 还有评价指标的设定(召回还是精确). ","date":"2019-05-12","objectID":"/posts/old/1905121610-reading-may.html:1:0","tags":["reading"],"title":"五月阅读","uri":"/posts/old/1905121610-reading-may.html"},{"categories":["unsort"],"content":"wondows平台, pip安装MeCab: pip install mecab-python3 出现问题: 'mecab-config' 不是内部或外部命令，也不是可运行的程序或批处理文件。 在网上找了一些资料, 一些日文资料写的云里雾里的, 比如这篇Windows環境でのMeCab(Python)のインストール(没有必要打开). 然后找到一篇中文的windows10+py36+MeCab安装总结, 按照步骤安装成功! ===我是分割线=== 上文的有些步骤可以精简, 具体步骤可以如下: 1 在pip安装之前, 需要先安装MeCab.exe 懒得麻烦, 点我直接下载 2 安装, 中间有个选择字典编码, 选择UTF-8 3 安装python依赖包 首先, 需要把MeCab安装路径下的./bin/libmecab.dll和./sdk/libmecab.lib拷贝到python的./Lib/site-packages下面. 然后, 就可以使用pip install mecab-python-windows安装依赖包了. 4 测试 import MeCab mecab = MeCab.Tagger(\"-Ochasen\") # `-Owakati` 只做分词 print(mecab.parse(\"pythonが大好きです\")) 参考: https://segmentfault.com/q/1010000015969023 https://blog.csdn.net/ZYXpaidaxing/article/details/81913708 https://www.jianshu.com/p/8f0ce2cff8d9 http://www.flickering.cn/nlp/2014/06/%E6%97%A5%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8-mecab-%E6%96%87%E6%A1%A3/ ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:0:0","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["unsort"],"content":"MeCab使用 # date: 2019/3/21 10:53 # author: wang ke # concat: ke.wang@ctrip.com # ================================ \"\"\"MeCab test. --- MeCab output format(http://taku910.github.io/mecab/): -Owakati: only segment python が 大好き です -Ochasen: ChaSen compatible(segment and tag?) python python python 名詞-固有名詞-組織 が ガ が 助詞-格助詞-一般 大好き ダイスキ 大好き 名詞-形容動詞語幹 です デス です 助動詞 特殊・デス 基本形 EOS -Oyomi: Yomi given(only translate?) pythonガダイスキデス -Odump: all information output 0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0 3 python 名詞,固有名詞,組織,*,*,*,* 0 6 1292 1292 45 5 1 1 0.000000 0.000000 0.000000 12857 8 が 助詞,格助詞,一般,*,*,*,が,ガ,ガ 6 9 148 148 13 6 0 1 0.000000 0.000000 0.000000 11729 21 大好き 名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ 9 18 1287 1287 40 2 0 1 0.000000 0.000000 0.000000 13008 48 です 助動詞,*,*,*,特殊・デス,基本形,です,デス,デス 18 24 460 460 25 6 0 1 0.000000 0.000000 0.000000 12875 56 EOS BOS/EOS,*,*,*,*,*,*,*,* 24 24 0 0 0 0 3 1 0.000000 0.000000 0.000000 11634 \"\": python 名詞,固有名詞,組織,*,*,*,* が 助詞,格助詞,一般,*,*,*,が,ガ,ガ 大好き 名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ です 助動詞,*,*,*,特殊・デス,基本形,です,デス,デス EOS \"\"\" import MeCab def format_tag_result(x): pieces = [] for i in x.splitlines()[:-1]: i = i.split() v = (i[0], i[-1]) pieces.append(v) return pieces mecab_tagger = MeCab.Tagger(\"-Ochasen\") # `-Owakati` 只做分词 text = \"pythonが大好きです\" print(format_tag_result(mecab_tagger.parse(text))) ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:1:0","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["unsort"],"content":"mecab ipadic字典提取 因为ipadic字典是用csv保存的, 因此还是很好提取的. 需要注意的是, 在打开csv时, 选择的文本编码是shift-jis. 比如说提取地点Noun.place.csv ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:1:1","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["unsort"],"content":"wikipedia dictionary download(日本语) https://www.zhihu.com/question/19803440 https://zh.wikipedia.org/wiki/Wikipedia:%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD google wikipedia dictionary 日本語ダウンロード Wikipedia:データベースダウンロード 下面是翻译成中文的 日文: https://dumps.wikimedia.org/jawiki/latest/ 中文: https://dumps.wikimedia.org/chwiki/latest/ 随便下载一个: chwiki-latest-geo_tags.sql 注意, 这是sql文件, 即为可执行的文件, 包含创建数据库和插入数据, 并不是数据库文件, 这个可以直接使用文本编辑软件打开 但是里面好像并没有什么… 中文搞错了, 应该是这个: https://dumps.wikimedia.org/zhwiki/latest/ ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:2:0","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["win"],"content":"保存win10 spotlight 壁纸到D盘下! 1 将下面文本保存为win10_spotlight.bat(直接下载) rem https://blog.csdn.net/qq_34260368/article/details/78364055 rem https://blog.csdn.net/linbounconstraint/article/details/80191846 rem https://blog.csdn.net/Anymake_ren/article/details/51125609 rem https://stackoverflow.com/questions/17587347/batch-file-to-run-xcopy-without-overwriting-existing-files @echo off MD wallpaper xcopy \"%UserProfile%\\AppData\\Local\\Packages\\Microsoft.Windows.ContentDeliveryManager_cw5n1h2txyewy\\LocalState\\Assets\" \"D:\\spotlight\\\" /S /Y /D D: cd \"D:\\spotlight\\\" ren * *.jpg pause 2 双击执行, 执行后前往D:\\spotlight\\查看 参考: https://blog.csdn.net/qq_34260368/article/details/78364055 https://blog.csdn.net/linbounconstraint/article/details/80191846 https://blog.csdn.net/Anymake_ren/article/details/51125609 https://stackoverflow.com/questions/17587347/batch-file-to-run-xcopy-without-overwriting-existing-files ","date":"2018-12-29","objectID":"/posts/old/dabble/save-win10-spotlight.html:0:0","tags":["win"],"title":"save-win10-spotlight","uri":"/posts/old/dabble/save-win10-spotlight.html"},{"categories":["unsort"],"content":"先看一个DNN, CNN, RNN的比较 深度 输入 参数 输出 DNN 神经元堆叠层数 向量 连接 向量 RNN 序列长度 向量 连接 向量 CNN 一组操作(卷积, 池化) 特征图 连接, 卷积核 特征图 Q:CNN中原图和卷积生成的特征图的位置是对应的吗? A: 是对应的. 因为卷积(或池化)生成的特征图一直都是一一对应(这也是局部关联的实现). 如果最后提取的特征图为4个神经元的正方形, 那么这四个神经元是对应的, 如果最后提取的特征图为一个神经元的输出, 那么, 这个神经元的信息已经包含了整张图片的信息. ","date":"2018-12-28","objectID":"/posts/old/deep-learning/cnn.html:0:0","tags":["unsort"],"title":"cnn","uri":"/posts/old/deep-learning/cnn.html"},{"categories":["unsort"],"content":" 转至: [Link] CNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数。总之，卷积网络的核心思想是将：局部感受野、权值共享（或者权值复制）以及时间或空间亚采样这三种结构思想结合起来获得了某种程度的位移、尺度、形变不变性。 ​ 下图左：如果我们有1000x1000像素的图像，有1百万个隐层神经元，那么他们全连接的话（每个隐层神经元都连接图像的每一个像素点），就有1000x1000x1000000=10^12个连接，也就是10^12个权值参数。然而图像的空间联系是局部的，就像人是通过一个局部的感受野去感受外界图像一样，每一个神经元都不需要对全局图像做感受，每个神经元只感受局部的图像区域，然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。这样，我们就可以减少连接的数目，也就是减少神经网络需要训练的权值参数的个数了。如下图右：假如局部感受野是10x10，隐层每个感受野只需要和这10x10的局部图像相连接，所以1百万个隐层神经元就只有一亿个连接，即10^8个参数。比原来减少了四个0（数量级），这样训练起来就没那么费力了，但还是感觉很多的啊，那还有啥办法没？ ​ 我们知道，隐含层的每一个神经元都连接10x10个图像区域，也就是说每一个神经元存在10x10=100个连接权值参数。那如果我们每个神经元这100个参数是相同的呢？也就是说每个神经元用的是同一个卷积核去卷积图像。这样我们就只有多少个参数？？只有100个参数啊！不管你隐层的神经元个数有多少，两层间的连接我只有100个参数啊！这就是权值共享。 好了，你就会想，这样提取特征也忒不靠谱吧，这样你只提取了一种特征啊？对了，真聪明，我们需要提取多种特征对不？假如一种滤波器，也就是一种卷积核就是提出图像的一种特征，例如某个方向的边缘。那么我们需要提取不同的特征，怎么办，加多几种滤波器不就行了吗？对了。所以假设我们加到100种滤波器，每种滤波器的参数不一样，表示它提出输入图像的不同特征，例如不同的边缘。这样每种滤波器去卷积图像就得到对图像的不同特征的放映，我们称之为Feature Map。所以100种卷积核就有100个Feature Map。这100个Feature Map就组成了一层神经元。到这个时候明了了吧。我们这一层有多少个参数了？100种卷积核x每种卷积核共享100个参数=100x100=10K，也就是1万个参数。才1万个参数。见下图右：不同的颜色表达不同的滤波器。 ​ 嘿哟，遗漏一个问题了。刚才说隐层的参数个数和隐层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。那么隐层的神经元个数怎么确定呢？它和原图像，也就是输入的大小（神经元个数）、滤波器的大小和滤波器在图像中的滑动步长都有关！例如，我的图像是1000x1000像素，而滤波器大小是10x10，假设滤波器没有重叠，也就是步长为10，这样隐层的神经元个数就是(1000x1000 )/ (10x10)=100x100个神经元了，假设步长是8，也就是卷积核会重叠两个像素，那么……我就不算了，思想懂了就好。注意了，这只是一种滤波器，也就是一个Feature Map的神经元个数哦，如果100个Feature Map就是100倍了。由此可见，图像越大，神经元个数和需要训练的权值参数个数的贫富差距就越大。 feature map计算方法： 在CNN网络中roi从原图映射到feature map中的计算方法 INPUT为3232，filter的大小即kernel size为55，stride = 1，pading=0,卷积后得到的feature maps边长的计算公式是： output_h =（originalSize_h+padding*2-kernelSize_h）/stride +1 所以，卷积层的feature map的变长为：conv1_h=（32-5）/1 + 1 = 28 卷积层的feature maps尺寸为28*28. 由于同一feature map共享权值，所以总共有6*（5*5+1）=156个参数。 卷积层之后是pooling层，也叫下采样层或子采样层（subsampling）。它是利用图像局部相关性的原理，对图像进行子抽样，这样在保留有用信息的同时可以减少数据处理量。pooling层不会减少feature maps的数量，只会缩减其尺寸。常用的pooling方法有两种，一种是取最大值，一种是取平均值。 pooling过程是非重叠的，S2中的每个点对应C1中22的区域（也叫感受野），也就是说kernelSize=2，stride=2，所以pool1_h = (onv1_h - kernelSize_h)/stride +1 = (28-2)/2+1=14。pooling后的feature map尺寸为1414. fast rcnn以及faster rcnn做检测任务的时候，涉及到从图像的roi区域到feature map中roi的映射，然后再进行roi_pooling之类的操作。 ​ 比如图像的大小是（600,800），在经过一系列的卷积以及pooling操作之后在某一个层中得到的feature map大小是（38,50），那么在原图中roi是（30,40,200,400）， 在feature map中对应的roi区域应该是 roi_start_w = round(30 * spatial_scale); roi_start_h = round(40 * spatial_scale); roi_end_w = round(200 * spatial_scale); roi_end_h = round(400 * spatial_scale); 其中spatial_scale的计算方式是spatial_scale=round(38/600)=round(50/800)=0.0625，所以在feature map中的roi区域[roi_start_w,roi_start_h,roi_end_w,roi_end_h]=[2,3,13,25]; 具体的代码可以参见caffe中roi_pooling_layer.cpp ","date":"2018-12-26","objectID":"/posts/old/deep-learning/2018-12-26-cnn-feature-map.html:0:0","tags":["unsort"],"title":"CNN-feature-map","uri":"/posts/old/deep-learning/2018-12-26-cnn-feature-map.html"},{"categories":["dl"],"content":"损失函数 在GAN中, Adversarial Model的功能是判别样本是否来自于Generative Model. 而Generative Model的目标是最大化的混淆Adversarial Model. 判别模型的目标函数 $$ \\max_{ D } E_{ x \\sim P_{ r } } [ \\log D ( x ) ] + E_{ x \\sim P_{ g } } [ \\log ( 1 - D ( x ) ] $$ 这难道不是最大化判别模型交叉熵损失吗? 但是, 当最大化交叉熵损失的时, 也就是全部样本分错的情况, 也不能混淆对抗模型呢? 事实上, GAN的目标函数就是交叉熵损失! $$ H ( p , q ) = - \\sum _ { i } p _ { i } \\log q _ { i } = - y \\log \\hat { y } - ( 1 - y ) \\log ( 1 - \\hat { y } ) $$ $x \\sim P _ { r }$ 对应着$y$, 对应着$1 - y$ 最小化交叉熵损失, 就是最大化对抗损失的负数 再来看看GAN的目标函数 D and G play the following two-player minimax game with value function $V (G; D)$ $$ \\min _ { G } \\max _ { D } V ( D , G ) = \\mathbb { E } _ { \\boldsymbol { x } \\sim p _ { \\text { data } } ( \\boldsymbol { x } ) } [ \\log D ( \\boldsymbol { x } ) ] + \\mathbb { E } _ { \\boldsymbol { z } \\sim p _ { \\boldsymbol { z } } ( \\boldsymbol { z } ) } [ \\log ( 1 - D ( G ( \\boldsymbol { z } ) ) ) ] $$ 为了表现出交叉熵的形式, 可以变形为 $$ \\max _ { G } \\min _ { D } V ( D , G ) = - \\mathbb { E } _ { \\boldsymbol { x } \\sim p _ { \\text { data } } ( \\boldsymbol { x } ) } [ \\log D ( \\boldsymbol { x } ) ] - \\mathbb { E } _ { \\boldsymbol { z } \\sim p _ { \\boldsymbol { z } } ( \\boldsymbol { z } ) } [ \\log ( 1 - D ( G ( \\boldsymbol { z } ) ) ) ] $$ 当更新G的时候, 使该目标函数最大! 也就是G尽量的混淆D 当更新D的时候, 使该目标函数最小! 也就是D尽量的判别G ","date":"2018-12-21","objectID":"/posts/old/2018-12-21-adversarial-model.html:1:0","tags":["gan"],"title":"adversarial-model","uri":"/posts/old/2018-12-21-adversarial-model.html"},{"categories":["dl"],"content":"最大化混淆和最大化损失 当更新G的时候, 需要最大化损失函数 但是, 不是全部分错的时候损失函数函数最大吗? 但是这个时候也不能混淆D呀! 这个时候, 因为只考虑了G, 而没有考虑D! 结合目标函数的交叉熵损失形式 当更新G时, 固定D, G的目的是最大化分错, 因此我们需要最大化交叉熵损失 当更新D时, 固定G, D的目的是最大化辨别, 因此我们需要最小化交叉熵损失 然后这两个模型就开始推太极了(two-player-game), 对于一个生成的样本, 其真实标记为1 当更新G时, 固定D, G的目的是最大化分错, 让D预测的标记趋近于0 当更新D时, 固定G, D的目的是最大化辨别, 让D预测的标记趋近于1 然后两兄弟开始推太极, 最后收敛到了0.5 ","date":"2018-12-21","objectID":"/posts/old/2018-12-21-adversarial-model.html:2:0","tags":["gan"],"title":"adversarial-model","uri":"/posts/old/2018-12-21-adversarial-model.html"},{"categories":["dl"],"content":"refs https://blog.csdn.net/u010089444/article/details/78946039 ","date":"2018-12-21","objectID":"/posts/old/2018-12-21-adversarial-model.html:3:0","tags":["gan"],"title":"adversarial-model","uri":"/posts/old/2018-12-21-adversarial-model.html"},{"categories":["ml"],"content":"在迁移学习中, 领域判别损失如下: 咋一看还看不懂了, 交叉熵损失也就是logloss不是这个样子的吗: $$ H(x)\\ =\\ -\\sum_{i}p_{i}\\log q_{i}\\ =\\ -y\\log{\\hat{y}}-(1-y)\\log(1-{\\hat{y}}) $$ 其实也是啊, 可以从两个角度进行解释: 首先 在Logistic Regression的公式推导中, 是使用的最大似然的对数取反来作为的损失, 也就是负的极大似然的对数. 其次 可以从熵的角度进行解释, 对于二分类任务来讲, 真实标记为类别, 预测的结果为概率, 因此为两个类别预测的熵. ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:0:0","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"交叉熵损失是熵吗? 显然不是, 熵可没有真实的标记! 交叉熵损失可以衡量两个分布的距离, 在二分类中, 一个分布为预测的概率, 一个分布为真实的标记. ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:1:0","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"这篇文章写得不错! ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:0","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"信息量 一个事件x的信息量是： $$ I(x)=-log(p(x)) $$ 解读：如果一个事件发生的概率越大，那么信息量就越小。如果是1，也就是100%发生，那么信息量为0。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:1","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"熵 是对信息量求期望值。 $$ H(X)=E[I(x)]=-\\sum\\limits_{x∈X}p(x)\\log p(x) $$ 举例： 如果10次考试9次不及格，一次及格。 假设事件为xAxA代表及格事件，那么这个事件的熵为： $$ H_A(x)=-[p(x_A)\\log(p(x_A))+(1-p(x_A))\\log(1-p(x_A))]=0.4690 $$ 其实也和后续的逻辑回归的二分类的损失函数有类似。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:2","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"KL散度 相对熵(relative entropy)又称为KL散度（Kullback-Leibler divergence），KL距离，是两个随机分布间距离的度量。记为DKL(p||q)。它度量当真实分布为p时，假设分布q的无效性。 $$ \\displaylines{ \\begin{align} D_{KL}(p||q)\u0026=E_p[\\log \\frac{p(x)}{q(x)}]=\\sum\\limits_{x∈\\mathcal{X}} p(x)\\log \\frac{p(x)}{q(x)} \\\\ \u0026= \\sum\\limits_{x∈\\mathcal{X}} [p(x)\\log p(x)-p(x)\\log q(x)]\\\\ \u0026= -H(p)-\\sum\\limits_{x∈\\mathcal{X}} p(x)\\log q(x)\\\\ \u0026= -H(p)+E_p[-\\log q(x)] \\end{align} } $$ 当p=q的时候，散度为0. ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:3","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"交叉熵 假设有两个分布p，q，则它们在给定样本集上的交叉熵定义如下： $$ \\begin{align*} CrossEntropy(p,q)\u0026=E_p[-\\log q]\\\\ \u0026=-\\sum\\limits_{x∈\\mathcal{X}} p(x)\\log q(x)\\\\ \u0026=H(p)+D_{KL}(p||q) \\end{align*} $$ 当p分布是已知，则熵是常量；于是交叉熵和KL散度则是等价的。最小化交叉熵等价于最小化KL距离。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:4","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"对应到logistic regression 在逻辑回归中我们用交叉熵来定义损失函数的。那么来再推导一次。详细参考： https://blog.csdn.net/iterate7/article/details/78992027 p:真实样本分布，服从参数为p的0-1分布，即X~B(1,p) q:待估计的模型，服从参数为q的0-1分布，即X~B(1,q) 0-1分布，我们把其中一种事件的结果发生的概率定为p，那么另一种结果的概率就是1-p，两者的概率和是1.[贝努力分布] $$ \\begin{align*} CrossEntropy(p,q)\u0026=-\\sum\\limits_{x∈\\mathcal{X}} \\textbf{p(x)}\\log \\textbf{q(x)} \\\\ \u0026=-[P_p(x=1)\\log P_q(x=1)+P_p(x=0)\\log P_q(x=0)]\\\\ \u0026=-[p\\log q+(1-p)\\log (1-q)]\\\\ \u0026=-[\\textbf{y}\\log \\textbf{h}_{\\theta}(x)+(1-\\textbf{y})\\log (1-\\textbf{h}_{\\theta}(x))] \\\\ \\end{align*} $$ 这里q则是假设函数。 对所有的训练样本平均值交叉熵为： $$ J = -\\frac{1}{m}\\sum\\limits_{i=1}^m[y^{(i)}\\log h_{\\theta}(x^{(i)})+(1-y^{(i)})\\log (1-h_{\\theta}(x^{(i)}))] $$ ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:5","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"总结 信息量代表的是一种不确定性； 熵代表的是不确定性的期望值； 确定的事件的熵为0； KL散度代表的是利用熵的概念来表示分布之间的距离； 交叉熵等价于KL散度；熵是常量，因为训练数据的分布已知。 在逻辑回归中用交叉熵作为损失函数的原因是：交叉熵可以等价于KL散度；交叉熵越小，则p和q分布差异越小，拟合更好。 用最大似然方法推导的损失函数和最大熵的方式结果是一致的，最大似然方法的推导可以参考：https://blog.csdn.net/iterate7/article/details/78992027 实际中，选用交叉熵易于计算。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:6","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"引用 https://blog.csdn.net/iterate7/article/details/78992027 https://en.wikipedia.org/wiki/Cross_entropy https://www.zhihu.com/question/65288314/answer/244601417 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:7","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["感受"],"content":"一 本科考研的时候, 我可能看上去比别人更努力一些. 为什么? 因为我知道我基础并不好, 所以我需要更多的努力. 俗话说, 笨鸟先飞, 我深深的铭记这句话. 每当他人问我为什么这么努力时, 我会说因为底子差, 比较笨, 所以需要更多努力, 笨鸟先飞嘛. 那个时候是一种笃定. ","date":"2018-11-04","objectID":"/posts/old/essay/stay-foolish.html:1:0","tags":["感受"],"title":"stay-foolish","uri":"/posts/old/essay/stay-foolish.html"},{"categories":["感受"],"content":"二 时过境迁. 现在也会遇到一些调侃: 你为何这么努力, 却还是一副窘境. 其实这真怪我, 也是我较为严重的一个缺点: 不喜欢逼自己, 好安逸, 好自由. 来了实验室也是刷刷微博, 翻翻知乎, 看看直播吧. 完全一副作死的样子. 我非常感谢周围能有一些非常优秀的朋友, 他们是我学习的榜样. 有天赋出众的, 有后期奋进的. 比如其中有一位基本不来实验室, 但是却通宵达旦的学习. 他可能还会表示一天都在宿舍玩游戏, 看动漫, 好像一天都在玩一样, 但是你这样想可能就错了, 他付出的努力可能远远超过你的想象, 或许他正在\"偷偷\"的学习, 这样说只是他心中的一个平衡罢了. 但是当他付出的努力取得成果时, 他会有一种优越感, 而你却往往只看到了一个结果, 而没有认识到努力的过程. 也可能是别人本就天赋出众, 这也是你未能具有的, 所以你只有付出更多的努力. 如今的我受到调侃却变得不那么的淡定, 内心波涛涌动. 认清自己才最为重要, 笨鸟先飞, 是一种态度, 也需要笃定. ","date":"2018-11-04","objectID":"/posts/old/essay/stay-foolish.html:2:0","tags":["感受"],"title":"stay-foolish","uri":"/posts/old/essay/stay-foolish.html"},{"categories":["paper"],"content":"2018-11-5 关于迁移学习, 领域适应, 对抗学习的论文, 代码汇总 zhaoxin94 / awsome-domain-adaptation jindongwang / transferlearning artix41 / awesome-transfer-learning zhaoxin94 / awsome-domain-adaptation barebell / DA zhangqianhui / AdversarialNetsPapers ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:1:0","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"2019-1-3 update https://xiaosean.github.io/posts/ 台湾一名学生的主页, 主要包含一些阅读笔记, 包含的领域有领域适应, 生成对抗网络等. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:2:0","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"迁移学习学者调研 参考jindongwang: transfer learning scholars, 对相关学者目前的状态做一个整理 ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:0","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Qiang Yang 杨强老师貌似不怎么做一线的研究了, 主页上没有什么新文章, 最后更新的也都是2016年的文章. Google Scholar有一些较新的文章, 关于迁移学习的文章比例大约为1/5, 且大多是第三作者. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:1","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Derek Hao Hu 这位学者已经不活跃了, 最近的论文也是2013年的. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:2","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Vincent W. Zheng 这位学者还战斗在一线, 但迁移学习的文章较少. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:3","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Sinno Jialin Pan 杨强老师的学生, 现在为老师, 比较年轻, 在2017年有部分迁移学习的文章. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:4","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Lixin Duan 2012年发表过关于迁移学习的文章, 现在在亚马逊任职, 主要做视觉方面的研究. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:5","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Mingsheng Long 挺年轻的一位学者, 坚持在迁移学习的一线. 推荐! ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:6","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Judy Hoffman 16, 17年有几篇迁移学习相关的论文, 主要做Domain Adaption, 包含对抗学习. 推荐! ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:7","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Fuzhen Zhuang 庄福振 主要做Multi-Task的. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:8","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["dl"],"content":"深入浅出：GAN原理与应用入门介绍 是一类在无监督学习中使用的神经网络 致力于通过学习恒等函数 f（x）= x 从数据中提取特征，且都依赖马尔可夫链来训练或生成样本。 相似的无监督学习方法还包含 玻尔兹曼机（Geoffrey Hinton 和 Terry Sejnowski，1985） 自动解码器（Dana H. Ballard，1987） ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:1:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"什么是GAN 想要学习生成器的分布，应该定义数据 $x$ 的参数 $p_g$，以及输入噪声变量 $p_z（z）$的分布。然后 $G（z，θ_g）$将 $z$ 从潜在空间 $Z$ 映射到数据空间，$D（x，θ_d）$输出单个标量——一个 $x$ 来自真实数据而不是 $p_g$ 的概率。 训练判别器以最大化正确标注实际数据和生成样本的概率。训练生成器用于最小化 $log（1-D（G（z）））$。换句话说，尽量减少判别器得出正确答案的概率。 可以将这样的训练任务看作具有值函数 $V（G，D）$的极大极小博弈： 换句话说，生成器努力生成判别器难以辨认的图像，判别器也愈加聪明，以免被生成器欺骗。 「对抗训练是继切片面包之后最酷的事情。」- Yann LeCun 当判别器不能区分 $p_g$ 和 $p_data$，即 $D（x，θ_d）= 1/2$ 时，训练过程停止。达成生成器与判别器之间判定误差的平衡。 ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:2:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"例如 我们应该获得每个标记的特征表示，但是应用常规机器学习和深度学习方法（包括卷积神经网络）存在一些问题： 它们需要大量标注图像； 商标没有标注； 标记无法从数据集分割出去。 这种新方法显示了如何使用 GAN 从商标的图像中提取和学习特征。在学习每个标记的表征之后，就可以在扫描文档上按图形搜索。 ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:3:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"生成模型具有模拟真实数据样本的性能 ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:4:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"tutorial 地址: pytorch: Training a Classifier. 当使用新的数据集进行测试时, 出现的问题及解决的方法. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:0:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 1 error: RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 484 and 549 in dimension 2 at /pytorch/aten/src/TH/generic/THTensorMath.cpp:3616 location: images, labels = data_iter.next() solution: 数据集中的图像大小不一致. 需要使用`transforms.Resize([height, width])`把所有图像缩放到同一大小. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:1:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 2 error: RuntimeError: invalid argument 2: size '[-1 x 400]' is invalid for input with 719104 elements at /pytorch/aten/src/TH/THStorage.cpp:80 location: x = x.view(-1, 16 * 5 * 5) solution: `Tensor.view()` 相当于 `numpy.reshape()` 方法, 即重塑形状. 其中`-1`表示依据其他维度进行推理得出的维度. 这里的参数需要计算得出, 不同的输入尺寸需要计算对应的参数! --- 我们来计算一下, 计算公式见下面的图片. --- input size = 3*32*32 - class Net_t1(nn.Module): def __init__(self): super(Net_t1, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = self.pool(x) x = F.relu(self.conv2(x)) x = self.pool(x) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x - 1 torch.Size([4, 6, 28, 28]), conv1, (32 - 5) / 1 + 1 = 28, padding=0 2 torch.Size([4, 6, 14, 14]), pool, 28 / 2 = 14 3 torch.Size([4, 16, 10, 10]), conv2, (14 - 5) / 1 + 1 = 10, padding=0 4 torch.Size([4, 16, 5, 5]), pool, 10 / 2 = 5 5 torch.Size([4, 400]), view, 16 * 5 * 5 = 400 6 torch.Size([4, 120]), full_connect 7 torch.Size([4, 84]), full_connect 8 torch.Size([4, 10]), full_connect --- input size = 3*224*224 - class Net_t2(nn.Module): def __init__(self): super(Net_t2, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 53 * 53, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 31) def forward(self, x): x = F.relu(self.conv1(x)) x = self.pool(x) x = F.relu(self.conv2(x)) x = self.pool(x) x = x.view(-1, 16 * 53 * 53) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x - 1 torch.Size([4, 6, 220, 220]), conv1, (224 - 5) / 1 + 1 = 220, padding=0 2 torch.Size([4, 6, 110, 110]), pool, 220 / 2 = 110 3 torch.Size([4, 16, 106, 106]), conv2, (110 - 5) / 1 + 1 = 106, padding=0 4 torch.Size([4, 16, 53, 53]), pool, 106 / 2 = 53 5 torch.Size([4, 44944]), view, 16 * 53 * 53 = 44944 6 torch.Size([4, 120]), full_connect 7 torch.Size([4, 84]), full_connect 8 torch.Size([4, 31]), full_connect ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:2:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 3 error: RuntimeError: Assertion `cur_target \u003e= 0 \u0026\u0026 cur_target \u003c n_classes' failed. at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93 location: loss = criterion(outputs, labels) solution: 预测的标签向量和实际的标签向量维度不一致! 设置输出层(最后一层)神经元个数为真实的标签个数. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:3:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 4 error: RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight' location: net = Net().to(device) solution: 在做计算时, 需要把所有需要计算的量都放在`device`上面. 因此不仅网络需要放在`device上面`, `inputs`和`labels`也要放在`device`上面. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:4:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"其他 torchvision.datasets.ImageFolder()会自动加载标签信息. 可以通过上述语句返回的对象调用len(dataset)返回样本个数, 调用dataset.classes返回标签集合. 2018-10-30 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:5:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"pytorch中的可训练性设置 在代码中看到两种设置 # method 1 for param in base_network.parameters(): param.requires_grad = False # method 2 base_network.train(False) 字面意思都是不训练base_network, 但是两个训练的结果不同. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"requires_grad 是pytorch中变量自动求导的一个属性[AUTOGRAD MECHANICS]. 当设置为False时, 反向传播时不使用梯度更新变量. 他的作用是用来冻结模型中的部分(freeze part of your model). ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:1","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Module.train(mode) pytorch doc: Module.train(mode) 针对于特有模型的特有表现, 比如Dropout, BathNorm等模型中, 不是需要梯度更新的参数(Dropout: mean, std). Even the parameters are the same, it doesn’t mean the inferences are the same. For dropout, when train(True), it does dropout; when train(False) it doesn’t do dropout (identitical output). And for batchnorm, train(True) uses batch mean and batch var; and train(False) use running mean and running var. [link] For dropout (there’s even no parameter in dropout), the dropout position is changing when train is True. For BatchNorm, the train(True) will use the batch norm instead of running_mean and running_var and also running_mean and running_var will also change. [link] A layer doesn’t have requires_grad, only Variables have. running_mean and running_var are buffers, and are updated during forwarding. I assume train(True) will still use the batch mean and batch var. [link] ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:2","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"如何固定预训练的ResNet I am wondering whether to set .eval() for those frozen layers since it may still update its running mean and running var. [link] Setting .requires_grad = False should work for convolution and FC layers. But how about networks that have instanceNormalization? Is setting .requires_grad = False enough for normalization layers too? [link] 当需要固定要预训练的ResNet, 相当于只做预测任务. 因此只需把模型的状态设置为.eval()即可. for k in range(200): # Make a prediction based on the current network weights net.train() # Set to training mode pred_tr = net.forward(z_tr) # Pass in input loss_tr = lossfn(pred_tr, y_tr) # Compute error between prediction and target # Optimize optimizer.zero_grad() # zero the gradient buffers loss_tr.backward() # Run a backward pass through the network optimizer.step() # Update your network parameters # Display loss \u0026 results on test data net.eval() # Set to eval mode pred_te = net.forward(z_te) loss_te = lossfn(pred_te, y_te) # Compute error between prediction and target print('Iter: {}, Training loss: {}, Test loss: {}'.format(k, loss_tr.data[0], loss_te.data[0])) # ref: https://courses.cs.washington.edu/courses/cse490r/18wi/lecture_slides/02_16/pytorch-tutorial.py refs: https://stackoverflow.com/a/48270921/6494418 https://stackoverflow.com/questions/50233272/pytorch-forward-pass-changes-each-time https://discuss.pytorch.org/t/batchnorm-eval-cause-worst-result/15948/6 https://github.com/bethgelab/foolbox/issues/74 https://courses.cs.washington.edu/courses/cse490r/18wi/lecture_slides/02_16/pytorch-tutorial.py pytorch save model 2018-10-31 当数据为图片时, 并且图片的标签是按照文件夹表示的, 使用torchvision.datasets.ImageFolder()读取数据后, 使用torch.utils.data.DataLoader()配置数据时, 一定要加入参数shuffle=True, 不然网络无法训练! 因为一个批量数据中可能就只有一个类别, 无法反向传播, 致使参数不下降, 或者为nan. 若使用GPU进行训练, 在读取DataLoader时, 把数据加载到GPU, 而不是在iteration时加入GPU, 将大大提升运行时间! 2018-11-13 tensor() 是不能直接和int, 等非tensor类型计算的, 计算结果会成0 2018-11-16 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:3","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"官网教程 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"pytorch是什么? 基于python的科学计算工具包: 基于GPU计算的numpy的替代物 深度学习研究平台 tensor 就是numpy的ndarray, 不同之处在于基于GPU的tensor能加速计算. torch.Tensor is the central class of the package. If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call .backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad attribute. Tensor是核心的数据结构 .requires_grad用来追踪Tensor是否需要计算每个算子的梯度 .backward()用来计算梯度 function Tensor和Function是相互联系的, 构成了一个非循环图, 它编码了完整的计算历史. autograd pytorch中所有神经网络的核心是autograd. gradient 反向传播(backprop)阶段, 损失是一个标量(scalar) 因为损失函数也是计算图中的一部分(最上层部分), 然后通过梯度分布在各个label上 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:1","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"neural networks A typical training procedure for a neural network is as follows: Define the neural network that has some learnable parameters (or weights) Iterate over a dataset of inputs Process input through the network Compute the loss (how far is the output from being correct) Propagate gradients back into the network’s parameters Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate* gradient define the networks You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function. basic classes Recap: torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor. nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc. nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to aModule. autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensoroperation, creates at least a single Function node, that connects to functions that created a Tensor and encodes its history. backprop To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients. Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward. net.zero_grad() # zeroes the gradient buffers of all parameters loss.backward() # backprop, calculate gradients optimizer.step() # Does the update the weight ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:2","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"training a classifier 代码框架: loading and normalizing data define the neural network define loss function and optimizer train the network test the network ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:3","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Github: pytorch-tutorial 2018-12-10 对于tensor.detach()的理解. pytorch想做gpu加速版的numpy，取代numpy在python中科学计算的地位。 pytorch的python前端在竭力从语法、命名规则、函数功能上与numpy统一，加持的自动微分和gpu加速功能尽可能地在吸引更大范围内的python用户人群。 [Link] 因此, 在使用pytorch的时候, 仅需要注意自动微分就行了! 而tensor.detach()就是解决禁用自动微分的方法[Link]. (与tensor.clone()区别, tensor.clone()保持了源tensor的requires_grad) 简单理解, 就是把计算图中的一部分拆解下来, 而这部分不需要自动微分. update 作用: 利用detach截断梯度流[Link] 返回一个新变量，与当前计算图分离。结果将永远不需要改变。 如果输入是易失的，输出也将变得不稳定。 返回的 Variable 永远不会需要梯度。 参考: https://discuss.pytorch.org/t/clone-and-detach-in-v0-4-0/16861/2 https://blog.csdn.net/u012436149/article/details/76714349 https://blog.csdn.net/Hungryof/article/details/78035332 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:8:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"torch.Tensor.register_hook[link] register_hook(hook)[SOURCE] Registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -\u003e Tensor or None The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. 登记一个钩子, 在反向传播是调用! refs: https://discuss.pytorch.org/t/solved-reverse-gradients-in-backward-pass/3589 https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94 2018-12-13 比赛心得和pytorch（等）踩得坑[Link] 2018-12-14 20:35:33 在使用某个工具之前, 一定要先看看别人已经踩过那些坑. 比如说使用github上面的开源代码, 先要看看issue里面别人踩过的坑, 然后自己尽量避免, 或者早有准备. 知乎上有一个问题, 里面的回答也非常有建设性: PyTorch 有哪些坑/bug？ 里面的一些回答也非常的不错, 比如: 总结一个代码模板 2019-1-7 22:08:19 又找到一个不错的教程 https://github.com/chenyuntc/pytorch-book 作者陈云, 北邮的研究生, 著有\u003c深度学习框架PyTorch：入门与实践\u003e, 热爱分享, 知乎和github都有不错的干货. 2019-1-9 17:14:17 https://mp.weixin.qq.com/s/mPmFOm32-ipbiIp8mPSd-A 黄海广老师对官网1.0版本教程的翻译 2019-1-13 15:31:59 在XXXLoss的前面不要加softmax? 有些损失需要加, 有些损失已经包含了softmax的计算. 具体来讲 nn.BCELoss前面需要加nn.Sigmoid(), 并且输出一维向量 nn.BCEWithLogitsLoss相当于(nn.Sigmoid() + nn.BCELoss), 因为损失函数包含了归一化 nn.CrossEntropyLoss不需要加nn.Softmax(dim=1), 因为损失函数里面包含了归一化 参考: pytorch loss function 总结 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:8:1","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Nvidia https://www.nvidia.cn/object/what-is-gpu-computing-cn.html ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:1:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"GPU 与 CPU 性能比较 理解 GPU 和 CPU 之间区别的一种简单方式是比较它们如何处理任务。CPU 由专为顺序串行处理而优化的几个核心组成，而 GPU 则拥有一个由数以千计的更小、更高效的核心（专为同时处理多重任务而设计）组成的大规模并行计算架构。 ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:2:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"CPU串行处理 VS GPU并行处理 总的来说在于核心数上面的优势, CPU一般核心数是较少了(Intel Core i9-7980XE @ 2.60GHz, 18core), 而GPU核心数较多(NVIDIA TESLA V100/CUDA 5120) ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:3:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"参考 http://itianti.sinaapp.com/index.php/cpu http://itianti.sinaapp.com/index.php/gpu https://ark.intel.com/compare/126699,120496 https://www.nvidia.com/zh-cn/deep-learning-ai/ https://item.jd.com/31346484884.html [京东NVIDIA TESLA V100介绍] ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:4:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"cuda CUDA 是 NVIDIA 发明的一种并行计算平台和编程模型。它通过利用图形处理器 (GPU) 的处理能力，可大幅提升计算性能。 https://www.geforce.cn/hardware/technology/cuda ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:5:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"tensor core https://www.nvidia.com/en-us/data-center/tensorcore/ ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:6:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"传说BERT牛皮得不行, 好奇看了看. 里面用到了Transformer Block, 这是什么结构? 其实也就是Attention as all you need的Transformer. 之前读Attention as all you need 也是云里雾里的, 今天又再看了看这个Transformer的结构. 首先说说attention 的原理: 将query 和key-value 对的集合 映射到输出 (将query 和key 计算出一个关于value 的weight (也就是attention), 然后输出) Transformer也就是attention的具体实现, 总的有两种结构: Scaled Dot-Product Attention Multi-Head Attention ","date":"2018-10-18","objectID":"/posts/old/deep-learning/attention.html:0:0","tags":["dl","attention"],"title":"Attention","uri":"/posts/old/deep-learning/attention.html"},{"categories":["dl"],"content":"Scaled Dot-Product Attention 右边的结构图中, 每个方框对应一个算子, 可以和公式中的每个计算对应 weight 就是attention. 就是对于value有不同的权重(attention) ","date":"2018-10-18","objectID":"/posts/old/deep-learning/attention.html:0:1","tags":["dl","attention"],"title":"Attention","uri":"/posts/old/deep-learning/attention.html"},{"categories":["dl"],"content":"Multi-Head Attention multi-head 就是将原始的向量拆分为多个子向量(或者做不同的映射成多个向量) 然后每个子向量分别做scale dot-product attetion 然后把计算的结果进行连接, 再做一次映射作为输出 // multi-head 相当于拆分后子空间的scale dot-product attention的集成(stacking), // 能学习到更多的参数, 支持并行 ","date":"2018-10-18","objectID":"/posts/old/deep-learning/attention.html:0:2","tags":["dl","attention"],"title":"Attention","uri":"/posts/old/deep-learning/attention.html"},{"categories":["reading"],"content":"2018-11-20 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:1:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"数据标注|人工智能背后的人 之前有了解过ImageNet有过巨大的标记成本, 找了些资料看了看, 这篇文章的信息量还是非常巨大的, 无论是从人工智能的技术角度, 还是社会角度, 都有所涉及. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:2:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"人工智能技术 人工智能技术还存在着许多的痛点与挑战, 这篇文章就从数据标注这个角度讲了讲. 数据标注的成本巨大 数据标注的质量参差不齐 因此, 也引发了如何解决数据标注的问题 小标记数据如何学习 迁移学习 半监督学习 无监督学习 数据增强 对于标注质量的参差不齐如何提升模型的性能 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:2:1","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"社会角度 数据标注的工作人员一般为生活在社会的底层, 然而这部分工作也是付出了血汗, 但是反馈并不高的行业, 如何解决这样的社会问题, 是需要投入更多教育, 让弱者也能更轻松的工作. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:2:2","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-10-30 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:3:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"Five Interview Questions to Predict a Good Data Scientist 想找数据科学的相关岗位, 看看招聘者是如何总结的. 要么有较高的文聘, 要么有特定领域有多年的经验. 有较好的编码能力. 对机器学习的基础知识有较好的掌握. 对数据科学的热情, 是否参加过比赛, 会议发言, 写书或文章. 方法论的形成, 失败后的总结. 对新问题的数据建模问题. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:4:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-10-26 https://weibo.com/1402400261/GEmL0cQoc 𝒎𝒊𝒏 𝑮 𝒎𝒂𝒙 𝑫 𝔼𝒙 [𝒍𝒐𝒈 𝑫 (𝒙))] + 𝔼𝒛 [𝒍𝒐𝒈(𝟏 − 𝑫(𝑮(𝒛)))] GANs: 最小的生成误差, 最大的判别误差 awesome GANs!!!😄 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:5:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-10-18 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:6:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"博士五年之后五年的总结（其一） $技术, 感悟 经验的积累, 方法的总结至关重要. 作为毕业五年的优秀毕业生, 他都有些什么感悟呢? 读什么东西，就成为什么样的人。 在日积月累之下，做或者不做这些，会让每个人最终成为不一样的人。 所以多看点动脑的内容，就不会让大脑生锈。做科研一个比较好的地方是工作本身并不重复，而是一直在开拓边界，这样自然会有更多的动脑时间。在闲暇时间，我经常会多看知乎上做数学和物理的同学们的回答，最好有几个不懂的名词需要自己去查去想想，手机上有个刷Arxiv的app经常看看，看一看一些优秀的github代码，也会动手刷刷题。 作者从多个角度总结了自己的经验 选方向？先要控制自己阅读的入口 如何选方向 如何抓紧时间 如何坚持一个长期的方向 作者的文笔是相当不存的, 简单而充实. 另外, 还有其他的后续的几篇, 值得阅读 远东轶事 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:7:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-11-15 [LINK] 曾经买了一本精要主义, 当然这本书中很多方法都是有效的, 不过主要还是在于吸收与执行. 如果只是读过而没有实践, 那么一切都是徒劳的大白话而已. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:8:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["随笔"],"content":"一 前些天又准备开始写那个计划了很久的小软件。虽然是一个小软件，但是对于我这种技术还没到家的人也显得有些困难，而且时间有限，写得一点也不舒服。好不容易在网上找了一个，功能完全能满足我的需要。但是因为是歪果仁写的，很多功能需要依赖Google Services（你要知道，在天朝是不允许的）另外还有一些界面不是很满意的地方，然后就想尽办法想去改变这些。为了达到自己心中的状态，花费了一天时间，也没有达到自己满意的状态（虽然有一些改观）。 ","date":"2016-07-08","objectID":"/posts/old/essay/stay-silence.html:1:0","tags":["感受"],"title":"别闹心","uri":"/posts/old/essay/stay-silence.html"},{"categories":["随笔"],"content":"二 需要一本书，但是已经绝版了。某宝有一些卖的，大多数都是二手或者盗版。看上了一家旧货店（明显标注正版），耐心询问（正版且比较干净整洁），店家也给了肯定答复。于是决定购入，还顺便买了一本其他的书。今天收到书了，XXX一本是盗版，印刷质量实在不敢恭维，另一本也说不上整洁（十分讨厌书上无规则的线条）。 ","date":"2016-07-08","objectID":"/posts/old/essay/stay-silence.html:2:0","tags":["感受"],"title":"别闹心","uri":"/posts/old/essay/stay-silence.html"},{"categories":["随笔"],"content":"总结 可以改变的，一定要努力做到更好 不能改变的，也要舒舒服服的接受 弄得自己身心俱累，真是愚笨 ","date":"2016-07-08","objectID":"/posts/old/essay/stay-silence.html:3:0","tags":["感受"],"title":"别闹心","uri":"/posts/old/essay/stay-silence.html"},{"categories":["机器学习"],"content":"k-NN(k-Nearest Neighbors) k-近邻算法 概述 k-近邻算法采用测量不同的特征值之间的距离方法进行分类 k-近邻算法的一般流程 收集数据：可以使用任何方法 准备数据：距离计算所需要的数值，最好是结构化的数据格式 分析数据：可以使用任何方法 训练算法：此步骤不适用于k-近邻算法 测试算法：计算错误率 使用算法：首先需要输入样本数据和结构化的输出结果，然后使用k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理 对未知类别属性的数据集中的每个点依次执行以下操作 计算已知类别数据集中的点与当前点的距离 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在类别的出现频率 返回前k个点出现频率最高的类别作为当前点的预测分类 ","date":"2016-05-14","objectID":"/posts/old/machine-learning/knn-in-c.html:1:0","tags":["算法","机器学习"],"title":"KNN算法的C语言实现","uri":"/posts/old/machine-learning/knn-in-c.html"},{"categories":["机器学习"],"content":"按照上述步骤，可以实现k-近邻算法 k-近邻算法的C语言实现 #include \u003cstdlib.h\u003e#include \u003cstdio.h\u003e#include \u003cmath.h\u003e #define SIZE_ATTR 3 /* 属性维度 */#define SIZE_TRAIN 500 /* 训练集大小 */#define SIZE_TEST 500 /* 测试集大小 */#define K 7 /* 所选k值 */ #define FILE_TRAIN \"train.txt\" /* 记录所构成的结构体变量 */ typedef struct _DataVector { int id; /* 标号 */ float attr[SIZE_ATTR]; /* 属性 */ int label; /* 类别 */ } DataVector; /* 把记录中的属性换成距离后的结构体变量 */ typedef struct _DistanceVector { int id; /* 标号 */ int label; /* 类别 */ float distance; /* 距离 */ } DistanceVector; /* 属性的结构体变量 可以先对属性值做一个分析，再做下一步针对性处理（如归一化特征值处理） */ typedef struct _AttrValue { float max; /* 属性的最大值 */ float min; /* 属性的最小值 */ float length; /* 属性的长度 */ } AttrValue; /* 定义全局变量 */ DataVector trainSet[SIZE_TRAIN]; /* 训练集 */ DataVector testSet[SIZE_TEST]; /* 测试集 */ DistanceVector knn[SIZE_TRAIN]; /* 距离存储 */ AttrValue av[SIZE_ATTR]; /* 属性的属性 */ /* 从文件中加载数据到内存 */ void loadDataFromFile(FILE *fp, char *fileName, DataVector *dv, int length) { int i, j; if ((fp = fopen(fileName, \"r\")) == NULL) { printf(\"open \\\"%s\\\"failured!/n\", fileName); exit(1); } for (i = 0; i \u003c length; ++i) { for (j = 0; j \u003c SIZE_ATTR; ++j) { fscanf(fp, \"%f \", \u0026dv[i].attr[j]); } fscanf(fp, \"%d\\n\", \u0026dv[i].label); } fclose(fp); } /* 准备数据 */ void loadData() { FILE *fp = NULL; loadDataFromFile(fp, FILE_TRAIN, trainSet, SIZE_TRAIN); loadDataFromFile(fp, FILE_TRAIN, testSet, SIZE_TEST); printf(\"loading data success!\\n\"); } /* 数据分析（预处理） 计算每个属性长度，为归一化特征值准备 */ void preProcess() { int i, j; /* 初始化 */ for (i = 0; i \u003c SIZE_ATTR; ++i) { av[i].max = trainSet[0].attr[i]; av[i].min = trainSet[0].attr[i]; } /* 计算属性最大最小值 */ for (i = 0; i \u003c SIZE_TRAIN; ++i) { for (j = 0; j \u003c SIZE_ATTR; ++j) { if (trainSet[i].attr[j] \u003e av[j].max) { av[j].max = trainSet[i].attr[j]; } else if (trainSet[i].attr[j] \u003c av[j].min) { av[j].min = trainSet[i].attr[j]; } } } /* 计算属性长度 */ for (i = 0; i \u003c SIZE_ATTR; ++i) { av[i].length = av[i].max - av[i].min; } } /* 归一化特征值 公式：newValue = (oldValue - min) / (max - min) */ float autoNorm(float oldValue, AttrValue *av) { return (oldValue - (av-\u003emin)) / (av-\u003elength); } /* 距离计算 这里计算的是欧式距离 */ float calcDistance(DataVector d1, DataVector d2) { float sum = 0.0; float newValue; int i; for (i = 0; i \u003c SIZE_ATTR; ++i) { newValue = autoNorm((d1.attr[i] - d2.attr[i]), av+i); sum += newValue * newValue; } return (float) sqrt(sum); } /* 把每个数据的属性向量转化为距离 */ void transDistance(DataVector dv) { int i; for (i = 0; i \u003c SIZE_TRAIN; ++i) { /* 对距离进行赋值 */ knn[i].id = i; knn[i].label = trainSet[i].label; knn[i].distance = calcDistance(trainSet[i], dv); } } /* 对所有距离进行排序，选取距离最小的k个数据向量（此处使用直接选择排序） */ void knnSort() { int i, j, k; DistanceVector temp; for (i = 0; i \u003c K; ++i) { k = i; /* 从无序序列中挑出一个最小的元素 */ for (j = i + 1; j \u003c= SIZE_TRAIN; ++j) { if (knn[k].distance \u003e knn[j].distance) { k = j; } } temp = knn[i]; knn[i] = knn[k]; knn[k] = temp; } } /* 预测分类 */ int forecastClassification() { int freq[K] = {0}; int maxFreq = 0; int i, j, k = 0; /* 确定前k个点所在类别出现的概率 这里有点欠妥，因为分类最多能出现k个，出现了重复类别重复计算*/ for (i = 0; i \u003c K; ++i) { for (j = 0; j \u003c K; ++j) { if (knn[j].label == knn[i].label) { freq[i]++; } } } /* 找到最大频率 */ for (i = 0; i \u003c K; ++i) { if (freq[i] \u003e maxFreq) { maxFreq = freq[i]; k = i; } } /* 得到最大频率的类别 */ return knn[k].label; } /* 对测试数据进行测试 */ void test() { int i; int k = 0; loadData(); preProcess(); /* 对每一条测试数据进行计算 */ for (i = 0; i \u003c SIZE_TEST; ++i) { transDistance(testSet[i]); knnSort(); if (testSet[i].label == forecastClassification()) { printf(\"1\"); } else { printf(\"0\"); ++k; } } printf(\"\\nTest end, wrong time is %d, the correct rate is %.2f%%\\n\", k, (float) (SIZE_TEST - k)/SIZE_TEST*100); } void main() { test(); system(\"pause\"); } 参考资料 机器学习实战. Peter Harrington 测试材料 机器学习实战源代码/Ch02/datingTestSet2.txt 另外，代码还有许多可以改进之处，比如当所取K值为偶数时，预测概率为50%时的下一步处理等 ","date":"2016-05-14","objectID":"/posts/old/machine-learning/knn-in-c.html:1:1","tags":["算法","机器学习"],"title":"KNN算法的C语言实现","uri":"/posts/old/machine-learning/knn-in-c.html"},{"categories":["涉猎"],"content":"[TOC] 经过好多天的努力，博客基本上也达到了自己满意的样子。博客以Bootstrap-Blog为蓝本，在此基础上增增减减，修修改改。因为自己并不是很懂网页，大多时候都是摸索和调试，所以费时还是挺多的。现在终于能踏踏实实的写文章了。 之前我也用过很多博客，但是自己却很少认认真真的写过文章。原因之一就是自己文笔不好，思考半天都写不出来一句话。从小学到大学，作文一直就是我的痛点，时间费得最多，效果还不是很好，这也是我可提高之处吧！现在，我想我得尽量多写吧，至少搭建这个博客我还是挺费心的。其实很多时候不必思考过多，随心所欲，应该也是很不错的。 写这篇文章的目的是对我的Hexo之旅做一个小结，也算对这次博客搭建告一段落。 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:0:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"所参考的一些链接 当然，最主要的还是Hexo官网上的资源，包括教程、主题等等。 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:1:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"详细教程 如何搭建一个独立博客——简明Github Pages与Hexo教程 这篇文章十分详细，作者在后面给了很多其他教程的链接。 图灵社区：Hexo合集 这篇文章对于Hexo的使用更加详细，适合想要深度折腾的同学。 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:2:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"增增减减 Hexo，添加返回顶部按钮，如果打不开，请点击这里 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:3:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"一些问题 npm国内被墙的解决方法，应该前两步就能解决。 连接挂掉, 补(http://blog.csdn.net/qq_23329541/article/details/68927747) npm config set strict-ssl false npm config set registry \"http://registry.npmjs.org/\" 被墙的另一个解决方法。 使用taobao npm镜像 npm install -g cnpm --registry=https://registry.npm.taobao.org 搭建 hexo，在执行 hexo deploy 后,出现 error deployer not found:github 的错误 OK, 剩下的就是好好的写文章吧！ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:4:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"160622更新 设置sublime_text为默认编辑器 git config --global core.editor \"'D:/Program Files (x86)/Sublime Text/sublime_text.exe' -w\" 设置Notepad++为默认编辑器 git config --global core.editor \"'D:/Program Files (x86)/Notepad++/notepad++.exe' -multiInst -nosession\" 直接使用命令subl编辑文本 把sublime_text.exe的所在目录添加至环境变量的path中 在Git下直接使用命令 subl filename.txt 就可以直接使用sublime text编辑文本文件了 参考链接: https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:5:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"161104更新 使用上述教程部署网站出现错误 $ hexo d ... nothing to commit, working tree clean bash: /dev/tty: No such device or address error: failed to execute prompt script (exit code 1) fatal: could not read Username for 'https://github.com': Invalid argument FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html Error: bash: /dev/tty: No such device or address error: failed to execute prompt script (exit code 1) fatal: could not read Username for 'https://github.com': Invalid argument ... 解决方法： 修改根目录文件：_config.yml 原有内容： repository: https://github.com/username/username.github.io.git 修改为： repository: git@github.com:username/username.github.io.git 参考链接：http://www.jianshu.com/p/d1fc64c445ce ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:6:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"update for detail steps download git download node.js // gen ssh-key ssh-keygen -t rsa -C \"username@gmail.com\" // add ssh-key to github (http://blog.csdn.net/keyboardota/article/details/7603630) // verify ssh -T git@github.com // config git config --global user.name \"username\" git config --global user.email \"username@gmail.com\" npm install -g cnpm --registry=https://registry.npm.taobao.org cnpm install -g hexo cnpm install hexo-deployer-git --save // or use taobao mirror // npm config set strict-ssl false // npm config set registry \"http://registry.npmjs.org/\" // npm install -g hexo // npm install hexo-deployer-git --save cd your_hexo_dir hexo init ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:7:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"2017-9-6 update add quick generate_deploy.bat @echo off \u0026 rem not print commond title Hexo \u0026 rem setting title rem cd D:/github/hexo \"D:\\Program Files\\Git\\bin\\sh.exe\" --login -i -c \"cd /d/github/hexo \u0026\u0026 hexo generate --deploy\" pause exit ref: https://stackoverflow.com/questions/5203723/how-do-i-write-a-batch-file-which-opens-the-gitbash-shell-and-runs-a-command-in https://segmentfault.com/q/1010000000263597 https://www.zhihu.com/question/38962022 http://t.cn/RA4BPda update ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:8:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"代码块空行问题 change hexo util file highlight.js file path: your_hexo_dir/node_modules/hexo-util/lib/highlight.js update code for (var i = 0, len = lines.length; i \u003c len; i++) { line = lines[i]; if (tab) line = replaceTabs(line, tab); numbers += '\u003cspan class=\"line\"\u003e' + (firstLine + i) + '\u003c/span\u003e'; content += '\u003cspan class=\"line'; content += (mark.indexOf(firstLine + i) !== -1) ? ' marked' : ''; content += '\"\u003e' + line + '\u003c/span\u003e'; } to code for (var i = 0, len = lines.length; i \u003c len; i++) { line = lines[i]; if (tab) line = replaceTabs(line, tab); // numbers += '\u003cspan class=\"line\"\u003e' + (firstLine + i) + '\u003c/span\u003e'; // content += '\u003cspan class=\"line'; // content += (mark.indexOf(firstLine + i) !== -1) ? ' marked' : ''; // content += '\"\u003e' + line + '\u003c/span\u003e'; numbers += '\u003cspan class=\"line\"\u003e' + (i + firstLine) + '\u003c/span\u003e\\n'; content += '\u003cspan class=\"line\"\u003e' + line + '\u003c/span\u003e\\n'; } ref: http://blog.csdn.net/tobacco5648/article/details/42584653 http://xingwu.me/2014/11/08/Hexo-Code-Block-Bugs-Comments-Style-and-Empty-Lines/ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:9:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"permalink config https://clearsky.me/hexo-permalinks.html http://www.wuliaole.com/post/permalink_and_internal_link_in_hexo/ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:10:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"2018-10-13 update ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加公式 hexo-math已经被抛弃了, 所以使用hexo-math并无任何效果, 使用hexo-renderer-mathjax进行替代 如果你已经安装了hexo-math, 需要先卸载 $ npm uninstall hexo-math --save 然后, 安装hexo-renderer-mathjax $ npm install hexo-renderer-mathjax --save 在主题的配置文件_config.yml中添加下面属性, 配置mathjax # MathJax Support mathjax: enable: true per_page: true cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML OK, 下面是一些测试 行內公式 $\\sin ^{ 2 }{ \\theta +\\cos ^{ 2 }{ \\theta =1 } }$ 行內公式 行外公式 $$\\frac { dy }{ dx } =\\frac { { e }^{ x } }{ 3{ y }^{ 2 } }$$ 行內公式 $\\sin ^{ 2 }{ \\theta +\\cos ^{ 2 }{ \\theta =1 } }$ 行內公式 行外公式 $$\\frac { dy }{ dx } =\\frac { { e }^{ x } }{ 3{ y }^{ 2 } }$$ 参考链接: https://nathaniel.blog/tutorials/make-hexo-support-math-again/ 另外, 推荐一个主题next ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:1","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"mathjax Mixed Content导致不能渲染公式 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:2","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"或者(chrome出现一个拦截图标, 不安全连接) 前面的两个错误Uncaught Reference Error 加载js的顺序不对, 需要先加载jquery, 然后再加载后两个js 第三个问题, 也就是渲染公式 修改[hexo_root]\\node_modules\\hexo-renderer-mathjax\\mathjax.html: 把最后一行 \u003cscript type=\"text/x-mathjax-config\"\u003e MathJax.Hub.Config({ tex2jax: { inlineMath: [ [\"$\",\"$\"], [\"\\\\(\",\"\\\\)\"] ], skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], processEscapes: true } }); MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(); for (var i = 0; i \u003c all.length; ++i) all[i].SourceElement().parentNode.className += ' has-jax'; }); \u003c/script\u003e \u003cscript src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"\u003e\u003c/script\u003e 替换为 \u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML\"\u003e\u003c/script\u003e 即修改了渲染js的地址, 地址来源于官网: https://docs.mathjax.org/en/latest/start.html 参考: https://github.com/hexojs/hexo/issues/3279 ! 尝试了修改hexo下的配置和主题下的配置添加下面内容都无效 mathjax: cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:3","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加新页面 https://github.com/hexojs/hexo/issues/1453 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:12:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"hexo template https://hexo.io/docs/templates.html https://hexo.io/zh-cn/docs/helpers.html ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:13:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加目录 article.ejs中添加 \u003c!-- TOC --\u003e \u003c% var mtoc = toc(post.content, {list_number: false}); %\u003e \u003c% if (mtoc) { %\u003e \u003cdiv class=\"toc\"\u003e \u003ch3\u003eContent\u003c/h3\u003e \u003c%- mtoc %\u003e \u003c/div\u003e \u003c% } %\u003e custom.css中添加 .toc { background-color: #eee; padding-left: 25px; } 参考: http://izhaoyi.top/2017/05/30/my-blog/#%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:14:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"windows图床工具 https://jverson.com/2017/05/28/qiniu-image-v2/ 七牛云不支持测试域名了, 需要自己的域名, 后面就没测试了 awesome themes: https://github.com/geekplux/hexo-theme-typing ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:14:1","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加 readme https://stackoverflow.com/questions/25258660/how-do-i-add-a-readme-md-file-into-the-root-directory-of-the-generated-blog-by-h/31051913#31051913 注意, 上面很多方法都是不可行的 包含 https://stackoverflow.com/questions/25258660/how-do-i-add-a-readme-md-file-into-the-root-directory-of-the-generated-blog-by-h https://github.com/hexojs/hexo/issues/3158 https://github.com/hexojs/hexo/issues/3248 http://xchb.work/2017/04/08/hexo%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6-skip-render-%E9%85%8D%E7%BD%AE/ https://xuanwo.org/2014/08/14/hexo-usual-problem/ 正确的解决方法 https://hexo.io/docs/configuration#Directory 即README.md文件必须要在source目录下!!! ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:14:2","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"永久链接 https://clearsky.me/hexo-permalinks.html 参考链接 https://juejin.im/entry/5a8079a85188257a6e402c17 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:15:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"站内搜索 https://hackfun.org/2017/10/04/%E7%BB%99hexo%E6%B7%BB%E5%8A%A0%E6%9C%AC%E5%9C%B0%E6%90%9C%E7%B4%A2%E7%AB%99%E5%86%85%E5%8A%9F%E8%83%BD/ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:16:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":null,"content":"前方漫漫 愿我成风 欢迎关注 新浪微博: 王虚极 github: colinwke 最后更新：2016-05-13 ","date":"2016-05-05","objectID":"/columns/page-about.html:0:0","tags":null,"title":"关于","uri":"/columns/page-about.html"},{"categories":null,"content":"[TOC] ","date":"2016-05-05","objectID":"/columns/good-blog.html:0:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"友情推荐 ","date":"2016-05-05","objectID":"/columns/good-blog.html:1:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"charmby 无详细介绍! ","date":"2016-05-05","objectID":"/columns/good-blog.html:1:1","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"苏建林 | 科学空间 里面很多干货, 主要偏NLP方向 作者本来是学数学的, 后面对NLP方向有较高的兴趣, 所以数学机器非常的棒 作者博客中涉猎特别广, 基本什么都写, 内容精良, 专研精神可贵, 值得关注! ","date":"2016-05-05","objectID":"/columns/good-blog.html:2:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"Mingsheng Long 清华大学博士, 副教授 transfer learning ","date":"2016-05-05","objectID":"/columns/good-blog.html:3:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"LancelotHolmes(懒死骆驼) 机器学习与数据挖掘 机器学习面试 https://github.com/LancelotHolmes/ ","date":"2016-05-05","objectID":"/columns/good-blog.html:4:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"lanbing 处女座，理工男，博士僧，爱折腾 喜欢编程、阅读、思考、创作 热衷于计算机视觉、机器学习、计算机技术领域。 https://github.com/lanbing510 ","date":"2016-05-05","objectID":"/columns/good-blog.html:5:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"误入机器学习的码农 机器学习应用感悟. 写作即反思，我见我思. 吴海波，花名吾加，蘑菇街搜索、推荐排序算法owner，大规模机器学习，自然语言处理 ","date":"2016-05-05","objectID":"/columns/good-blog.html:6:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"算法工程师的自我修养 关注互联网领域算法工程师专业能力、综合能力成长。 杨旭东，关注机器学习、人工智能、推荐系统、信息检索、在线广告等领域的经典和前沿技术分享。 ","date":"2016-05-05","objectID":"/columns/good-blog.html:7:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"其他 Charles Xiao, DeepFM ","date":"2016-05-05","objectID":"/columns/good-blog.html:8:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"王喆的机器学习笔记 关注计算广告，推荐系统等机器学习领域前沿知识 王喆 广告/推荐 Sr. Research SDE@Hulu ","date":"2016-05-05","objectID":"/columns/good-blog.html:9:0","tags":null,"title":"博客精华","uri":"/columns/good-blog.html"},{"categories":null,"content":"[TOC] ","date":"2016-05-05","objectID":"/columns/good-post.html:0:0","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"2016-05 ","date":"2016-05-05","objectID":"/columns/good-post.html:1:0","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"程序人生 这是雷军1996年在BBS上写的一篇帖子，现在读起来，也别有一番韵味。尤其是那一句“编程不仅仅是技术，还是艺术”更是说出了编程的真谛，我想也是现在的编程者思考的问题。 ","date":"2016-05-05","objectID":"/columns/good-post.html:1:1","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"KISS principle KISS即keep it simple， stupid的缩写，它是解决复杂代码的法宝，也是优秀代码的准则。 ","date":"2016-05-05","objectID":"/columns/good-post.html:1:2","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"Digital Carpenter Evolves To Geoscientist 这篇文章来自android weekly网站创始人Martin Gauer博客中的一篇文章，主要讲诉了他作为网站前端开发的一个老手转变为地球科学家的过程。这其中是对自己的逐渐的认识，敢于做出改变，然后下定决心和付出行动。 ","date":"2016-05-05","objectID":"/columns/good-post.html:1:3","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"Java的第20年：Java和我的故事 一篇跨越20年的回忆史，通过Java的故事线回忆着作者自己的人生。 ","date":"2016-05-05","objectID":"/columns/good-post.html:1:4","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"2017-10 ","date":"2016-05-05","objectID":"/columns/good-post.html:2:0","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"Discover Feature Engineering, How to Engineer Features and How to Get Good at It 特征工程是数据挖掘中至关重要的一环. 甚至, 在业界中传有, 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已. 这篇文章中, 不仅详实的叙述了特征工程的概念, 还发散性的引导构建特征的思维, 但谨记, 特征工程即使存在一个模式, 也要努力的跳出这个模式. 我想, 特征工程也许就是一个思维发散的过程. ","date":"2016-05-05","objectID":"/columns/good-post.html:2:1","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"进程与线程的一个简单解释 工厂的例子很形象, 插图也非常的不错! ","date":"2016-05-05","objectID":"/columns/good-post.html:2:2","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"关于 KaiMing He 的两篇论文 https://www.zhihu.com/question/67119841 https://www.zhihu.com/question/57403701 周博磊评论“从现象和问题出发追溯本质的思想” 现今深度学习在机器学习领域的研究和应用中, 处于一个十分火热的状态, 甚至对统计学习呈一种打压的景象. 所以不禁会问, 继续学习统计学习还有必要吗? 可以从周博磊的评论中能得到一点启发: 从现象和问题出发追溯本质的思想. 深度学习的应用主要在cv, speech recognition和nlp问题上, 这类问题一般为表达结构较为复杂的问题. 深度学习简化了提取特征的这个步骤, 而对于一些结构化的数据, 统计学习依然能够, 或者胜任深度学习. 另一方面, 在KDD领域, 统计学习依然占有一席之地, 而KDD更偏重于使用机器学习算法解决实际的问题, 也更注重于特征工程. 最后, 对于统计学习和深度学习的讨论可以参见周志华老师机器学习序言的问题三. ","date":"2016-05-05","objectID":"/columns/good-post.html:2:3","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"裴健当选SIGKDD主席, 研究被引超7万次，他还有一个遗憾 | 专访 数据挖掘的核心是对数据和业务的理解能力和对算法的构建能力。 是不是数据永远是越大越好？对于研究者来说，怎样才算是适合的数据？ 一般来说，数据是越多越好。 深度学习需要大量的数据来产生可以generalize的模型。 在实际应用中，数据往往是有成本的。 有很多应用场景不容易获取大量的高质量数据。 所以说我们需要针对具体问题，获取合适的数据。 在这方面，统计学对数据的采集评价有一系列的方法和原则，值得深入学习。 另一个方面，要很好利用大量的数据，通常需要比较复杂的模型，对计算的要求也相应地比较高，所以我们要根据数据量和应用来选择合适的模型。 ","date":"2016-05-05","objectID":"/columns/good-post.html:2:4","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"数据挖掘最强华人齐聚加拿大，干货满满亮点不断 | KDD 2017 杨强教授提到, 特征比模型更为重要(Big Data is useless unless it can deliver big feature space). 对于Data Minming来讲, 实际也是这么回事, 但是也不能否认模型的重要性, 只不过大多数模型的算法已经成熟了, 而特征则更需要对业务的分析与理解, 所以, 特征才表现出了更加的重要. ","date":"2016-05-05","objectID":"/columns/good-post.html:2:5","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"机器学习该怎么入门？ ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。 一同门一直在哔哔, 现在理解不深, 可能以后会加深理解. (边笑边逃) ","date":"2016-05-05","objectID":"/columns/good-post.html:2:6","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"Yann LeCun创造的新词“预测学习”将要落脚于GANs？ 如果你沉浸在传统机器学习领域, 又对深度学习了解很少, 那么你可以看看这一篇文章. 首先是Yann LeCun的\"蛋糕比喻\", 形象的描述了人工智能的三大领域: 强化学习, 监督学习和无监督学习. 其次是对于GANs(Generative Adversarail Networks)的描述. GANs是无监督学习的一个方法, 如LeCun所描述的那样: 对抗性网络是“20年来机器学习领域最酷的想法”, GANs的概念充满了想象力, 也让读者有一些引申的思考. 最后, 通过这篇文章也会增进你对深度学习的理解. 最后, 看看GANs生成的图片, 是不是很新奇呢? ","date":"2016-05-05","objectID":"/columns/good-post.html:2:7","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"清华大学刘知远：在 NLP 领域「做事」兼「发声」 如果你想了解一个人, 你就和他多接触一下吧, 听听他的故事. 文章通过两个项目, 以及为学生, 为老师三方面介绍了刘知远博士, 大神的练成并不简单, 更多的是努力和智谋. 顺便文章还介绍了一些NLP的项目, 值得收藏. ","date":"2016-05-05","objectID":"/columns/good-post.html:2:8","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"KDD 2017最佳论文得主叶艳芳专访：AI时代的互联网安全 – 攻与防的黑白博弈 又一篇关于或学者或导师的访谈. 在研究方面, 机遇与挑战共存. 在导师方面, 站的够高, 看的更远. ","date":"2016-05-05","objectID":"/columns/good-post.html:2:9","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"2017 NBA 选秀有哪些值得关注的新秀？ 走向成功的道路并不平坦, 但是他们胸怀勇气, 披荆斩棘, 坚持不懈, 最后得到了成功. 每个人都有着他们不同寻常的故事, 也造就了他们的成就! ","date":"2016-05-05","objectID":"/columns/good-post.html:2:10","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"2017-12 ","date":"2016-05-05","objectID":"/columns/good-post.html:3:0","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"很傻很天真的贝叶斯定理 贝叶斯定理是一种思维上的推理方式. 一般的推理是建立在因果关系上的, 对于执果索因, 就需要使用贝叶斯定理. ","date":"2016-05-05","objectID":"/columns/good-post.html:3:1","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"F1 比赛中最严重的事故是哪次？ 一些事故让人感到沮丧, 但是, 他们以更坚强的方式活着! ","date":"2016-05-05","objectID":"/columns/good-post.html:3:2","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"文本情感分类与深度学习模型 共三篇关于NLP的文章, 每次都是对上次内容的更正. 其中第三篇为一年后的更正, 实在难为可贵! http://kexue.fm/archives/3360/ http://spaces.ac.cn/archives/3414/ http://kexue.fm/archives/3863/ 详细的讲述了作者做情感分析的过程及想法, 内容详实, 对于刚上手NLP分类有很好的引导作用. 本以为是团体运维的博客, 但就是个人博客. 博客内容精良, 专研精神可贵, 涉猎广泛, 值得关注! ","date":"2016-05-05","objectID":"/columns/good-post.html:3:3","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"2018-01 ","date":"2016-05-05","objectID":"/columns/good-post.html:4:0","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"How to do machine learning efficiently 10秒原则 时间是最贵重的成本. 永远不允许计算超过10s的问题(抽样). 急于成功 更专注一个问题. 由简至繁的构建. ","date":"2016-05-05","objectID":"/columns/good-post.html:4:1","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"2018-11 ","date":"2016-05-05","objectID":"/columns/good-post.html:5:0","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"Ask Me Anything session with a Kaggle Grandmaster 作者是一位物理专业毕业的博士生, 后面转向数据科学(DS). 过程中也遇到很多困难, 坚持可热情, 还有总结沉淀下来的方法让他成为了Kaggle Grandmaster. 在这个过程中, 作者是付出了很多精力的. 在全职的工作上要保持比赛的强度, 是工作也比赛的权衡. 当然, 作者也会发费时间在运动, 旅游上面. 但是更多的闲暇时间用在了比赛和学习上. 当最后成为Kaggle Grandmaster时, 作者认为一切都是值得的. 里面有很多比赛方法上的经验, 在打比赛之前和之后都可以看看! 千里之行始于足下, 最困难的一步也就是第一步. 来自于爱可可老师的推荐 “你在Kaggle学到的技能只是你在工业或学术界工作时所需技能的一小部分，那些Kaggle不涉及的技术领域基础教育可能是至关重要的” “要加速磁盘jpeg图像I/O，不应该用PIL，Skimage甚至OpenCV，而是用libjpeg-turbo或PyVips。” “Kaggle技能是我从学术界和其他知识来源获得的一系列技能的有力补充” “高估你的专业、大学等在该行业找工作中的影响是不明智的。一家公司雇用你、愿意付钱给你是希望解决他们面临的问题。学位和专业只是评估能力的参考……” “机器学习领域的论文、竞赛、博客和书籍实在太多，根本看不过来。实际上，当我遇到问题，会专注于查看最近结果并深入研究。完成后，再切换到下个问题，只是掌握缺少实践经验领域的高级知识……NIPS、CVPR等会议，可以很好地代表在目前研究阶段我们能做什么和不能做什么” 附上机器之心翻译[link]版本. ","date":"2016-05-05","objectID":"/columns/good-post.html:5:1","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"从「深度学习」到「深度」学习 | 龙明盛老师专访 近期读了龙明盛老师的几篇论文, 感觉其是一位很nice的年轻老师, 学者. 偶然在网上看到这篇专访, 讲述了龙明盛老师的为师者, 为学者, 为人者的态度和践行. 一位优秀的往往能启发更优秀的学生, 值得学习! ","date":"2016-05-05","objectID":"/columns/good-post.html:5:2","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 吴海波 花名吾加，蘑菇街搜索、推荐排序算法owner，大规模机器学习从业者. 醍醐灌顶! 前辈走出的路, 总结的经验一定要好好的看, 尤其是如此优秀的文章! 首先文章以算法工程师面试最为引子, 算法工程师首先需要能解决工程问题, 因此代码能力是必不可少的, 要求也是严格的, 其次是机器学习理论和实践上的能力. 同时具有上面两个能力才能称作是一个合格的算法工程师. 但是作者并没有局限在于算法工程师上面, 而是在于职业的操守. 理论 + 实践, KPI, ROI, 这些才是算法工程师要面对的问题. 而为了有产出, 必须要对算法进行一些探索, 工程化. 最后是作者也是深耕于搜索, 推荐排序的算法工程师, 而且乐于分享自己的想法, 推动行业的前进, 值得关注! 作者关于此问题写了两篇 其一: 论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 其二: 算法工程师又不只是工程师 另外还有作者还引用了多篇质量非常高的关于推荐排序的博文和专栏, 包含了杨旭东的几篇文章, 之前也有看过. 另外还引用了很多领域的论文. 最后, 附上吴海波的专栏误入机器学习的码农, 以及杨旭东的专栏算法工程师的自我修养. ","date":"2016-05-05","objectID":"/columns/good-post.html:5:3","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"爬虫租房, 是一个不错的注意 另外还有一个开源的代码 https://github.com/lorien/awesome-web-scraping/blob/master/python.md 又发现了一个 https://github.com/XuefengHuang/lianjia-scrawler ","date":"2016-05-05","objectID":"/columns/good-post.html:5:4","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"一份写给NLP研究者的编程指南 开发经验非常的重要! 快速原型开发!!!!!!!!!然后再重构与模块化!!!!!!!!!! 写安全的工程代码 ","date":"2016-05-05","objectID":"/columns/good-post.html:5:5","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"贫富差距是如何逐渐扩大的 总结一下: 先投资自己 出发点是基于长期的回报而不是即时的满足 多种收入来源 不断的投资自己 从不指责, 对自己的失败承担全部责任 不睡懒觉 清晰的愿景和目标 进步后娱乐 ","date":"2016-05-05","objectID":"/columns/good-post.html:5:6","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"学习Git, 这里有个非常耐撕的工具 demo hands on ","date":"2016-05-05","objectID":"/columns/good-post.html:5:7","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"善用资源，找书，看书，而不是囤书 依然是那句话, 你拥有的资源多少并不重要, 如果你不知道利用的话, 一切都是无用的. 深度强化学习综述: https://weibo.com/1402400261/H4rJvt4Fk 机器学习在线课程: https://weibo.com/1402400261/H4rH3EhXb 机器学习相关学习资料: https://github.com/iamalotaibi/Machine-Learning ","date":"2016-05-05","objectID":"/columns/good-post.html:5:8","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"我的八年博士生涯 – 王赟 Maigo 学术篇 娱乐篇 在知乎上, 感觉王赟是一个非常平易近人的人, 他通过两个方面总结了他的博士生涯, 看上去也非常的评议近人. 在学术上, 做的研究和项目好像也没有很大的高大上, 但是却踏踏实实, 虽然走了很多弯路, 但是也有一些不错的收获. 在完成博士论文可谓惊险, 正是这种惊险也使得博士生涯更具有一番味道. 在娱乐上, 可能是比较玩得开的人, 也愿意玩的人, 让整个博士生涯变得不那么的单调, 很好! ","date":"2016-05-05","objectID":"/columns/good-post.html:5:9","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"斯坦福机器学习课程“CS 229 - Machine Learning”速查表(中文版) 全面, 简洁, 整理得很好! ","date":"2016-05-05","objectID":"/columns/good-post.html:5:10","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"图文详解如何利用Git+Github进行团队协作开发 Git在版本控制中是非常重要的工具, 这篇文章把多分支开发说的很详细! ","date":"2016-05-05","objectID":"/columns/good-post.html:5:11","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"张小龙总结微信8年 https://mp.weixin.qq.com/s/F1LWh9UkeClSgZjE0O5A8Q http://wx2.sinaimg.cn/large/6106a4f0ly1fz1cqvtg3xj21bf2s3n3o.jpg ","date":"2016-05-05","objectID":"/columns/good-post.html:5:12","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"高效学习 https://weibo.com/1707613190/HbgL0n5E0 ","date":"2016-05-05","objectID":"/columns/good-post.html:5:13","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/516385585 这是一个系列视频, 指的get ","date":"2016-05-05","objectID":"/columns/good-post.html:5:14","tags":null,"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":null,"content":"[TOC] ","date":"2016-05-05","objectID":"/columns/reading-note.html:0:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"2021-05 真感觉太崇拜雷军了, 简直快成为了他的教徒了! 今天偶人看了两个视频: ","date":"2016-05-05","objectID":"/columns/reading-note.html:1:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"许知远vs雷军《灵魂对话》 这里很打动的两个点是 要适当跳出自己的舒适圈, 说服自己与培养兴趣, 只有深入调研, 才有发言权. 创业, 要有信仰, 自己相信, 还要别人相信. 所有, 就有了说服自己和说服别人, 要让别人不仅能和你站在同一艘船上, 而且要心甘情愿, 信心满满的, 全力以赴的支持你. 不仅想到了在公司低谷的时候, 老大是如何的所谓别人口中的画饼, 现在也慢慢理解了, 如果不能说服你认真努力工作, 那leader用来干嘛呢? 所以, 不仅是创业, 到后面带领团队也一样, 一定要有一个很好的说服能力, 能够很好的组织工作, 高效工作. ","date":"2016-05-05","objectID":"/columns/reading-note.html:1:1","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"雷军：创业第一课 内容很多, 干货也很多, 没有看完, 创业必看! 我是一个不同时代的分界线 ","date":"2016-05-05","objectID":"/columns/reading-note.html:2:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"2016-05 ","date":"2016-05-05","objectID":"/columns/reading-note.html:3:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"程序人生 这是雷军1996年在BBS上写的一篇帖子，现在读起来，也别有一番韵味。尤其是那一句“编程不仅仅是技术，还是艺术”更是说出了编程的真谛，我想也是现在的编程者思考的问题。 ","date":"2016-05-05","objectID":"/columns/reading-note.html:3:1","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"KISS principle KISS即keep it simple， stupid的缩写，它是解决复杂代码的法宝，也是优秀代码的准则。 ","date":"2016-05-05","objectID":"/columns/reading-note.html:3:2","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"Digital Carpenter Evolves To Geoscientist 这篇文章来自android weekly网站创始人Martin Gauer博客中的一篇文章，主要讲诉了他作为网站前端开发的一个老手转变为地球科学家的过程。这其中是对自己的逐渐的认识，敢于做出改变，然后下定决心和付出行动。 ","date":"2016-05-05","objectID":"/columns/reading-note.html:3:3","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"Java的第20年：Java和我的故事 一篇跨越20年的回忆史，通过Java的故事线回忆着作者自己的人生。 ","date":"2016-05-05","objectID":"/columns/reading-note.html:3:4","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"2017-10 ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"Discover Feature Engineering, How to Engineer Features and How to Get Good at It 特征工程是数据挖掘中至关重要的一环. 甚至, 在业界中传有, 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已. 这篇文章中, 不仅详实的叙述了特征工程的概念, 还发散性的引导构建特征的思维, 但谨记, 特征工程即使存在一个模式, 也要努力的跳出这个模式. 我想, 特征工程也许就是一个思维发散的过程. ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:1","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"进程与线程的一个简单解释 工厂的例子很形象, 插图也非常的不错! ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:2","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"关于 KaiMing He 的两篇论文 https://www.zhihu.com/question/67119841 https://www.zhihu.com/question/57403701 周博磊评论“从现象和问题出发追溯本质的思想” 现今深度学习在机器学习领域的研究和应用中, 处于一个十分火热的状态, 甚至对统计学习呈一种打压的景象. 所以不禁会问, 继续学习统计学习还有必要吗? 可以从周博磊的评论中能得到一点启发: 从现象和问题出发追溯本质的思想. 深度学习的应用主要在cv, speech recognition和nlp问题上, 这类问题一般为表达结构较为复杂的问题. 深度学习简化了提取特征的这个步骤, 而对于一些结构化的数据, 统计学习依然能够, 或者胜任深度学习. 另一方面, 在KDD领域, 统计学习依然占有一席之地, 而KDD更偏重于使用机器学习算法解决实际的问题, 也更注重于特征工程. 最后, 对于统计学习和深度学习的讨论可以参见周志华老师机器学习序言的问题三. ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:3","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"裴健当选SIGKDD主席, 研究被引超7万次，他还有一个遗憾 | 专访 数据挖掘的核心是对数据和业务的理解能力和对算法的构建能力。 是不是数据永远是越大越好？对于研究者来说，怎样才算是适合的数据？ 一般来说，数据是越多越好。 深度学习需要大量的数据来产生可以generalize的模型。 在实际应用中，数据往往是有成本的。 有很多应用场景不容易获取大量的高质量数据。 所以说我们需要针对具体问题，获取合适的数据。 在这方面，统计学对数据的采集评价有一系列的方法和原则，值得深入学习。 另一个方面，要很好利用大量的数据，通常需要比较复杂的模型，对计算的要求也相应地比较高，所以我们要根据数据量和应用来选择合适的模型。 ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:4","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"数据挖掘最强华人齐聚加拿大，干货满满亮点不断 | KDD 2017 杨强教授提到, 特征比模型更为重要(Big Data is useless unless it can deliver big feature space). 对于Data Minming来讲, 实际也是这么回事, 但是也不能否认模型的重要性, 只不过大多数模型的算法已经成熟了, 而特征则更需要对业务的分析与理解, 所以, 特征才表现出了更加的重要. ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:5","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"机器学习该怎么入门？ ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。 一同门一直在哔哔, 现在理解不深, 可能以后会加深理解. (边笑边逃) ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:6","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"Yann LeCun创造的新词“预测学习”将要落脚于GANs？ 如果你沉浸在传统机器学习领域, 又对深度学习了解很少, 那么你可以看看这一篇文章. 首先是Yann LeCun的\"蛋糕比喻\", 形象的描述了人工智能的三大领域: 强化学习, 监督学习和无监督学习. 其次是对于GANs(Generative Adversarail Networks)的描述. GANs是无监督学习的一个方法, 如LeCun所描述的那样: 对抗性网络是“20年来机器学习领域最酷的想法”, GANs的概念充满了想象力, 也让读者有一些引申的思考. 最后, 通过这篇文章也会增进你对深度学习的理解. 最后, 看看GANs生成的图片, 是不是很新奇呢? ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:7","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"清华大学刘知远：在 NLP 领域「做事」兼「发声」 如果你想了解一个人, 你就和他多接触一下吧, 听听他的故事. 文章通过两个项目, 以及为学生, 为老师三方面介绍了刘知远博士, 大神的练成并不简单, 更多的是努力和智谋. 顺便文章还介绍了一些NLP的项目, 值得收藏. ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:8","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"KDD 2017最佳论文得主叶艳芳专访：AI时代的互联网安全 – 攻与防的黑白博弈 又一篇关于或学者或导师的访谈. 在研究方面, 机遇与挑战共存. 在导师方面, 站的够高, 看的更远. ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:9","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"2017 NBA 选秀有哪些值得关注的新秀？ 走向成功的道路并不平坦, 但是他们胸怀勇气, 披荆斩棘, 坚持不懈, 最后得到了成功. 每个人都有着他们不同寻常的故事, 也造就了他们的成就! ","date":"2016-05-05","objectID":"/columns/reading-note.html:4:10","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"2017-12 ","date":"2016-05-05","objectID":"/columns/reading-note.html:5:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"很傻很天真的贝叶斯定理 贝叶斯定理是一种思维上的推理方式. 一般的推理是建立在因果关系上的, 对于执果索因, 就需要使用贝叶斯定理. ","date":"2016-05-05","objectID":"/columns/reading-note.html:5:1","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"F1 比赛中最严重的事故是哪次？ 一些事故让人感到沮丧, 但是, 他们以更坚强的方式活着! ","date":"2016-05-05","objectID":"/columns/reading-note.html:5:2","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"文本情感分类与深度学习模型 共三篇关于NLP的文章, 每次都是对上次内容的更正. 其中第三篇为一年后的更正, 实在难为可贵! http://kexue.fm/archives/3360/ http://spaces.ac.cn/archives/3414/ http://kexue.fm/archives/3863/ 详细的讲述了作者做情感分析的过程及想法, 内容详实, 对于刚上手NLP分类有很好的引导作用. 本以为是团体运维的博客, 但就是个人博客. 博客内容精良, 专研精神可贵, 涉猎广泛, 值得关注! ","date":"2016-05-05","objectID":"/columns/reading-note.html:5:3","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"2018-01 ","date":"2016-05-05","objectID":"/columns/reading-note.html:6:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"How to do machine learning efficiently 10秒原则 时间是最贵重的成本. 永远不允许计算超过10s的问题(抽样). 急于成功 更专注一个问题. 由简至繁的构建. ","date":"2016-05-05","objectID":"/columns/reading-note.html:6:1","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"2018-11 ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:0","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"Ask Me Anything session with a Kaggle Grandmaster 作者是一位物理专业毕业的博士生, 后面转向数据科学(DS). 过程中也遇到很多困难, 坚持可热情, 还有总结沉淀下来的方法让他成为了Kaggle Grandmaster. 在这个过程中, 作者是付出了很多精力的. 在全职的工作上要保持比赛的强度, 是工作也比赛的权衡. 当然, 作者也会发费时间在运动, 旅游上面. 但是更多的闲暇时间用在了比赛和学习上. 当最后成为Kaggle Grandmaster时, 作者认为一切都是值得的. 里面有很多比赛方法上的经验, 在打比赛之前和之后都可以看看! 千里之行始于足下, 最困难的一步也就是第一步. 来自于爱可可老师的推荐 “你在Kaggle学到的技能只是你在工业或学术界工作时所需技能的一小部分，那些Kaggle不涉及的技术领域基础教育可能是至关重要的” “要加速磁盘jpeg图像I/O，不应该用PIL，Skimage甚至OpenCV，而是用libjpeg-turbo或PyVips。” “Kaggle技能是我从学术界和其他知识来源获得的一系列技能的有力补充” “高估你的专业、大学等在该行业找工作中的影响是不明智的。一家公司雇用你、愿意付钱给你是希望解决他们面临的问题。学位和专业只是评估能力的参考……” “机器学习领域的论文、竞赛、博客和书籍实在太多，根本看不过来。实际上，当我遇到问题，会专注于查看最近结果并深入研究。完成后，再切换到下个问题，只是掌握缺少实践经验领域的高级知识……NIPS、CVPR等会议，可以很好地代表在目前研究阶段我们能做什么和不能做什么” 附上机器之心翻译[link]版本. ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:1","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"从「深度学习」到「深度」学习 | 龙明盛老师专访 近期读了龙明盛老师的几篇论文, 感觉其是一位很nice的年轻老师, 学者. 偶然在网上看到这篇专访, 讲述了龙明盛老师的为师者, 为学者, 为人者的态度和践行. 一位优秀的往往能启发更优秀的学生, 值得学习! ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:2","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 吴海波 花名吾加，蘑菇街搜索、推荐排序算法owner，大规模机器学习从业者. 醍醐灌顶! 前辈走出的路, 总结的经验一定要好好的看, 尤其是如此优秀的文章! 首先文章以算法工程师面试最为引子, 算法工程师首先需要能解决工程问题, 因此代码能力是必不可少的, 要求也是严格的, 其次是机器学习理论和实践上的能力. 同时具有上面两个能力才能称作是一个合格的算法工程师. 但是作者并没有局限在于算法工程师上面, 而是在于职业的操守. 理论 + 实践, KPI, ROI, 这些才是算法工程师要面对的问题. 而为了有产出, 必须要对算法进行一些探索, 工程化. 最后是作者也是深耕于搜索, 推荐排序的算法工程师, 而且乐于分享自己的想法, 推动行业的前进, 值得关注! 作者关于此问题写了两篇 其一: 论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 其二: 算法工程师又不只是工程师 另外还有作者还引用了多篇质量非常高的关于推荐排序的博文和专栏, 包含了杨旭东的几篇文章, 之前也有看过. 另外还引用了很多领域的论文. 最后, 附上吴海波的专栏误入机器学习的码农, 以及杨旭东的专栏算法工程师的自我修养. ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:3","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"爬虫租房, 是一个不错的注意 另外还有一个开源的代码 https://github.com/lorien/awesome-web-scraping/blob/master/python.md 又发现了一个 https://github.com/XuefengHuang/lianjia-scrawler ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:4","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"一份写给NLP研究者的编程指南 开发经验非常的重要! 快速原型开发!!!!!!!!!然后再重构与模块化!!!!!!!!!! 写安全的工程代码 ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:5","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"贫富差距是如何逐渐扩大的 总结一下: 先投资自己 出发点是基于长期的回报而不是即时的满足 多种收入来源 不断的投资自己 从不指责, 对自己的失败承担全部责任 不睡懒觉 清晰的愿景和目标 进步后娱乐 ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:6","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"学习Git, 这里有个非常耐撕的工具 demo hands on ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:7","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"善用资源，找书，看书，而不是囤书 依然是那句话, 你拥有的资源多少并不重要, 如果你不知道利用的话, 一切都是无用的. 深度强化学习综述: https://weibo.com/1402400261/H4rJvt4Fk 机器学习在线课程: https://weibo.com/1402400261/H4rH3EhXb 机器学习相关学习资料: https://github.com/iamalotaibi/Machine-Learning ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:8","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"我的八年博士生涯 – 王赟 Maigo 学术篇 娱乐篇 在知乎上, 感觉王赟是一个非常平易近人的人, 他通过两个方面总结了他的博士生涯, 看上去也非常的评议近人. 在学术上, 做的研究和项目好像也没有很大的高大上, 但是却踏踏实实, 虽然走了很多弯路, 但是也有一些不错的收获. 在完成博士论文可谓惊险, 正是这种惊险也使得博士生涯更具有一番味道. 在娱乐上, 可能是比较玩得开的人, 也愿意玩的人, 让整个博士生涯变得不那么的单调, 很好! ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:9","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"斯坦福机器学习课程“CS 229 - Machine Learning”速查表(中文版) 全面, 简洁, 整理得很好! ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:10","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"图文详解如何利用Git+Github进行团队协作开发 Git在版本控制中是非常重要的工具, 这篇文章把多分支开发说的很详细! ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:11","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"张小龙总结微信8年 https://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==\u0026mid=2691354540\u0026idx=2\u0026sn=fa42b5fea385f75a0afe2ad06ca92f08 http://wx2.sinaimg.cn/large/6106a4f0ly1fz1cqvtg3xj21bf2s3n3o.jpg ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:12","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"高效学习 https://weibo.com/1707613190/HbgL0n5E0 ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:13","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/516385585 这是一个系列视频, 指的get ","date":"2016-05-05","objectID":"/columns/reading-note.html:7:14","tags":null,"title":"阅读推荐","uri":"/columns/reading-note.html"},{"categories":null,"content":"[TOC] ","date":"2016-05-05","objectID":"/columns/reading-except.html:0:0","tags":null,"title":"阅读摘抄","uri":"/columns/reading-except.html"},{"categories":null,"content":"2016-05 被克服的困难就是胜利的契机（契诃夫） 于千万人之中，遇见你要遇见的人。于千万年之中，时间无涯的荒野里，没有早一步，也没有迟一步，遇上了也只能轻轻地说一句：你也在这里吗？（张爱玲） 如果你不调戏女人，她说你不是一个男人；如果你调戏她，她说你不是一个上等人。（张爱玲） ","date":"2016-05-05","objectID":"/columns/reading-except.html:1:0","tags":null,"title":"阅读摘抄","uri":"/columns/reading-except.html"},{"categories":null,"content":"2018-11 我是个俗气至顶的人. 见山是山, 见海是海, 见花便是花. 唯独见了你. 云海开始翻涌, 江潮开始澎湃, 昆虫的小触须挠着全世界的痒. 你无需开口, 我和天地万物便通通奔向你. – 你听过哪些让人心呯呯跳的情话亦或者诗句 ","date":"2016-05-05","objectID":"/columns/reading-except.html:2:0","tags":null,"title":"阅读摘抄","uri":"/columns/reading-except.html"},{"categories":null,"content":"2019-01 具有远大抱负，不但应该有改变自己的意愿，更加应该具备改变世界的情怀。 ","date":"2016-05-05","objectID":"/columns/reading-except.html:3:0","tags":null,"title":"阅读摘抄","uri":"/columns/reading-except.html"},{"categories":["随笔"],"content":"很多时候发现自己过于浮躁，一些简单的事都不能好好的完成。总是一会看看这里，一会又做做那里，最后什么也没有做。或者是整天都循环的、重复的看一些无用的内容，没有认真的去思考，也没有深入的去理解。这个时候，我想自己更应该静下心来，好好做一个规划，然后认真的实行。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:0:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"学会工作 工作应该全身心的去投入，不要让其他的事情干扰你。 坚持去克服存在的问题，不要逃避，勇敢面对。 工作应总结经验，学会使工作更有效率。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:1:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"学会生活 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"阅读 坚持阅读，阅读时间坚持1.5h以上。 阅读来源主要是简书等上的优质文章和其他技术性、科技资讯等文章。 读文章一定要仔细品读，认真思考，切勿囫囵吞枣。 积极参与评论，做好阅读笔记。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:1","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"写作 发表自己的看法和观点，积极参与其中。 收集一些自己认为好的文章，加以评论。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:2","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"学习 制定好学习计划，循序渐进，阶段性学习。 做好学习笔记。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:3","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"能力培养 积极尝试新的事物，迎接新的挑战。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:4","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"运动 身体是革命的本钱！ ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:5","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"最后几点忠告 善于用脑。 计划和规划。 平心静气，不骄不躁。 相信你会明白，工作是一种状态，学习是另一种状态。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:3:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"}]