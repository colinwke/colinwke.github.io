[{"categories":["data"],"content":"使用DataFrame直接创建hive表, 并作为其中的一个分区数据 ","date":"2023-04-27","objectID":"/posts/2304271038-dataframe-create-hive-table.html:0:0","tags":["data"],"title":"save DataFrame as partition of hive table","uri":"/posts/2304271038-dataframe-create-hive-table.html"},{"categories":["data"],"content":"test 1 table .write .format(\"hive\") .mode(\"overwrite\") .option(\"path\", inputPath + \"_table\") .insertInto(tableName) error, 需要先创建表 Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Table not found: hdp_lbg_ectech_ads.zp_compensate_ad_detail_test1; ","date":"2023-04-27","objectID":"/posts/2304271038-dataframe-create-hive-table.html:0:1","tags":["data"],"title":"save DataFrame as partition of hive table","uri":"/posts/2304271038-dataframe-create-hive-table.html"},{"categories":["data"],"content":"test 2 首先判别表是否存在 def checkTableExist(tableName: String): Boolean = { // https://www.mikulskibartosz.name/save-spark-dataframe-as-partitioned-hive-table/ val tableDbName = tableName.split(\"\\\\.\") // Dog.print(tableName, tableDbName.mkString(\"..\")) SparkEnvir.ss.sql(s\"SHOW TABLES IN ${tableDbName(0)}\") .where(col(\"tableName\") === tableDbName(1)) .count() == 1 } 表存在则直接插入, 表不存在则创建 ss.sql(\"SET hive.exec.dynamic.partition=true\") ss.sql(\"SET hive.exec.dynamic.partition.mode=nonstrict\") ss.sql(\"SET hive.exec.max.dynamic.partitions.pernode=400\") val writer: DataFrameWriter[Row] = table .write .mode(\"append\") // overwrite .format(\"parquet\") // difference of hive and parquet(different with format(SERDE, INPUT, OUTPUT)) .option(\"path\", outputTablePath) // external table val tableExist = SparkUtils.checkTableExist(tableName) if (tableExist) { Dog.print(s\"[insertInto(tableName)] ${tableName}\") writer.insertInto(tableName) } else { Dog.print(s\"[saveAsTable(tableName)] ${tableName}\") writer .partitionBy(\"dt\") .saveAsTable(tableName) } 这里创建表时没有问题的, path下会根据partition自动生成路径, path/partition_name=xx 但是insertInto(tableName)一直在hanging状态, 这里还未排查出原因, 怀疑是external path的问题? 这里另外需要注意的是, 创建表需要partition信息, 而insertInto就不需要partition信息了, 会根据表名的metadata获取partition信息. 参考 ","date":"2023-04-27","objectID":"/posts/2304271038-dataframe-create-hive-table.html:0:2","tags":["data"],"title":"save DataFrame as partition of hive table","uri":"/posts/2304271038-dataframe-create-hive-table.html"},{"categories":["data"],"content":"test 3(worked) 创建表使用saveAsTable, 添加分区数据使用parquet, 然后使用MSCK REPAIR TABLE ${tableName}更新数据路径的映射. ss.sql(\"SET hive.exec.dynamic.partition=true\") ss.sql(\"SET hive.exec.dynamic.partition.mode=nonstrict\") ss.sql(\"SET hive.exec.max.dynamic.partitions.pernode=400\") val writer: DataFrameWriter[Row] = table .coalesce(1) .write .mode(\"append\") // overwrite .format(\"parquet\") // difference of hive and parquet(different with format(SERDE, INPUT, OUTPUT)) .option(\"path\", outputTablePath) // external table .option(\"partitionOverwriteMode\", \"dynamic\") // .partitionBy(\"dt\") val tableExist = SparkUtils.checkTableExist(tableName) if (tableExist) { Dog.print(s\"[insertInto(tableName)] ${tableName}\") writer.parquet(outputTablePath) // writer.insertInto(tableName) } else { Dog.print(s\"[saveAsTable(tableName)] ${tableName}\") writer.saveAsTable(tableName) } ss.sql(s\"MSCK REPAIR TABLE ${tableName}\") val selectExample = s\"SELECT * FROM ${tableName}WHERE dt=${date}\" Dog.print(\"[selectExample] \" + selectExample) ss.sql(selectExample).show() SparkEnvir.close() ","date":"2023-04-27","objectID":"/posts/2304271038-dataframe-create-hive-table.html:0:3","tags":["data"],"title":"save DataFrame as partition of hive table","uri":"/posts/2304271038-dataframe-create-hive-table.html"},{"categories":["data"],"content":"其他 1 使用DDL生成建表sql def getCreateTableSql(df: DataFrame, tableName: String, partitioned: Array[String]): String = { // https://stackoverflow.com/a/69365683/6494418 val schemaDDL = df.schema.toDDL val partitionedStr = partitioned.map(_ + \" STRING\").mkString(\",\") val createTableSql = s\"CREATE TABLE ${tableName}(${schemaDDL}) PARTITIONED BY (${partitionedStr})\" Dog.print(\"[df.schema.fields] \" + df.schema.fields.mkString(\", \")) Dog.print(\"[create table sql] \" + createTableSql) Dog.print(\"[schemaDDL ] \" + schemaDDL) createTableSql } 2 format parquet 和format hive的区别 两者的区别在于序列化方式不同, 查看建表语句show create table tableName parquet ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' hive ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' 3 其他方式 之前使用DataFrame创建hive表的一个分区一直使用的是手动的, hivesql的 ---- CREATE TABLE IF NOT EXIST CREATEEXTERNALTABLEIFNOTEXISTS${table_name}(${col_normal_str})COMMENT\"create by wangke09\"PARTITIONEDBY(${col_partition_str})ROWFORMATDELIMITEDFIELDSTERMINATEDBY\"${fields_terminated}\"LINESTERMINATEDBY\"\\n\"STOREDASTEXTFILELOCATION\"${table_location}\";---- LINK DATA ALTERTABLE${table_name}ADDIFNOTEXISTSPARTITION(${partition_valued})-- partition valued and sep by(,), like dt='1', bg='2' LOCATION\"${partition_location}\";-- link partition_location to partition_valued(no need align to partition_valued like table_location/1/2) ---- DROP TABLE IF EXISTS ${table_name}; ---- SELECT * FROM ${table_name} WHERE ${partition_valued_and} ORDER BY RAND() LIMIT 500; 主要使用add partition去链接external data, 现在可以用更简单的方式创建表和连接表, 但是可能会有下面问题(未验证): external path的格式, 如果不是创建表的标准格式, MSCK REPAIR可能不能扫描(未测试), 这种情况只有通过add partition链接数据 parquet是一种序列化方法(类似json, hive?, csv, 等), 非文本格式, 不能直接使用hadoop -text 查看原数据 创建表不能添加comment, 需要自己修改metadata 4 是否可以直接使用parquet创建表? 5 NULL值处理 不处理null值检索全部列(*)貌似非常的慢, 最后做缺失值填充 ","date":"2023-04-27","objectID":"/posts/2304271038-dataframe-create-hive-table.html:0:4","tags":["data"],"title":"save DataFrame as partition of hive table","uri":"/posts/2304271038-dataframe-create-hive-table.html"},{"categories":["data"],"content":"json reader json没有 var rawData: DataFrame = ss.read.json(files: _*).repartition(numPartition).cache() ","date":"2023-04-27","objectID":"/posts/2304271038-dataframe-create-hive-table.html:1:0","tags":["data"],"title":"save DataFrame as partition of hive table","uri":"/posts/2304271038-dataframe-create-hive-table.html"},{"categories":["data"],"content":"refs https://www.jianshu.com/p/c1b0dc86f9b0 msck repair https://www.mikulskibartosz.name/save-spark-dataframe-as-partitioned-hive-table/ https://stackoverflow.com/a/31371571/6494418 https://stackoverflow.com/a/48251020/6494418 https://stackoverflow.com/a/45706191/6494418 [TOC] ","date":"2023-04-27","objectID":"/posts/2304271038-dataframe-create-hive-table.html:1:1","tags":["data"],"title":"save DataFrame as partition of hive table","uri":"/posts/2304271038-dataframe-create-hive-table.html"},{"categories":["column"],"content":"2023-06 ","date":"2023-03-11","objectID":"/columns/good-post.html:1:0","tags":["column"],"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":["column"],"content":"雷军武大演讲: 相信自己, 每个人的人生都有无限可能! https://mp.weixin.qq.com/s/poKFingit1zMAf-tkbvLOw 相信自己, 每个人的人生都有无限可能; 不仅要有梦想, 更要有一步一步可实现的目标. ","date":"2023-03-11","objectID":"/columns/good-post.html:1:1","tags":["column"],"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":["column"],"content":"2023-03 ","date":"2023-03-11","objectID":"/columns/good-post.html:2:0","tags":["column"],"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":["column"],"content":"第一次在九届全国人大会议的记者招待会上回答中外记者提问 https://www.youtube.com/watch?v=6N5vEbhODdY ","date":"2023-03-11","objectID":"/columns/good-post.html:2:1","tags":["column"],"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":["column"],"content":"总理华语群众大会演讲完整视频（National Day Rally 2019 - Chinese) https://www.youtube.com/watch?v=d6SOV4FX1DE 像新加坡这样的\"小\"国家, 甚至可以简单的看做一个非常大的公司了, 但是也有很多的特别之处: 要建立归属感, 让人民能效忠自己的国家 国家领导和人民同舟共济, 在大的问题上应保持一致的态度, 互相支持工作 要寻求出路, 经济, 外交, 等等. 尤其在经济上, 新增各行各业的培训, 不断学习, 寻求和激发新的思路, 寻求新的增长点 最后举的例子很有意思, 指向了多个社会问题 餐饮行业就业问题 企业发展问题(培训, 学习) 生育问题 李显龙总理很亲切!! ","date":"2023-03-11","objectID":"/columns/good-post.html:2:2","tags":["column"],"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":["column"],"content":"考古 阅读推荐2021 [TOC] ","date":"2023-03-11","objectID":"/columns/good-post.html:3:0","tags":["column"],"title":"阅读推荐","uri":"/columns/good-post.html"},{"categories":["column"],"content":"2023-10 ","date":"2023-01-10","objectID":"/columns/reading-note.html:1:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"线上多参数调整 预估排序 https://www.infoq.cn/article/nozs4xy7bvbcf34vzhhu 复杂多目标: Ensemble Sort 和在线自动调参 https://zhuanlan.zhihu.com/p/441117034 推荐系统中的多任务学习与多目标排序工程实践(下) https://tech.meituan.com/2015/12/07/rerank-solution-offline.html 美团O2O排序解决方案——线下篇 https://help.aliyun.com/document_detail/419538.html 推荐全链路 / 深度定制开放平台 / 选型指导 https://juejin.cn/post/7210310775276519484 重排在快手短视频推荐系统中的应用\u0026手淘信息流多兴趣多目标重排技术 https://aws.amazon.com/cn/blogs/china/optimization-practice-of-the-ranking-model-of-the-recommendation-system-series/ amazon https://mx-1024.github.io/posts/2020/02/09/62333/ 推荐系统排序优化迭代的一些经验 https://zhuanlan.zhihu.com/p/500237779 推荐系统多目标优化专题(2)—融合公式设计思路 ","date":"2023-01-10","objectID":"/columns/reading-note.html:1:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"2023-08 ","date":"2023-01-10","objectID":"/columns/reading-note.html:2:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"围绕 transformers 构建现代 NLP 开发环境 https://mp.weixin.qq.com/s/quyUMyFyoYYub0MJ5EoWVQ 5⭐️ 围绕transformers构建现代NLP开发环境 -\u003e 围绕huggingFace构建个性化的NLP开发环境 本文首先介绍了如何重写部分代码, 实现huggingFace个性化(存储, 方法)兼容; 然后是如何基于其他插件进行整个pipeline的管理, 而这个whole pipeline已经具有software2.0的雏形(或者叫形态). 什么是软件2.0 他预⾔, 为了实现软件2.0, 需要⼀整套服务于深度模型开发的⼯具栈, 就像传统软件需要 pip, conda 这类包管理器(package manager), GDB 这类 debug ⼯具, Github 这类开源社区⼀ 样, 深度学习也需要模型 debug ⼯具, 模型和数据集的管理器和开源社区. 但他没有预⾔到的是, 类transformer 架构在之后的年⽉⾥⼤放异彩, 不仅统⼀了 NLP 领域, ⽽ 且 正 在 逐 步 统 ⼀ CV 等 其 他 领 域 . \" 预 训 练 - 微 调 \" 成 为 业 界 最 常 ⻅ 的 模 型 开 发 范 式 . ⽽ huggingface 公 司 , 借 着 transformers 库 的 东 ⻛ , 以 及 围 绕 它 建 设 的 模 型 开 源 社 区 (huggingface hub), 成为当前 NLP 开发事实上的标准, 连⽬前最⽕的⼤模型, 都选择在 huggingface 发布, 例如 ChatGLM, 这些已经有了软件 2.0 的雏形. 最后的问题可谓这篇文章的点睛之笔 随着软件 2.0 ⼯具栈的成熟, 算法开发流程将逐步标准化, ⼯程化, 流⽔线化, 不仅⼤量⾮科班 的玩家都能⽤ LoRA 微调⼤模型, ⽤ diffusers ⽣成⼈物了, 甚⾄连 AI 都能开发软件了, 那么在未来, “算法⼯程师” 这个 title 会变成什么呢, 会调⽤ import transformers 算不算懂 NLP 呢? 这道题作为课后练习, 留给同学们进⾏思考! 对这个问题的思考: 从现有的工作环境中, 包含有AI平台工程, 我想就是把算法开发流程标准化, 工程化, 流水化的工作. 智能类的AI应用且基于算法手段做优化的仍然是算法工程师的主要工作范畴, 算法工程师就是针对某一目标使用某种(算法)手段做最优化的一群人, 对某一目标做优化除了使用现有的软件2.0之外, 还涉及到数据优化, 模型优化, 目标优化等算法范畴的工作. 我们再来看看huggingFace(https://huggingface.co/models) The AI community building the future. The platform where the machine learning community collaborates on models, datasets, and applications. ","date":"2023-01-10","objectID":"/columns/reading-note.html:2:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"LLM https://zhuanlan.zhihu.com/p/589747432 ChatGPT内核：InstructGPT，基于反馈指令的PPO强化学习 ","date":"2023-01-10","objectID":"/columns/reading-note.html:2:2","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"多场景/多目标 https://zhuanlan.zhihu.com/p/639351273 多目标 or 多场景? – 推荐系统的细枝末节(第四章) https://zhuanlan.zhihu.com/p/580951049 动态权重在推荐系统中的应用 https://zhuanlan.zhihu.com/p/472726462 poso https://mp.weixin.qq.com/s/GdB4le5ZSFHAAETHYxbJSA POSO方法的实际应用和分析思考 POSO: Personalized Cold Start Modules for Large-scale Recommender Systems https://t.zsxq.com/10fQsAzPr 知识星球链接 ","date":"2023-01-10","objectID":"/columns/reading-note.html:2:3","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"2023-07 ","date":"2023-01-10","objectID":"/columns/reading-note.html:3:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"profile 性能分析 https://mp.weixin.qq.com/s/RKqmy8dw7B7WtQc6Xy2CLA ","date":"2023-01-10","objectID":"/columns/reading-note.html:3:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"2023-06 ","date":"2023-01-10","objectID":"/columns/reading-note.html:4:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"依赖注入inject与解耦 深深耦合在一起了, 如果要换手机, 他就要拿起刀来改造自己, 把自己体内所有方法中的iphone6 都换成 iphoneX 我也从其中获得了这样的感悟: 如果一个类A 的功能实现需要借助于类B, 那么就称类B是类A的依赖, 如果在类A的内部去实例化类B, 那么两者之间会出现较高的耦合, 一旦类B出现了问题, 类A也需要进行改造, 如果这样的情况较多, 每个类之间都有很多依赖, 那么就会出现牵一发而动全身的情况, 程序会极难维护, 并且很容易出现问题. 要解决这个问题, 就要把A类对B类的控制权抽离出来, 交给一个第三方去做, 把控制权反转给第三方, 就称作控制反转(IOC Inversion Of Control). 控制反转是一种思想, 是能够解决问题的一种可能的结果, 而依赖注入(Dependency Injection)就是其最典型的实现方法. 由第三方(我们称作IOC容器)来控制依赖, 把他通过构造函数, 属性或者工厂模式等方法, 注入到类A内, 这样就极大程度的对类A和类B进行了解耦. Injector对象是Guice的核心, 它负责创建和管理对象的生命周期 ","date":"2023-01-10","objectID":"/columns/reading-note.html:4:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"北极星指标图 https://www.woshipm.com/operate/3291568.html https://www.woshipm.com/operate/1072916.html 北极星指标也叫唯一关键指标, 产品现阶段最关键的指标, 其实简单说来就是公司制定的发展目标, 不同阶段会有不同的目标. 为什么叫\"北极星\"指标, 其实大概的寓意就是要像北极星一样指引公司前进的方向, 目标制定最好是能符合SMART原则. ","date":"2023-01-10","objectID":"/columns/reading-note.html:4:2","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"2023-05 ","date":"2023-01-10","objectID":"/columns/reading-note.html:5:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"你真的懂点击率(CTR)建模吗? https://mp.weixin.qq.com/s/p13DHPO59d4a_LWKV41lrg ctr模型性能评价的两个维度: 序的准确性 auc/gauc 值的准确性 copc/logloss 模型训练的时候是基于全空间坐标系, 评估可能是一个子坐标系, 存在偏差 校准本质上是在基础模型之上, 引入了第二个学习目标. 基础模型的目标是在给定解空间拟合数据, 最大化AUC(序的准确性); 校准模型的目标是在后验的统计意义上调整pCTR值的大小, 使得预测值尽可能逼近观测到的统计值(值的准确性). 然而, 这种两段式建模方式, 虽然第二阶段的校准可以尽量保序, 从而不影响模型的AUC表现, 但两段式建模非最优. 因为基础模型预测的pCTR分布代表了模型对数据的归纳; 现在既然已经知道有特定维度上这个归纳不准确, end-2-end联合建模显然能够触及更高的天花板. 这个方向的工作我们团队正在推进. 这里说的end2end的方法是否可以是把子空间作为特征, 然后做多场景多任务学习, 即把子空间作为一个场景? 其他参考 https://zhuanlan.zhihu.com/p/262877350 推荐系统采样评估指标及线上线下一致性问题 https://blog.csdn.net/u013019431/article/details/102473137 ctr预估中的评估指标及校准 ctr预估不仅要保序, 即正负样本排序好, 还需要保距, 即pctr之间的比值关系与真实ctr的比值也基本相等, 这也是ctr预估中最难的地方. 怎样的模型是个好的ctr预估模型? 最理想的模型当然是开了上帝之眼, 预估为5%点击的流量群统计后也是5%, 即在各个流量上的预估都非常准. 这里就是各个场景的统计 logloss主要用于评估模型输出概率与训练数据的概率的一致程度 保序回归: 在不改变模型输出auc的情况下根据样本调整每个pctr区间内的预估值, 有点像前面copc部分, 为每个细分流量都进行校准从而保证模型输出在有序的基础上还能保距. 其他思考 各个位置模型/排序策略的效果差异 是否还是预估越准, ecpm就越高, 这里还需要考虑其他排序因子对序的影响 先提升各个场景的准确率(多场景), 再提升模型的准确率(多任务, sim, can) 2023-06-08 update 点击率预估的目标维度我想是人, 而不是物品. 所以在正样本中, 丢弃掉没有点击(非冷启动)用户的样本(没有指导意义的样本)是业务上正确的选择, 因为你还是预估的是有点击(正常用户)对于这些帖子的点击率 … ","date":"2023-01-10","objectID":"/columns/reading-note.html:5:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"CAN(Co-Action Network) https://mp.weixin.qq.com/s/XFXBgMhf5TGCprF46kOOJg 点击率模型特征交叉方向的发展及CAN模型介绍 https://github.com/CAN-Paper/Co-Action-Network can code https://github.com/UIC-Paper/MIMN mimn code https://github.com/mouna99/dien dien code https://github.com/tttwwy/sim sim code https://github.com/ruozhichen/deep_learning/tree/master mmoe https://github.com/ShowMeAI-Hub/multi-task-learning/tree/main https://github.com/qiaoguan/deep-ctr-prediction https://github.com/openbenchmark/BARS scholar TODO(整理迭代路径) https://scholar.google.com/scholar?hl=en\u0026as_sdt=0%2C5\u0026q=click-through+rate+prediction https://scholar.google.com/citations?user=PXO4ygEAAAAJ gaikun https://scholar.google.com/citations?user=eUMnOc0AAAAJ zhuxiaoqiang https://scholar.google.com/citations?user=n_E0Bg4AAAAJ zhouguorui https://scholar.google.com/citations?user=_LnryAgAAAAJ piqi https://scholar.google.com/citations?user=3Gq0df8AAAAJ mouna https://dblp.org/pid/205/2948.html bianweijie https://github.com/wjbianjason bianweijie 可以实践 ","date":"2023-01-10","objectID":"/columns/reading-note.html:5:2","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"2023-04 ","date":"2023-01-10","objectID":"/columns/reading-note.html:6:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"如何评价 OpenAI 的超级对话模型 ChatGPT ？ https://www.zhihu.com/question/570189639/answer/2804667785 ChatGPT如何解决 “AI校正(Alignment)“这个问题 感觉驱动器: Alignment就是对模型的校正，使其精度更高。 至于如何进行alignment，总体来说就是人工干预，防止模型自主学习过程中的“好坏通吃”。 就相当于你小时候吃东西（样本），什么都吃，不知道什么是好是坏，这时候父母一方面告诉你什么是好的食物，什么是坏的食物（强监督式学习），另一方面惩罚你吃垃圾食品的行为，奖励你吃健康食品的行为（强化学习）。(RLHF) ","date":"2023-01-10","objectID":"/columns/reading-note.html:6:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"从腾讯离职了，四年工作总结 https://mp.weixin.qq.com/s/s0hnAdLHIQYxUXQV4aIJLA 普普通通吧, 虽然没有醍醐灌顶, 但是怎么也会有可借鉴之处. 前面一段时间, 和同事闲谈了一个结论, 就是不要替老板操心, 更不能也不可能替老板做决策. 我们能做的是做好自己, 实现自己的价值与在公司的价值. 也即精进自我专业技能, 提升自我核心竞争力, 这样才能无论到哪里, 都有口饭吃. 接受自己的平凡, 接受自己的选择带来的结果, 但也并非是不思进取, 更需要思考在接受的同时怎么能做的更好. 技术上要不断学习, 开拓眼界, 增强自我竞争力(技术沉淀, 技术分享), 注重方法论(系统的, 规范的, 文档化的, 专业的) 加油吧, 已不再是少年. ","date":"2023-01-10","objectID":"/columns/reading-note.html:6:2","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"2023-03 ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"多场景多任务学习在美团到店餐饮推荐的实践 https://mp.weixin.qq.com/s/v-GN1ors-bWutfsIJcZYbQ ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"快手精排模型实践 https://mp.weixin.qq.com/s/SsgVqei9sL5y7N1GUXOJLg ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:2","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"从用户体验洞察到商业价值变现，以京东为例 https://mp.weixin.qq.com/s/NG39SvW4TGlU0_4I6TfJdQ 焦文健 京东 技术与数据中心 数据产品专家 京东零售数据产品专家, 十年大数据和算法智能领域的产品经验, 在客户体验管理, 用户洞察方面有丰富经验, 擅长通过数智化手段助力业务创新. 业务型算法工程师应当具备较好的数据能力, 产品能力, 来解决业务中的问题. 所以这类文章虽然读起来仍然会发现和职业的产品经理, 数据分析师有一定的差距, 但是整体来说还是非常受益的, 毕竟业务型算法工程师是个全栈更好[哭笑]. 最近一段时间一直在思考技术驱动和产品驱动的问题, 他们的目标基本上是一致的, 都是以一定的方法去解决业务上的某一问题, 只不过解决的方法论可能不同. 作为算法工程师, 可以多从技术角度思考如何解决问题, 或者在现有的产品上如何去解决问题, 这就设计到数据分析能力和问题建模能力了. 抛开职能的讨论, 回到这篇文章讨论问题: 如今移动互联网基本饱和, 如何从增量时代向存量时代的转变过程总, 有哪些机遇及挑战, 以及经营思路的转变? 1 这类问题可能有历史可参照 虽然互联网是还算新兴行业, 但是对于某项新型技术或者产品来说, 他们发展可能都有所参见的. 对于竞争问题, 文章就举例了美的和格兰仕的微波炉之争, 及最后美的如何从泥潭中完成自救与扭亏为盈, 其主要不在价格的竞争, 价格的竞争只会两败俱伤, 或者被反垄断请喝茶, 而是从跟随策略向差异化策略的转变, 持续的以用户\u0026客户的产品创新, 在产品上更具有竞争力(参见boss直聘). 对于存量与增量问题, 文章又举例了添可的品类创新带来的增量, 所以持续的品类创新带来持续的增量? (目前来见, 电子行业多数是的(手机, 相机), 但是很多行业可能已经遇到天花板了, 又如何的创新?) 另一个是微笑曲线 微笑曲线的两端, 一个是面向消费者的体验管理, 存量时代已无法仅靠投广告拉流量来 保持增长, 而是应该注重提升消费者体验; 另一个是产品的创新, 通过前文中的实例已经可以看 到产品差异化是可以带来显著增长的. 2 如何做好用户体验洞察 主要在于用户/客户需求的精细化识别, 包含画像/行为/认知的洞察. 理解用户/客户的需求是第一要务: 站在平台角度(三方:用户/客户/平台): 业务存在的意义是解决用户\u0026客户的需求 所以理解用户\u0026客户是当下的第一要务 站在BC角度: BC角度没有平台的参与, 所以理解的只有用户(客户是自身) 在传统的行为分析(是否达成服务, 基于行为日志数据)的基础上, 可以增加一些心智分析(达成服务的质量, 基于文本数据), 这些对于算法工程师来讲都是easy的. 最后一点是数据产品经理的自我修养. 我理解算法工程师还是可以持续在算法上深入, 可以通过一些case去打开思路, 至于系统的数据分析/产品管理的学习, 需要自己衡量其在实际工作中的重要程度了, 如果主要仍是算法方向, 就得不偿失了. ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:3","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"复旦邱锡鹏：深度剖析 ChatGPT 类大语言模型的关键技术 https://mp.weixin.qq.com/s/S8gPrKzvzpYH1pCJCzGyzA 通常以百亿/千亿级参数量作为 LLM 研究的分水岭. 大规模语言模型也被看作是实现通用人工智能(AGI)的希望. ChatGPT 的三个关键技术为: 情景学习, 思维链, 自然指令学习 情景学习(In-context learning): 对于一些 LLM 没有见过的新任务，只需要设计一些任务的语言描述，并给出几个任务实例，作为模型的输入，即可让模型从给定的情景中学习新任务并给出满意的回答结果。这种训练方式能够有效提升模型小样本学习的能力。(QA: 这个和word2vec所具有的距离关系是否相似? 模型学习的的知识具备一定的数据结构, 而这个数据结构是通用的, 可以通过部分少数描述来组合这种数据结构?) 思维链(Chain-of-Thought, CoT): 对于一些逻辑较为复杂的问题，直接向大规模语言模型提问可能会得到不准确的回答，但是如果以提示的方式在输入中给出有逻辑的解题步骤的示例后再提出问题，大模型就能给出正确题解。也就是说将复杂问题拆解为多个子问题解决再从中抽取答案，就可以得到正确的答案。由于 CoT 技术能够激发大规模语言模型对复杂问题的求解能力，该技术也被认为是打破比例定律的关键。 自然指令学习(Learning from Natural Instructions): 早期研究人员希望把所有的自然语言处理任务都能够指令化，对每个任务标注数据。这种训练方式就是会在前面添加一个“指令”，该指令能够以自然语言的形式描述任务内容，从而使得大模型根据输入来输出任务期望的答案。该方式将下游任务进一步和自然语言形式对齐，能显著提升模型对未知任务的泛化能力。(QA: 指令对于多任务模型来说, 就是监督的label(task), 但是对于LLM模型来说, 他也可以成为label下的input, 所以已经弱化了多任务学习, 而是学习一个task, 但是这个task是复合的) 人们把 NLP 任务做到 1000 多种，目前最新模型可以做到 2000 多种 NLP 任务，接下来再对 NLP 任务进行分类，比如能力 A、能力 B，大模型指令能力、泛化能力非常强，学到四五十个任务时就可以泛化到上百种任务。但距离真正的 ChatGPT 还有一步，那就是和真实的人类意图对齐，这就是 OpenAI 做的 GPT。核心逻辑非常简单，一开始时让人写答案，但是成本太高，改成让人来选答案，这样对标注员的能力要求稍微低一点，可以迅速提升迭代和规模。基于打分再训练一个打分器，通过打分器自动评价模型的好坏，然后用强化学习开始迭代，这种方法可以大规模地把数据模型迭代给转起来，这是 OpenAI 做的 Instruct GPT 逻辑，强化学习的人类反馈(RLHF)。 总结: LLM模型是走向AGI的一个途径, 在图片领域飞速发展后, NLP也迎来了快速发展(Transformer, Bert, GPT), 文本和图像因为数据结构的特殊性, 在模型训练上有更多的挑战和可能. 另外, GPT的应用在特定的垂直领域, 仍然是一个发展的方向. ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:4","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"GPT-4: 一张手绘草图能生一个网站, 60 秒搞定一个游戏开发! https://mp.weixin.qq.com/s/wNQK_vPGhj5YOVVRLPmF_g “我们花了 6 个月的时间, 利用对抗性测试项目以及 ChatGPT 的经验, 反复调整 GPT-4, 结果在真实性, 可操作性以及拒绝超出道德等边界方面取得了有史以来最好的结果(尽管远非完美)”, OpenAI 分享道. 微软的 Azure 与 OpenAI 共同设计了一台超级计算机, 搭载了英伟达上万颗 A100 芯片. 基于这台超算, OpenAI 在一年前将 GPT-3.5 作为系统的第一次\"测试运行\"进行了训练, 在修复一些 Bug 并提高理论基础之上, OpenAI 基于此又训练出了首个能够提前准确预测其训练性能的大模型 GPT-4. OpenAI 表示, 他们的目标是开发能够在任何智力任务方面取代人类的 AGI(通用人工智能), 尽管 GPT-4 还没有达到这个目标. 尽管有这样的能力, GPT-4 与早期的 GPT 模型有类似的局限性: 它不是完全可靠的, 比如, 它也会对事实产生\"幻觉\", 推理出错误的结果. 对此, OpenAI 首席执行官 Sam Altman 在推特上说: “它(GPT-4)仍然有缺陷, 仍然有局限性, 但它在首次使用时仍然能给你留下深刻印象. \" https://mp.weixin.qq.com/s/wkTciKHC8jmYacvp3lg8xA GPT-4测试案例 ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:5","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"OpenAI 发布多模态 GPT-4 模型, 会开创哪些新的研究方向? https://www.zhihu.com/question/589640227 @张俊林 https://www.zhihu.com/question/589640227/answer/2937925226 竞争关系和训练集(模型)缺陷可能导致LLM封闭化发展 训练的高效性仍然是LLM的一个方向(例如模型蒸馏后的小模型) 强化学习仍然是固有数据集的一种增强方式, 可以让模型更加的鲁棒 相关阅读 https://www.zhihu.com/question/589639535 https://www.zhihu.com/question/584515782 ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:6","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"SIGIR'22 | 大规模推荐系统中冷启动用户预热的融合序列建模 对整个用户向量进行建模, 而不是对用户id进行建模 基于DSSM模型 正负反馈融合建模, 利用充分的负反馈增加模型的信息 交互数据中的二八原则, 是否是二的用户贡献了八的交互 https://mp.weixin.qq.com/s/SmFYTlhYZQr3Tf3Km8cg2g ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:7","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"新时期的阿里妈妈广告引擎 https://zhuanlan.zhihu.com/p/523549804 讲的还算全面, 查漏补缺, 主要是对整个服务架构的由全图化架构(服务化?)到serverless化架构, 主要内容有: 整体架构上serverless 特征抽取, 业务算子; 数据规范, 抽象, 规范; 统一框架, 抽象, 接口, 配置化(数据+算子+模型) serverless就是在原有全图化的基础上, 再次做拆分和抽象, 实现服务化的拆分为去服务化 召回架构 智能出价 基于公式的出价: 业务整体的限制, 可个性化不高, 业务限制大, 迭代成本高, 尝试的空间不高 基于模型的出价有更大的个性化空间, 更大的参数空间, 最优化能力更强. 智能创意 这里创意可以在b端客户发帖时建议多个标题, 供客户筛选, 而线上则简单的选取创意模型保证创意的可控(创意工具化) ","date":"2023-01-10","objectID":"/columns/reading-note.html:7:8","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"2023-02 ","date":"2023-01-10","objectID":"/columns/reading-note.html:8:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"Java生态圈技术框架,中间件,系统架构汇总 https://juejin.cn/post/6844903620979212296 2018年的汇总文章, 但是对java开发的整体框架有一个较好的总结 ","date":"2023-01-10","objectID":"/columns/reading-note.html:8:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"快手精排模型实践 https://mp.weixin.qq.com/s/SsgVqei9sL5y7N1GUXOJLg 业务特点, 时序的重要性, 可能RNN, transformer等不及sum pooling, 那么就需要思考是否是相关性大于时序性了 推荐的相关性和多样性如何保证? ","date":"2023-01-10","objectID":"/columns/reading-note.html:8:2","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"阿里新一代Rank技术 https://mp.weixin.qq.com/s/5k2Clf6e_Xnx3Tuedtst0g 阿里巴巴深度学习迭代路径 DIN(18) 多峰兴趣建模 DIEN(19) 多峰兴趣漂移\u0026演化建模 MIMN(19) 长期兴趣建模 SIM(20) life-long兴趣建模 注意: 这一系列一直致力于用户兴趣建模 SIM: GSU/ESU GSU: general search: actioned embedding distance ESU: exact search: cate tree etc. ","date":"2023-01-10","objectID":"/columns/reading-note.html:8:3","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"算法工程师的\"天地之间” https://zhuanlan.zhihu.com/p/495479206 系统架构是天, 数据细节是地, 天地之间, 建模能力和技术能力才是算法工程师的术与道 系统架构是天, 业务理解是地, 建模能力和工程技术才是算法工程师的核心 ","date":"2023-01-10","objectID":"/columns/reading-note.html:8:4","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"待读 冷启动 https://zhuanlan.zhihu.com/p/377229717 https://zhuanlan.zhihu.com/p/548297540 检索 https://theory.stanford.edu/~sergei/papers/vldb09-indexing.pdf https://zhuanlan.zhihu.com/p/591048449 https://www.zhihu.com/question/68232124/answer/2498391364 https://www.zhihu.com/people/leon_zju/posts?page=1 多场景\u0026多目标 https://github.com/shenweichen/DeepCTR https://github.com/shenweichen/DeepMatch https://mp.weixin.qq.com/s/Nm5sMkeYJmr-zShVY1WoPQ 一文梳理业界多场景多目标精排建模方案 https://mp.weixin.qq.com/s/4FRc-keU_4H8ZCYiKftqaA 多场景多任务推荐方法汇总 https://mp.weixin.qq.com/s/0GfIdgnYvQWANTiLh2qH3A 多目标推荐场景下的深度学习实践(58罗景) chatGLM https://mp.weixin.qq.com/s/kNXAuCiX4I7Tj4iwZSTP2Q 保姆级部署, 但是没有fineturn https://mp.weixin.qq.com/s/JDd7aSZRHIPtJWlSRevzlg 一篇功能介绍和测评的, 还有与其他大模型的对比 https://mp.weixin.qq.com/s/51EtMK4SQLR4TvYDi_Uxxw fineturn 推荐 https://mp.weixin.qq.com/s/gbdV0L2bD-8ToXVGEKhdpQ 简单部署+论文解读 https://mp.weixin.qq.com/s/ob0EBqiTANlePKrDC9_uQA 部署(这个挺详细的) lora ","date":"2023-01-10","objectID":"/columns/reading-note.html:9:0","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["column"],"content":"UNSORT https://zhuanlan.zhihu.com/p/651080682 KDD 2023 | 搜广推相关论文集锦 https://www.zhihu.com/question/515459299/answer/2519427121 羽毛球掌握什么原理后让你球技大涨? [TOC] ","date":"2023-01-10","objectID":"/columns/reading-note.html:9:1","tags":["column"],"title":"阅读笔记","uri":"/columns/reading-note.html"},{"categories":["unsort"],"content":"问题描述 原来在/etc/hosts配置了github的ip解析, 突然有一天push很慢, 甚至经常timeout. 然后自己把hosts中关于github的映射都删除了, 但是链接不上了github了, 自己有用飞机软件. ","date":"2022-11-03","objectID":"/posts/2211031619-ssh-git-github-permission-denied-problem.html:1:0","tags":["git","github","ssh"],"title":"ssh git github permission denied problem","uri":"/posts/2211031619-ssh-git-github-permission-denied-problem.html"},{"categories":["unsort"],"content":"具体表象 重新生成了ssh-key, 也配置到github上了, 使用ssh -T git@github.com验证时要求输入密码, 但是任何密码都permission denied. $ ssh -T git@github.com The authenticity of host 'github.com (10.252.108.57)' can't be established. ED25519 key fingerprint is SHA256:XXXXXXXXXCssXNTIE. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'github.com' (ED25519) to the list of known hosts. git@github.com's password: Permission denied, please try again. git@github.com's password: Permission denied, please try again. git@github.com's password: git@github.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password). 试了多种方法, 各种密码, 这一步一直验证失败 ","date":"2022-11-03","objectID":"/posts/2211031619-ssh-git-github-permission-denied-problem.html:1:1","tags":["git","github","ssh"],"title":"ssh git github permission denied problem","uri":"/posts/2211031619-ssh-git-github-permission-denied-problem.html"},{"categories":["unsort"],"content":"解决方案 首先在这里看到了dns pollution的概念, 然后具体搜索关键词dns pollution ssh -T git@github.com找到了解决方案, 自己配置github的ip解析. 我遇到的问题和解决方案略有区别, 我解析到的github的ip并不是本机ip(127.0.0.1), 而是另一个看着挺正常的ip. 具体的的解决方法, 查询github.com, ssh.github.com的ip地址, 然后配置到hosts 方法1: 使用ipaddress查询 方法2: 使用bash查询 nslookup github.com 8.8.8.8 | awk '{print $2}' | tail -n 3 | tr '\\n' ' ' | awk '{print $2,$1}' nslookup ssh.github.com 8.8.8.8 | awk '{print $2}' | tail -n 3 | tr '\\n' ' ' | awk '{print $2,$1}' 把查询的ip映射添加到hosts里面 sudo open -a “Sublime Text” /etc/hosts # github # https://juejin.cn/post/7103738143513247781 # nslookup ssh.github.com 8.8.8.8 | awk '{print $2}' | tail -n 3 | tr '\\n' ' ' | awk '{print $2,$1}' 20.205.243.166 github.com 20.205.243.160 ssh.github.com 现在使用验证ssh -T git@github.com成功通过! ","date":"2022-11-03","objectID":"/posts/2211031619-ssh-git-github-permission-denied-problem.html:1:2","tags":["git","github","ssh"],"title":"ssh git github permission denied problem","uri":"/posts/2211031619-ssh-git-github-permission-denied-problem.html"},{"categories":["unsort"],"content":"参考链接 https://juejin.cn/post/7103738143513247781 https://www.shouxicto.com/article/5947.html https://docs.gitlab.com/ee/user/ssh.html#generate-an-ssh-key-pair https://docs.github.com/cn/authentication/connecting-to-github-with-ssh https://docs.github.com/en/github-ae@latest/authentication/troubleshooting-ssh/error-permission-denied-publickey https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent https://stackoverflow.com/a/72236267/6494418 git push To igit.58corp.com:RecruitBusiness/AdRetrievalSys/lego_scf_zpdsp.git ! [rejected] 4.1.43_dev_wangke_2022-12-22 -\u003e 4.1.43_dev_wangke_2022-12-22 (non-fast-forward) error: failed to push some refs to 'igit.58corp.com:RecruitBusiness/AdRetrievalSys/lego_scf_zpdsp.git' hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details. [wangke@colin service] $ git pull hint: You have divergent branches and need to specify how to reconcile them. hint: You can do so by running one of the following commands sometime before hint: your next pull: hint: hint: git config pull.rebase false # merge hint: git config pull.rebase true # rebase hint: git config pull.ff only # fast-forward only hint: hint: You can replace \"git config\" with \"git config --global\" to set a default hint: preference for all repositories. You can also pass --rebase, --no-rebase, hint: or --ff-only on the command line to override the configured default per hint: invocation. fatal: Need to specify how to reconcile divergent branches. solution # https://stackoverflow.com/questions/20467179/git-push-rejected-non-fast-forward git fetch git rebase feature/my_feature_branch git push origin feature/my_feature_branch or # https://docs.github.com/en/get-started/using-git/dealing-with-non-fast-forward-errors git fetch origin git merge origin YOUR_BRANCH_NAME git pull origin YOUR_BRANCH_NAME new error [wangke@colin service] $ git pull origin 4.1.43_dev_wangke_2022-12-22 From igit.58corp.com:RecruitBusiness/AdRetrievalSys/lego_scf_zpdsp * branch 4.1.43_dev_wangke_2022-12-22 -\u003e FETCH_HEAD hint: You have divergent branches and need to specify how to reconcile them. hint: You can do so by running one of the following commands sometime before hint: your next pull: hint: hint: git config pull.rebase false # merge hint: git config pull.rebase true # rebase hint: git config pull.ff only # fast-forward only hint: hint: You can replace \"git config\" with \"git config --global\" to set a default hint: preference for all repositories. You can also pass --rebase, --no-rebase, hint: or --ff-only on the command line to override the configured default per hint: invocation. fatal: Need to specify how to reconcile divergent branches. solution git config --global pull.ff only git config pull.ff only git pull -a fatal: Not possible to fast-forward, aborting. # git pull --no-ff or git pull --rebase # then git push ","date":"2022-11-03","objectID":"/posts/2211031619-ssh-git-github-permission-denied-problem.html:2:0","tags":["git","github","ssh"],"title":"ssh git github permission denied problem","uri":"/posts/2211031619-ssh-git-github-permission-denied-problem.html"},{"categories":["unsort"],"content":"问题描述 使用scala开发了udaf, 在scala程序中能使用, 无法在pyspark中使用 使用udaf有两种方法: 第一种是hive使用 ss.sql(\"CREATE TEMPORARY FUNCTION MostFreq22 AS 'com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF' USING JAR '%s'\" % jar_path) # SparkConf未指定spark.jars # error # pyspark.sql.utils.AnalysisException: Can not load class 'com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF' when registering the function 'MostFreq22', please make sure it is on the classpath 当指定了spark.jars, 仍然报错 config(\"spark.jars\", jar_path) # IMPORTANT for java dependence jar # 22/10/21 15:16:08 WARN SparkContext: The jar /Users/wangke/working/bussiness_data_offline/python/pyspark/bussiness_data_offline_1.0.9_shaded.jar has been added already. Overwriting of added jars is not supported in the current version. # ... # pyspark.sql.utils.AnalysisException: Can not load class 'com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF' when registering the function 'MostFreq22', please make sure it is on the classpath 添加了下面参数, 仍然为找到classpath SparkSession.builder .appName(name=\"wktk_pyspark_env\") # basename(__file__) .config(\"spark.jars\", jar_path) # IMPORTANT for java dependence jar .config(\"spark.driver.extraLibraryPath\", jar_path) .config(\"spark.executor.extraLibraryPath\", jar_path) .config(\"spark.driver.extraClassPath\", jar_path) .config(\"spark.executor.extraClassPath\", jar_path) .enableHiveSupport() .getOrCreate()) 问题1: 为什么找不到classpath? 因为本地配置错误? 线上运行 SparkConf配置jar参数, sql不含ADD JAR ss.sql(\"\"\"CREATE TEMPORARY FUNCTION MostFreq22 AS 'com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF'\"\"\") # ERROR pyspark.sql.utils.AnalysisException: No handler for UDF/UDAF/UDTF 'com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF'; line 1 pos 7 猜想: handler 第二种为register ss.udf.registerJavaUDAF(\"MostFreq22\", \"com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF\") # ERROR pyspark.sql.utils.AnalysisException: Can not load class com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF, please make sure it is on the classpath 线上运行 # ERROR pyspark.sql.utils.AnalysisException: class com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF doesn't implement interface UserDefinedAggregateFunction ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:1:0","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"好像pyspark3.x和hive不支持Aggx pyspark.sql.udf.UDFRegistration.registerJavaUDAF https://github.com/apache/spark/blob/master/sql/core/src/test/java/test/org/apache/spark/sql/MyDoubleAvg.java def registerJavaUDAF(self, name, javaClassName): \"\"\"Register a Java user-defined aggregate function as a SQL function. .. versionadded:: 2.3.0 name : str name of the user-defined aggregate function javaClassName : str fully qualified name of java class Examples -------- \u003e\u003e\u003e spark.udf.registerJavaUDAF(\"javaUDAF\", \"test.org.apache.spark.sql.MyDoubleAvg\") ... # doctest: +SKIP \u003e\u003e\u003e df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"a\")],[\"id\", \"name\"]) \u003e\u003e\u003e df.createOrReplaceTempView(\"df\") \u003e\u003e\u003e q = \"SELECT name, javaUDAF(id) as avg from df group by name order by name desc\" \u003e\u003e\u003e spark.sql(q).collect() # doctest: +SKIP [Row(name='b', avg=102.0), Row(name='a', avg=102.0)] \"\"\" self.sparkSession._jsparkSession.udf().registerJavaUDAF(name, javaClassName) ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:1:1","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"scala调用 spark.sql(\"\"\"CREATE TEMPORARY FUNCTION MostFreq22 AS 'com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF' USING JAR '/Users/wangke/working/bussiness_data_offline/target/bussiness_data_offline_1.0.9_shaded.jar'\"\"\") # ERROR Exception in thread \"main\" org.apache.spark.sql.AnalysisException: No handler for UDF/UDAF/UDTF 'com.company.strategy.rank.bussiness.util.udf.MostFreqUDAF'; line 1 pos 7 ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:2:0","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"结论 hive和pyspark目前并不能直接register继承Aggregator实现的UDAF(version3.1), 只有java/scala支持, 且注册为spark.udf.register(\"MostFreq22\", functions.udaf(MostFreq)) hive是支持继承类UserDefinedAggregateFunction的 是否可以添加转化使得pyspark和hive支持Aggregator? 没有成功! object UDFHandler { def evaluate(): UserDefinedFunction = { functions.udaf(MostFreq) } } java_import(sc._gateway.jvm, \"com.company.strategy.rank.bussiness.util.udf.UDFHandler\") print(type(sc._gateway.jvm.com.company.strategy.rank.bussiness.util.udf.UDFHandler)) inst = sc._gateway.jvm.com.company.strategy.rank.bussiness.util.udf.UDFHandler() ss.udf.register(\"MostFreq22\", inst.evaluate) # ERROR py4j.Py4JException: Constructor com.company.strategy.rank.bussiness.util.udf.UDFHandler([]) does not exist ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:3:0","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"引用 ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:4:0","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"开发参考 https://spark.apache.org/docs/3.1.1/sql-ref-functions-udf-aggregate.html https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala https://stackoverflow.com/a/66819248/6494418 https://docs.databricks.com/spark/latest/spark-sql/udaf-scala.html https://blog.csdn.net/Bayerngu/article/details/118035754 handler? https://stackoverflow.com/questions/67776394/no-handler-for-udf-udaf-udtf-spark-udf3 handler by 继承 ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:4:1","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"待回答问题 https://stackoverflow.com/questions/73327627/spark-custom-aggregator-register-and-invoke-through-pyspark [TOC] ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:4:2","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"spark UDF different with hive UDF spark UDF的使用和hive UDF是不一样的 在spark中, 直接在SparkSession中注册(register)即可使用 在hive中, java/scala和python也略有不同 其中java/scala直接通过类进行注册, 然后直接构造函数进行使用 CREATETEMPORARYFUNCTIONTopFreqAS'com.bj58.strategy.rank.zhaopin.util.udf.TopFreq'USINGJAR'/Users/wangke/working/zhaopin_data_offline/target/zhaopin_data_offline_1.0.9_shaded.jar python看着略微复杂一些 addfilewasbs:///hiveudf.py;SELECTTRANSFORM(clientid,devicemake,devicemodel)USING'python hiveudf.py'AS(clientidstring,phoneLabelstring,phoneHashstring)FROMhivesampletableORDERBYclientidLIMIT50; 其中, python的实现为 import sys for line in sys.stdin: line = line.strip() (emp_id,emp_name) = line.split('\\t') print(emp_id + '\\t' + emp_name + '，亲') ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:5:0","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["unsort"],"content":"refs https://github.com/zpz/hive-udf http://xueai8.com/course/178/article https://cloud.tencent.com/developer/article/1571103 https://dp.58corp.com/data-develop/udf-manager https://spark.apache.org/docs/3.1.1/sql-ref-functions-udf-hive.html https://spark.apache.org/docs/latest/sql-ref-functions-udf-hive.html https://stackoverflow.com/a/52038422/6494418 https://learn.microsoft.com/en-us/azure/hdinsight/hadoop/python-udf-hdinsight 推荐 saprk的udf和hive的udf是区别的, hive的udf需要使用类进行注册(python使用文件) 而spark直接注册方法就可以了 ","date":"2022-10-21","objectID":"/posts/2210211456-pyspark-udf-udaf-jar.html:5:1","tags":["unsort"],"title":"pyspark udf udaf with jar","uri":"/posts/2210211456-pyspark-udf-udaf-jar.html"},{"categories":["column"],"content":"2022-12 ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:1:0","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["column"],"content":"机器学习工程方面的最佳做法 https://developers.google.com/machine-learning/guides/rules-of-ml ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:1:1","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["column"],"content":"你一生的故事: 不论怎样, 未来都会爱你 https://www.xiaoyuzhoufm.com/episode/63aa28e83c5c5f89defdd606 我们注定要失去, 所以我们可以用多重视角看待我们经历的事, 我们所面对的问题: 原因论, 追溯过去, 但过去无法改变, 我们会增加更多的负面情绪 目的论, 经历是为了什么, 为了历练我们的身心, 强大我们的身心, 我们可以积极面对 人生活一辈子是为了得到还是体验? 爬到山顶并不是风景变好了, 而是视野变宽广了 ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:1:2","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["column"],"content":"2022-03 ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:2:0","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["column"],"content":"谷歌搜索正在消亡 https://mp.weixin.qq.com/s/qgnCUw7tp27eUT_GqK6V6Q 受众群体不一样, 产品定位不一样, 经营模式不一样. ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:2:1","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["column"],"content":"2021年，算法工程师必备的能力是什么？ https://mp.weixin.qq.com/s/ngT7cGDeeGsh7PoTOXIfUQ https://mp.weixin.qq.com/s/AppkIr3_BT_d5Zifo_ProQ 前几天听博客有个嘉宾把内卷的概念说得非常不错: 内卷就是靠拼命剥削自己, 然后让自己在社会或者职场上获取一些竞争优势, 并挤压对手的生存空间[link]. 现在的算法工程师岗位, 也正经历着互联网大环境饱和带来的饱和和瓶颈, 从而更加需要思考如何在职场上才更具竞争力. 目前的算法工程师, 也不再仅仅局限于算法模型, 或多或少都有更高(精)甚至于更多(广)的要求, 其中工程能力是基础也是大势所趋(或许未来不在存在或者极少存在单纯的算法岗位, 更多会对工程能力要求较高的复合型人才). 另外, 算法需要对整个pipeline有极深极广的认识, 需要从最初的业务认识, 数据分析, 数据工程, 问题建模, 代码建模, 总结复盘, 沉淀输出的长流程, 多方面的能力. 这个场景是否有些熟悉, 没错, 就是软件开发, 从前端, 后端, 客户端的软件全栈开发工程师, 后面也慢慢的会有数据分析, 数据开发, 算法设计, 算法实现, 工程上线的算法全栈开发工程师. ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:2:2","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["column"],"content":"书单：分享技术人的读书笔记和几本好书 https://mp.weixin.qq.com/s/2ZR4sV9CoG-g-yT2MBSKkw 最近也在想人活一世的意义是什么, 该文中有一回答. 生命的意义可分为三层: 第一层为生存, 即活着还有温饱; 第二层为追求质量, 更好的事业的精神追求, 更多的财富的物质追求, 有了经济基础可能随之带来更多的精神追求, 比如旅游, 可选择上班等; 第三层为立于天地, 即为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平. 另外, 其中的还有基本数的思想也很引入深思, 比如说刻意练习, 教育方法等, 可能对自己都有不错的启发和提升. 这个只是听听别人的推荐, 自己看上去可能也不全是那么回事, 这篇文章推荐了多本书, 但是我想多看书还是好的吧. ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:2:3","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["column"],"content":"历史 阅读笔记2021 [TOC] ","date":"2022-03-10","objectID":"/columns/reading-note-2022.html:3:0","tags":["column"],"title":"阅读笔记2022","uri":"/columns/reading-note-2022.html"},{"categories":["tf","dl"],"content":"原理 dropout原理, 随机丢弃一些(输入)神经元, 防止参数过拟合 Applies Dropout to the input. Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. The units that are kept are scaled by 1 / (1 - rate), so that their sum is unchanged at training time and inference time. ","date":"2021-04-27","objectID":"/posts/2104271104-dropout-note.html:1:0","tags":["tf","dl"],"title":"dropout笔记","uri":"/posts/2104271104-dropout-note.html"},{"categories":["tf","dl"],"content":"核心实现 # tensorflow.python.ops.nn_ops.dropout_v2 ln:4174 noise_shape = _get_noise_shape(x, noise_shape) # Sample a uniform distribution on [0.0, 1.0) and select values larger than # rate. # # NOTE: Random uniform actually can only generate 2^23 floats on [1.0, 2.0) # and subtract 1.0. random_tensor = random_ops.random_uniform( noise_shape, seed=seed, dtype=x.dtype) keep_prob = 1 - rate scale = 1 / keep_prob # NOTE: if (1.0 + rate) - 1 is equal to rate, then we want to consider that # float to be selected, hence we use a \u003e= comparison. keep_mask = random_tensor \u003e= rate ret = x * scale * math_ops.cast(keep_mask, x.dtype) if not context.executing_eagerly(): ret.set_shape(x.get_shape()) return ret 这里并没有mode==training了, 返回到上上一层调用, 可以看到mode # tensorflow.python.keras.layers.core.Dropout.call def call(self, inputs, training=None): if training is None: training = K.learning_phase() def dropped_inputs(): return nn.dropout( inputs, noise_shape=self._get_noise_shape(inputs), seed=self.seed, rate=self.rate) output = tf_utils.smart_cond(training, dropped_inputs, lambda: array_ops.identity(inputs)) return output 综合上面来看, tensorflow dropout的处理方法是在训练的时候按照dropout(rate)丢弃一些神经元, 丢弃后, 然后在整体上, 又乘以一个scale( = 1 / keep_prob, keep_prob = 1 - rate), 所以在预估的时候是不需要做任何处理的. 所以在训练时, 一定要加training=True or False? 但是, 这里有一个判定是否是learning_phase, 所以, 其实不传也没有问题? # tensorflow.python.keras.layers.core.Dropout.call ln:149 def call(self, inputs, training=None): if training is None: # -- see me training = K.learning_phase() def dropped_inputs(): return nn.dropout( inputs, noise_shape=self._get_noise_shape(inputs), seed=self.seed, rate=self.rate) output = tf_utils.smart_cond(training, dropped_inputs, lambda: array_ops.identity(inputs)) 这里可能是错误的, tf_utils.smart_cond根据training进行下一步调用, 如果第一个参数是phase_pred则调用第一个方法(dropped_inputs), 否则调用第二个方法(lambda: array_ops.identity(inputs)) ","date":"2021-04-27","objectID":"/posts/2104271104-dropout-note.html:2:0","tags":["tf","dl"],"title":"dropout笔记","uri":"/posts/2104271104-dropout-note.html"},{"categories":["tf","dl"],"content":"问题思考 dropout是防止下层参数的过拟合还是上层参数的过拟合呢? 按照上面的分析, 可以理解为dropout随机丢弃一些输入, 因此在一定程度上防止下层参数的过拟合 每一层参数都需要dropout来防止过拟合吗? 换个角度就是每层都有可能过拟合吗? 这里需要回到dropout在集成学习上的原理, 通过随机丢弃神经元, dropout可以看做是多个网络模型的组合, 当有n个神经元的输入设置dropout=0.5时, 网络相当于有2^n种结构的集成, 因此, 当有dropout输入的神经元越多, 网络的集成度就约复杂, 越能防止过拟合, 这个还要根据实际数据情况设置 是否可以对参数进行dropout? 可以是可以, 但是对参数进行dropout后, 对于同一个batch数据, 缺失的特征列都是一样的了, 这样对模型的训练与预估势必会有一些影响 ","date":"2021-04-27","objectID":"/posts/2104271104-dropout-note.html:3:0","tags":["tf","dl"],"title":"dropout笔记","uri":"/posts/2104271104-dropout-note.html"},{"categories":["unsort"],"content":"相信未来, 拥抱未来! ","date":"2021-04-04","objectID":"/posts/2104040035-hello.html:0:0","tags":["unsort"],"title":"hello","uri":"/posts/2104040035-hello.html"},{"categories":["unsort"],"content":"多git有两种状态 多个git账号(user, email) 多个认证(identities) ","date":"2021-04-03","objectID":"/posts/old/2104031558-multi-git.html:0:0","tags":["unsort"],"title":"多git协作","uri":"/posts/old/2104031558-multi-git.html"},{"categories":["unsort"],"content":"设置多个git账号 条件: git版本号高于2.13(git --version) 编辑~/.gitconfig. 注意: 路径后面要加斜杠/ # specify multiple users with git # refs: https://stackoverflow.com/a/43654115 and https://stackoverflow.com/a/48088291 # use `git config --get user.email` to check config is working # default [user] name = wangke email = wangkest@qq.com # specific for working [includeIf \"gitdir:/Users/wangke/working/\"] # 末尾加斜杠 path = ~/.gitconfig_58corp # specific for working2 [includeIf \"gitdir:/Users/wangke/working/2/\"] # 末尾加斜杠 path = ~/.gitconfig_58corp2 然后配置刚才指定的配置文件 vim .gitconfig_working [user] name = working_name email = working_name@working_name.com vim .gitconfig_working2 [user] name = working_name2 email = working_name2@working_name.com 然后在已配置路径下的~/这个路径下使用私人账号/或者~/这个路径下使用工作账号/执行命令, 可发现可以自动选择用户了 cd ~/该路径下使用私人账号/ git config --get user.email ","date":"2021-04-03","objectID":"/posts/old/2104031558-multi-git.html:1:0","tags":["unsort"],"title":"多git协作","uri":"/posts/old/2104031558-multi-git.html"},{"categories":["unsort"],"content":"refs https://gist.github.com/bgauduch/06a8c4ec2fec8fef6354afe94358c89e ","date":"2021-04-03","objectID":"/posts/old/2104031558-multi-git.html:2:0","tags":["unsort"],"title":"多git协作","uri":"/posts/old/2104031558-multi-git.html"},{"categories":["unsort"],"content":"用了一个很久的, 可以试一下 https://sbirdo.me 特点 简单(中文的, 有详细教程), 成本低(可以每月付, 最低10元) 点我的邀请链接会给我返利哟, 算对我的支持吧 [邀请链接] 另外一个, 有个朋友一直在用的 https://home.shadowsocks.ch/cart.php 特点 用的人挺多, 目前稳定性应该是非常好的 但是只能年付, 虽然算下来也是每月10元, 但是成本还是算高 还有一个, 另外一个朋友在用的, 需要自己搭, 我还没有试过, 后面换的话可能会试 自建ss服务器教程 https://zoomyale.com/2016/vultr_and_ss 科学上网-vultr-vps-搭建-shadowsocks-ss-教程 特点 设置较为复杂, 自己买服务器, 好像成本也挺高的, 但是流量较于前两个多 ","date":"2019-06-10","objectID":"/posts/old/1906101549-proxy.html:0:0","tags":["unsort"],"title":"proxy","uri":"/posts/old/1906101549-proxy.html"},{"categories":["unsort"],"content":"1 下载AutoHotKey 官网, 点我直接官网下载 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:1:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"2 编辑脚本 另存下面脚本为capslock_plus.ahk 注: 下面脚本只适用于AutoHotKey1.XXX, 不使用于2.XXX(V2和其他版本有区别!)直接下载 ; Autohotkey Capslock Remapping Script ; Colinwke ; More info at: TODO ; ; Functionality: ; - Deactivates capslock for normal (accidental) use. ; - Access the following functions when pressing Capslock: ; Cursor keys - J, K, L, I ; Home, PgDn, PgUp, End - U, O, P, ; ; Backspace and Del - Y, H ; ; Insert - [ ; Close tab, window - W, N ; Previous, next tab - M, , ; Undo, redo - ., / ; Menu - ' ; - Numpad at the right hand resting position when holding Ctrl+Shift+Alt (using keys m,.jkluio and spacebar) ; ; To use capslock as you normally would, you can press WinKey + Capslock ; This script update from Danik's work: https://gist.github.com/Danik/5808330 ;================================================================ #Persistent SetCapsLockState, AlwaysOff ;================================================================ ; Capslock + (ikjl) -\u003e (up, down, left, right) Capslock \u0026 i::Send {Blind}{Up DownTemp} Capslock \u0026 i up::Send {Blind}{Up Up} Capslock \u0026 k::Send {Blind}{Down DownTemp} Capslock \u0026 k up::Send {Blind}{Down Up} Capslock \u0026 j::Send {Blind}{Left DownTemp} Capslock \u0026 j up::Send {Blind}{Left Up} Capslock \u0026 l::Send {Blind}{Right DownTemp} Capslock \u0026 l up::Send {Blind}{Right Up} ;================================================================ ; Capslock + (uop;) -\u003e (home, end, pgup, pgdown) Capslock \u0026 u::SendInput {Blind}{Home Down} Capslock \u0026 u up::SendInput {Blind}{Home Up} Capslock \u0026 o::SendInput {Blind}{End Down} Capslock \u0026 o up::SendInput {Blind}{End Up} Capslock \u0026 `;::SendInput {Blind}{PgDn Down} Capslock \u0026 `; up::SendInput {Blind}{PgDn Up} Capslock \u0026 p::SendInput {Blind}{PgUp Down} Capslock \u0026 p up::SendInput {Blind}{PgUp Up} ;================================================================ ; Capslock + (nw) -\u003e (close tab|window) Capslock \u0026 n::SendInput {Ctrl down}{F4}{Ctrl up} Capslock \u0026 w::SendInput {Alt down}{F4}{Alt up} ;================================================================ ; Capslock + ([hy) -\u003e (insert, backspace, del) Capslock \u0026 [::SendInput {Blind}{Insert Down} Capslock \u0026 y::SendInput {Blind}{Del Down} Capslock \u0026 h::SendInput {Blind}{BS Down} Capslock \u0026 BS::SendInput {Blind}{BS Down} ;================================================================ ; Capslock + (m,) -\u003e (prev|next tab) Capslock \u0026 m::SendInput {Ctrl Down}{Tab Down} Capslock \u0026 m up::SendInput {Ctrl Up}{Tab Up} Capslock \u0026 ,::SendInput {Ctrl Down}{Shift Down}{Tab Down} Capslock \u0026 , up::SendInput {Ctrl Up}{Shift Up}{Tab Up} ;================================================================ ; Capslock + (./) -\u003e (undo, redo) Capslock \u0026 .::SendInput {Ctrl Down}{z Down} Capslock \u0026 . up::SendInput {Ctrl Up}{z Up} Capslock \u0026 /::SendInput {Ctrl Down}{y Down} Capslock \u0026 / up::SendInput {Ctrl Up}{y Up} ;================================================================ ; Capslock + ; -\u003e menu Capslock \u0026 '::Send {Blind}{AppsKey DownTemp} Capslock \u0026 ' up::Send {Blind}{AppsKey Up} ;================================================================ ; Numpad using Ctrl+Shift+Alt + m,.jkluio or space +^!Space:: SendInput {Numpad0} +^!m:: SendInput {Numpad1} +^!,:: SendInput {Numpad2} +^!.:: SendInput {Numpad3} +^!j:: SendInput {Numpad4} +^!k:: SendInput {Numpad5} +^!l:: SendInput {Numpad6} +^!u:: SendInput {Numpad7} +^!i:: SendInput {Numpad8} +^!o:: SendInput {Numpad9} ;================================================================ ; Make Capslock \u0026 Enter equivalent to Control+Enter Capslock \u0026 Enter::SendInput {Ctrl down}{Enter}{Ctrl up} ; Make Capslock \u0026 Alt Equivalent to Control+Alt !Capslock::SendInput {Ctrl down}{Alt Down} !Capslock up::SendInput {Ctrl up}{Alt up} ;================================================================ ; Make Win Key + Capslock work like Capslock (in case it's ever needed) #Capslock:: If GetKeyState(\"CapsLock\", \"T\") = 1 SetCapsLockState, AlwaysOff Else SetCapsLockState, AlwaysOn Return ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:2:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"3 修改打开方式 修改*.ahk的默认打开方式为AutoHotkeyU64.exe ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:3:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"4 运行 win+r 输入 shell:startup 回车, 把脚本拷贝至当前路径下, 使得自动运行生效 双击脚本立即生效 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:4:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"5 映射表 参考: https://gist.github.com/Danik/5808330 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:5:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"其他参考 capslock+ikjl 代替上下左右按键 ahk注释: 单行注释用分号;，多行同C语言用/* */括起。 按键列表 映射符号 如下输入 `;::LCtrl 遇到可能被误引用的字符时，前缀` 比如`\" `{ `% `` 比较好的教程, 另一个教程 另外一个神器capslock+ 初学者向导 指南和概述 脚本展示 自启动 在运行里面输入: shell:startup 打开之后把要启动的脚本放进去即可 热键, 组合键 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:6:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["unsort"],"content":"几个capslock wsas代替上下左右 hjkl代替上下左右 ijkl代替上下左右, 推荐 ","date":"2019-05-27","objectID":"/posts/old/1905272156-autohotkey.html:7:0","tags":["unsort"],"title":"AutoHotKey","uri":"/posts/old/1905272156-autohotkey.html"},{"categories":["reading"],"content":"2019-5-12 最近计划学习一下深度学习框架, kaggle是个不错的平台, 就找了其中的比赛Jigsaw Unintended Bias in Toxicity Classification. 在比赛的第四段, 描述了比赛的背景, 和技术中存在的问题: Here’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. “gay”), even when those comments were not actually toxic (such as “I am a gay woman”). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users. 谷歌翻译: 以下是背景：当对话AI团队首次构建毒性模型时，他们发现模型错误地学会将频繁攻击身份的名称与毒性联系起来。模型预测含有这些身份（例如“同性恋”）的评论很可能具有毒性，即使这些评论实际上并不具有毒性（例如“我是同性恋女性”）。发生这种情况是因为训练数据是从可用来源中提取的，不幸的是，某些身份以令人反感的方式被压倒性地提及。从具有这些不平衡的数据中训练模型可能会将这些偏差反映回用户。 简单说来就是大样本覆盖小样本的问题, 也是高频扰乱低频的问题, 这个问题在大多数场景都是存在的, 例如在淘宝购物中, 食品类的产品买了又买, 且一次也可以买多件, 但是对于电器类的产品, 一次一般只买一件, 且买了后很长一段时间都不会再买. 通常包含了这两个类品的数据构建模型时, 高频的数据就会对低频的数据造成影响. 这从另一个方面也反应了模型的偏差和方差问题, 也是过拟合和欠拟合问题, 对于\"电器\"和\"食品\"的分类, 可以考虑分开建模的方法, 即食品单独建一个模型, 电器单独建一个模型, 或者设置加权的损失函数. 似乎电器和食品的例子和这个比赛也有不同, 这个比赛是关于一个词语的偏差, 可能在比赛中还是更应该注意偏差和方差, 还有评价指标的设定(召回还是精确). ","date":"2019-05-12","objectID":"/posts/old/1905121610-reading-may.html:1:0","tags":["reading"],"title":"五月阅读","uri":"/posts/old/1905121610-reading-may.html"},{"categories":["unsort"],"content":"wondows平台, pip安装MeCab: pip install mecab-python3 出现问题: 'mecab-config' 不是内部或外部命令，也不是可运行的程序或批处理文件。 在网上找了一些资料, 一些日文资料写的云里雾里的, 比如这篇Windows環境でのMeCab(Python)のインストール(没有必要打开). 然后找到一篇中文的windows10+py36+MeCab安装总结, 按照步骤安装成功! ===我是分割线=== 上文的有些步骤可以精简, 具体步骤可以如下: 1 在pip安装之前, 需要先安装MeCab.exe 懒得麻烦, 点我直接下载 2 安装, 中间有个选择字典编码, 选择UTF-8 3 安装python依赖包 首先, 需要把MeCab安装路径下的./bin/libmecab.dll和./sdk/libmecab.lib拷贝到python的./Lib/site-packages下面. 然后, 就可以使用pip install mecab-python-windows安装依赖包了. 4 测试 import MeCab mecab = MeCab.Tagger(\"-Ochasen\") # `-Owakati` 只做分词 print(mecab.parse(\"pythonが大好きです\")) 参考: https://segmentfault.com/q/1010000015969023 https://blog.csdn.net/ZYXpaidaxing/article/details/81913708 https://www.jianshu.com/p/8f0ce2cff8d9 http://www.flickering.cn/nlp/2014/06/%E6%97%A5%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8-mecab-%E6%96%87%E6%A1%A3/ ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:0:0","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["unsort"],"content":"MeCab使用 # date: 2019/3/21 10:53 # author: wang ke # concat: ke.wang@ctrip.com # ================================ \"\"\"MeCab test. --- MeCab output format(http://taku910.github.io/mecab/): -Owakati: only segment python が 大好き です -Ochasen: ChaSen compatible(segment and tag?) python python python 名詞-固有名詞-組織 が ガ が 助詞-格助詞-一般 大好き ダイスキ 大好き 名詞-形容動詞語幹 です デス です 助動詞 特殊・デス 基本形 EOS -Oyomi: Yomi given(only translate?) pythonガダイスキデス -Odump: all information output 0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0 3 python 名詞,固有名詞,組織,*,*,*,* 0 6 1292 1292 45 5 1 1 0.000000 0.000000 0.000000 12857 8 が 助詞,格助詞,一般,*,*,*,が,ガ,ガ 6 9 148 148 13 6 0 1 0.000000 0.000000 0.000000 11729 21 大好き 名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ 9 18 1287 1287 40 2 0 1 0.000000 0.000000 0.000000 13008 48 です 助動詞,*,*,*,特殊・デス,基本形,です,デス,デス 18 24 460 460 25 6 0 1 0.000000 0.000000 0.000000 12875 56 EOS BOS/EOS,*,*,*,*,*,*,*,* 24 24 0 0 0 0 3 1 0.000000 0.000000 0.000000 11634 \"\": python 名詞,固有名詞,組織,*,*,*,* が 助詞,格助詞,一般,*,*,*,が,ガ,ガ 大好き 名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ です 助動詞,*,*,*,特殊・デス,基本形,です,デス,デス EOS \"\"\" import MeCab def format_tag_result(x): pieces = [] for i in x.splitlines()[:-1]: i = i.split() v = (i[0], i[-1]) pieces.append(v) return pieces mecab_tagger = MeCab.Tagger(\"-Ochasen\") # `-Owakati` 只做分词 text = \"pythonが大好きです\" print(format_tag_result(mecab_tagger.parse(text))) ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:1:0","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["unsort"],"content":"mecab ipadic字典提取 因为ipadic字典是用csv保存的, 因此还是很好提取的. 需要注意的是, 在打开csv时, 选择的文本编码是shift-jis. 比如说提取地点Noun.place.csv ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:1:1","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["unsort"],"content":"wikipedia dictionary download(日本语) https://www.zhihu.com/question/19803440 https://zh.wikipedia.org/wiki/Wikipedia:%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD google wikipedia dictionary 日本語ダウンロード Wikipedia:データベースダウンロード 下面是翻译成中文的 日文: https://dumps.wikimedia.org/jawiki/latest/ 中文: https://dumps.wikimedia.org/chwiki/latest/ 随便下载一个: chwiki-latest-geo_tags.sql 注意, 这是sql文件, 即为可执行的文件, 包含创建数据库和插入数据, 并不是数据库文件, 这个可以直接使用文本编辑软件打开 但是里面好像并没有什么… 中文搞错了, 应该是这个: https://dumps.wikimedia.org/zhwiki/latest/ ","date":"2019-05-07","objectID":"/posts/old/1905071330-mecab.html:2:0","tags":["unsort"],"title":"mecab","uri":"/posts/old/1905071330-mecab.html"},{"categories":["win"],"content":"保存win10 spotlight 壁纸到D盘下! 1 将下面文本保存为win10_spotlight.bat(直接下载) rem https://blog.csdn.net/qq_34260368/article/details/78364055 rem https://blog.csdn.net/linbounconstraint/article/details/80191846 rem https://blog.csdn.net/Anymake_ren/article/details/51125609 rem https://stackoverflow.com/questions/17587347/batch-file-to-run-xcopy-without-overwriting-existing-files @echo off MD wallpaper xcopy \"%UserProfile%\\AppData\\Local\\Packages\\Microsoft.Windows.ContentDeliveryManager_cw5n1h2txyewy\\LocalState\\Assets\" \"D:\\spotlight\\\" /S /Y /D D: cd \"D:\\spotlight\\\" ren * *.jpg pause 2 双击执行, 执行后前往D:\\spotlight\\查看 参考: https://blog.csdn.net/qq_34260368/article/details/78364055 https://blog.csdn.net/linbounconstraint/article/details/80191846 https://blog.csdn.net/Anymake_ren/article/details/51125609 https://stackoverflow.com/questions/17587347/batch-file-to-run-xcopy-without-overwriting-existing-files ","date":"2018-12-29","objectID":"/posts/old/dabble/save-win10-spotlight.html:0:0","tags":["win"],"title":"save-win10-spotlight","uri":"/posts/old/dabble/save-win10-spotlight.html"},{"categories":["unsort"],"content":"先看一个DNN, CNN, RNN的比较 深度 输入 参数 输出 DNN 神经元堆叠层数 向量 连接 向量 RNN 序列长度 向量 连接 向量 CNN 一组操作(卷积, 池化) 特征图 连接, 卷积核 特征图 Q:CNN中原图和卷积生成的特征图的位置是对应的吗? A: 是对应的. 因为卷积(或池化)生成的特征图一直都是一一对应(这也是局部关联的实现). 如果最后提取的特征图为4个神经元的正方形, 那么这四个神经元是对应的, 如果最后提取的特征图为一个神经元的输出, 那么, 这个神经元的信息已经包含了整张图片的信息. ","date":"2018-12-28","objectID":"/posts/old/deep-learning/cnn.html:0:0","tags":["unsort"],"title":"cnn","uri":"/posts/old/deep-learning/cnn.html"},{"categories":["unsort"],"content":" 转至: [Link] CNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数。总之，卷积网络的核心思想是将：局部感受野、权值共享（或者权值复制）以及时间或空间亚采样这三种结构思想结合起来获得了某种程度的位移、尺度、形变不变性。 ​ 下图左：如果我们有1000x1000像素的图像，有1百万个隐层神经元，那么他们全连接的话（每个隐层神经元都连接图像的每一个像素点），就有1000x1000x1000000=10^12个连接，也就是10^12个权值参数。然而图像的空间联系是局部的，就像人是通过一个局部的感受野去感受外界图像一样，每一个神经元都不需要对全局图像做感受，每个神经元只感受局部的图像区域，然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。这样，我们就可以减少连接的数目，也就是减少神经网络需要训练的权值参数的个数了。如下图右：假如局部感受野是10x10，隐层每个感受野只需要和这10x10的局部图像相连接，所以1百万个隐层神经元就只有一亿个连接，即10^8个参数。比原来减少了四个0（数量级），这样训练起来就没那么费力了，但还是感觉很多的啊，那还有啥办法没？ ​ 我们知道，隐含层的每一个神经元都连接10x10个图像区域，也就是说每一个神经元存在10x10=100个连接权值参数。那如果我们每个神经元这100个参数是相同的呢？也就是说每个神经元用的是同一个卷积核去卷积图像。这样我们就只有多少个参数？？只有100个参数啊！不管你隐层的神经元个数有多少，两层间的连接我只有100个参数啊！这就是权值共享。 好了，你就会想，这样提取特征也忒不靠谱吧，这样你只提取了一种特征啊？对了，真聪明，我们需要提取多种特征对不？假如一种滤波器，也就是一种卷积核就是提出图像的一种特征，例如某个方向的边缘。那么我们需要提取不同的特征，怎么办，加多几种滤波器不就行了吗？对了。所以假设我们加到100种滤波器，每种滤波器的参数不一样，表示它提出输入图像的不同特征，例如不同的边缘。这样每种滤波器去卷积图像就得到对图像的不同特征的放映，我们称之为Feature Map。所以100种卷积核就有100个Feature Map。这100个Feature Map就组成了一层神经元。到这个时候明了了吧。我们这一层有多少个参数了？100种卷积核x每种卷积核共享100个参数=100x100=10K，也就是1万个参数。才1万个参数。见下图右：不同的颜色表达不同的滤波器。 ​ 嘿哟，遗漏一个问题了。刚才说隐层的参数个数和隐层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。那么隐层的神经元个数怎么确定呢？它和原图像，也就是输入的大小（神经元个数）、滤波器的大小和滤波器在图像中的滑动步长都有关！例如，我的图像是1000x1000像素，而滤波器大小是10x10，假设滤波器没有重叠，也就是步长为10，这样隐层的神经元个数就是(1000x1000 )/ (10x10)=100x100个神经元了，假设步长是8，也就是卷积核会重叠两个像素，那么……我就不算了，思想懂了就好。注意了，这只是一种滤波器，也就是一个Feature Map的神经元个数哦，如果100个Feature Map就是100倍了。由此可见，图像越大，神经元个数和需要训练的权值参数个数的贫富差距就越大。 feature map计算方法： 在CNN网络中roi从原图映射到feature map中的计算方法 INPUT为3232，filter的大小即kernel size为55，stride = 1，pading=0,卷积后得到的feature maps边长的计算公式是： output_h =（originalSize_h+padding*2-kernelSize_h）/stride +1 所以，卷积层的feature map的变长为：conv1_h=（32-5）/1 + 1 = 28 卷积层的feature maps尺寸为28*28. 由于同一feature map共享权值，所以总共有6*（5*5+1）=156个参数。 卷积层之后是pooling层，也叫下采样层或子采样层（subsampling）。它是利用图像局部相关性的原理，对图像进行子抽样，这样在保留有用信息的同时可以减少数据处理量。pooling层不会减少feature maps的数量，只会缩减其尺寸。常用的pooling方法有两种，一种是取最大值，一种是取平均值。 pooling过程是非重叠的，S2中的每个点对应C1中22的区域（也叫感受野），也就是说kernelSize=2，stride=2，所以pool1_h = (onv1_h - kernelSize_h)/stride +1 = (28-2)/2+1=14。pooling后的feature map尺寸为1414. fast rcnn以及faster rcnn做检测任务的时候，涉及到从图像的roi区域到feature map中roi的映射，然后再进行roi_pooling之类的操作。 ​ 比如图像的大小是（600,800），在经过一系列的卷积以及pooling操作之后在某一个层中得到的feature map大小是（38,50），那么在原图中roi是（30,40,200,400）， 在feature map中对应的roi区域应该是 roi_start_w = round(30 * spatial_scale); roi_start_h = round(40 * spatial_scale); roi_end_w = round(200 * spatial_scale); roi_end_h = round(400 * spatial_scale); 其中spatial_scale的计算方式是spatial_scale=round(38/600)=round(50/800)=0.0625，所以在feature map中的roi区域[roi_start_w,roi_start_h,roi_end_w,roi_end_h]=[2,3,13,25]; 具体的代码可以参见caffe中roi_pooling_layer.cpp ","date":"2018-12-26","objectID":"/posts/old/deep-learning/2018-12-26-cnn-feature-map.html:0:0","tags":["unsort"],"title":"CNN-feature-map","uri":"/posts/old/deep-learning/2018-12-26-cnn-feature-map.html"},{"categories":["dl"],"content":"损失函数 在GAN中, Adversarial Model的功能是判别样本是否来自于Generative Model. 而Generative Model的目标是最大化的混淆Adversarial Model. 判别模型的目标函数 $$ \\max_{ D } E_{ x \\sim P_{ r } } [ \\log D ( x ) ] + E_{ x \\sim P_{ g } } [ \\log ( 1 - D ( x ) ] $$ 这难道不是最大化判别模型交叉熵损失吗? 但是, 当最大化交叉熵损失的时, 也就是全部样本分错的情况, 也不能混淆对抗模型呢? 事实上, GAN的目标函数就是交叉熵损失! $$ H ( p , q ) = - \\sum _ { i } p _ { i } \\log q _ { i } = - y \\log \\hat { y } - ( 1 - y ) \\log ( 1 - \\hat { y } ) $$ $x \\sim P _ { r }$ 对应着$y$, 对应着$1 - y$ 最小化交叉熵损失, 就是最大化对抗损失的负数 再来看看GAN的目标函数 D and G play the following two-player minimax game with value function $V (G; D)$ $$ \\min _ { G } \\max _ { D } V ( D , G ) = \\mathbb { E } _ { \\boldsymbol { x } \\sim p _ { \\text { data } } ( \\boldsymbol { x } ) } [ \\log D ( \\boldsymbol { x } ) ] + \\mathbb { E } _ { \\boldsymbol { z } \\sim p _ { \\boldsymbol { z } } ( \\boldsymbol { z } ) } [ \\log ( 1 - D ( G ( \\boldsymbol { z } ) ) ) ] $$ 为了表现出交叉熵的形式, 可以变形为 $$ \\max _ { G } \\min _ { D } V ( D , G ) = - \\mathbb { E } _ { \\boldsymbol { x } \\sim p _ { \\text { data } } ( \\boldsymbol { x } ) } [ \\log D ( \\boldsymbol { x } ) ] - \\mathbb { E } _ { \\boldsymbol { z } \\sim p _ { \\boldsymbol { z } } ( \\boldsymbol { z } ) } [ \\log ( 1 - D ( G ( \\boldsymbol { z } ) ) ) ] $$ 当更新G的时候, 使该目标函数最大! 也就是G尽量的混淆D 当更新D的时候, 使该目标函数最小! 也就是D尽量的判别G ","date":"2018-12-21","objectID":"/posts/old/2018-12-21-adversarial-model.html:1:0","tags":["gan"],"title":"adversarial-model","uri":"/posts/old/2018-12-21-adversarial-model.html"},{"categories":["dl"],"content":"最大化混淆和最大化损失 当更新G的时候, 需要最大化损失函数 但是, 不是全部分错的时候损失函数函数最大吗? 但是这个时候也不能混淆D呀! 这个时候, 因为只考虑了G, 而没有考虑D! 结合目标函数的交叉熵损失形式 当更新G时, 固定D, G的目的是最大化分错, 因此我们需要最大化交叉熵损失 当更新D时, 固定G, D的目的是最大化辨别, 因此我们需要最小化交叉熵损失 然后这两个模型就开始推太极了(two-player-game), 对于一个生成的样本, 其真实标记为1 当更新G时, 固定D, G的目的是最大化分错, 让D预测的标记趋近于0 当更新D时, 固定G, D的目的是最大化辨别, 让D预测的标记趋近于1 然后两兄弟开始推太极, 最后收敛到了0.5 ","date":"2018-12-21","objectID":"/posts/old/2018-12-21-adversarial-model.html:2:0","tags":["gan"],"title":"adversarial-model","uri":"/posts/old/2018-12-21-adversarial-model.html"},{"categories":["dl"],"content":"refs https://blog.csdn.net/u010089444/article/details/78946039 ","date":"2018-12-21","objectID":"/posts/old/2018-12-21-adversarial-model.html:3:0","tags":["gan"],"title":"adversarial-model","uri":"/posts/old/2018-12-21-adversarial-model.html"},{"categories":["ml"],"content":"在迁移学习中, 领域判别损失如下: 咋一看还看不懂了, 交叉熵损失也就是logloss不是这个样子的吗: $$ H(x)\\ =\\ -\\sum_{i}p_{i}\\log q_{i}\\ =\\ -y\\log{\\hat{y}}-(1-y)\\log(1-{\\hat{y}}) $$ 其实也是啊, 可以从两个角度进行解释: 首先 在Logistic Regression的公式推导中, 是使用的最大似然的对数取反来作为的损失, 也就是负的极大似然的对数. 其次 可以从熵的角度进行解释, 对于二分类任务来讲, 真实标记为类别, 预测的结果为概率, 因此为两个类别预测的熵. ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:0:0","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"交叉熵损失是熵吗? 显然不是, 熵可没有真实的标记! 交叉熵损失可以衡量两个分布的距离, 在二分类中, 一个分布为预测的概率, 一个分布为真实的标记. ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:1:0","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"这篇文章写得不错! ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:0","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"信息量 一个事件x的信息量是： $$ I(x)=-log(p(x)) $$ 解读：如果一个事件发生的概率越大，那么信息量就越小。如果是1，也就是100%发生，那么信息量为0。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:1","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"熵 是对信息量求期望值。 $$ H(X)=E[I(x)]=-\\sum\\limits_{x∈X}p(x)\\log p(x) $$ 举例： 如果10次考试9次不及格，一次及格。 假设事件为xAxA代表及格事件，那么这个事件的熵为： $$ H_A(x)=-[p(x_A)\\log(p(x_A))+(1-p(x_A))\\log(1-p(x_A))]=0.4690 $$ 其实也和后续的逻辑回归的二分类的损失函数有类似。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:2","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"KL散度 相对熵(relative entropy)又称为KL散度（Kullback-Leibler divergence），KL距离，是两个随机分布间距离的度量。记为DKL(p||q)。它度量当真实分布为p时，假设分布q的无效性。 $$ \\displaylines{ \\begin{align} D_{KL}(p||q)\u0026=E_p[\\log \\frac{p(x)}{q(x)}]=\\sum\\limits_{x∈\\mathcal{X}} p(x)\\log \\frac{p(x)}{q(x)} \\\\ \u0026= \\sum\\limits_{x∈\\mathcal{X}} [p(x)\\log p(x)-p(x)\\log q(x)]\\\\ \u0026= -H(p)-\\sum\\limits_{x∈\\mathcal{X}} p(x)\\log q(x)\\\\ \u0026= -H(p)+E_p[-\\log q(x)] \\end{align} } $$ 当p=q的时候，散度为0. ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:3","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"交叉熵 假设有两个分布p，q，则它们在给定样本集上的交叉熵定义如下： $$ \\begin{align*} CrossEntropy(p,q)\u0026=E_p[-\\log q]\\\\ \u0026=-\\sum\\limits_{x∈\\mathcal{X}} p(x)\\log q(x)\\\\ \u0026=H(p)+D_{KL}(p||q) \\end{align*} $$ 当p分布是已知，则熵是常量；于是交叉熵和KL散度则是等价的。最小化交叉熵等价于最小化KL距离。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:4","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"对应到logistic regression 在逻辑回归中我们用交叉熵来定义损失函数的。那么来再推导一次。详细参考： https://blog.csdn.net/iterate7/article/details/78992027 p:真实样本分布，服从参数为p的0-1分布，即X~B(1,p) q:待估计的模型，服从参数为q的0-1分布，即X~B(1,q) 0-1分布，我们把其中一种事件的结果发生的概率定为p，那么另一种结果的概率就是1-p，两者的概率和是1.[贝努力分布] $$ \\begin{align*} CrossEntropy(p,q)\u0026=-\\sum\\limits_{x∈\\mathcal{X}} \\textbf{p(x)}\\log \\textbf{q(x)} \\\\ \u0026=-[P_p(x=1)\\log P_q(x=1)+P_p(x=0)\\log P_q(x=0)]\\\\ \u0026=-[p\\log q+(1-p)\\log (1-q)]\\\\ \u0026=-[\\textbf{y}\\log \\textbf{h}{\\theta}(x)+(1-\\textbf{y})\\log (1-\\textbf{h}{\\theta}(x))] \\\\ \\end{align*} $$ 这里q则是假设函数。 对所有的训练样本平均值交叉熵为： $$ J = -\\frac{1}{m}\\sum\\limits_{i=1}^m[y^{(i)}\\log h_{\\theta}(x^{(i)})+(1-y^{(i)})\\log (1-h_{\\theta}(x^{(i)}))] $$ ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:5","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"总结 信息量代表的是一种不确定性； 熵代表的是不确定性的期望值； 确定的事件的熵为0； KL散度代表的是利用熵的概念来表示分布之间的距离； 交叉熵等价于KL散度；熵是常量，因为训练数据的分布已知。 在逻辑回归中用交叉熵作为损失函数的原因是：交叉熵可以等价于KL散度；交叉熵越小，则p和q分布差异越小，拟合更好。 用最大似然方法推导的损失函数和最大熵的方式结果是一致的，最大似然方法的推导可以参考：https://blog.csdn.net/iterate7/article/details/78992027 实际中，选用交叉熵易于计算。 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:6","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["ml"],"content":"引用 https://blog.csdn.net/iterate7/article/details/78992027 https://en.wikipedia.org/wiki/Cross_entropy https://www.zhihu.com/question/65288314/answer/244601417 ","date":"2018-11-28","objectID":"/posts/old/cross-entropy-loss.html:2:7","tags":["ml"],"title":"谈一谈熵","uri":"/posts/old/cross-entropy-loss.html"},{"categories":["感受"],"content":"一 本科考研的时候, 我可能看上去比别人更努力一些. 为什么? 因为我知道我基础并不好, 所以我需要更多的努力. 俗话说, 笨鸟先飞, 我深深的铭记这句话. 每当他人问我为什么这么努力时, 我会说因为底子差, 比较笨, 所以需要更多努力, 笨鸟先飞嘛. 那个时候是一种笃定. ","date":"2018-11-04","objectID":"/posts/old/essay/stay-foolish.html:1:0","tags":["感受"],"title":"stay-foolish","uri":"/posts/old/essay/stay-foolish.html"},{"categories":["感受"],"content":"二 时过境迁. 现在也会遇到一些调侃: 你为何这么努力, 却还是一副窘境. 其实这真怪我, 也是我较为严重的一个缺点: 不喜欢逼自己, 好安逸, 好自由. 来了实验室也是刷刷微博, 翻翻知乎, 看看直播吧. 完全一副作死的样子. 我非常感谢周围能有一些非常优秀的朋友, 他们是我学习的榜样. 有天赋出众的, 有后期奋进的. 比如其中有一位基本不来实验室, 但是却通宵达旦的学习. 他可能还会表示一天都在宿舍玩游戏, 看动漫, 好像一天都在玩一样, 但是你这样想可能就错了, 他付出的努力可能远远超过你的想象, 或许他正在\"偷偷\"的学习, 这样说只是他心中的一个平衡罢了. 但是当他付出的努力取得成果时, 他会有一种优越感, 而你却往往只看到了一个结果, 而没有认识到努力的过程. 也可能是别人本就天赋出众, 这也是你未能具有的, 所以你只有付出更多的努力. 如今的我受到调侃却变得不那么的淡定, 内心波涛涌动. 认清自己才最为重要, 笨鸟先飞, 是一种态度, 也需要笃定. ","date":"2018-11-04","objectID":"/posts/old/essay/stay-foolish.html:2:0","tags":["感受"],"title":"stay-foolish","uri":"/posts/old/essay/stay-foolish.html"},{"categories":["paper"],"content":"2018-11-5 关于迁移学习, 领域适应, 对抗学习的论文, 代码汇总 zhaoxin94 / awsome-domain-adaptation jindongwang / transferlearning artix41 / awesome-transfer-learning zhaoxin94 / awsome-domain-adaptation barebell / DA zhangqianhui / AdversarialNetsPapers ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:1:0","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"2019-1-3 update https://xiaosean.github.io/posts/ 台湾一名学生的主页, 主要包含一些阅读笔记, 包含的领域有领域适应, 生成对抗网络等. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:2:0","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"迁移学习学者调研 参考jindongwang: transfer learning scholars, 对相关学者目前的状态做一个整理 ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:0","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Qiang Yang 杨强老师貌似不怎么做一线的研究了, 主页上没有什么新文章, 最后更新的也都是2016年的文章. Google Scholar有一些较新的文章, 关于迁移学习的文章比例大约为1/5, 且大多是第三作者. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:1","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Derek Hao Hu 这位学者已经不活跃了, 最近的论文也是2013年的. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:2","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Vincent W. Zheng 这位学者还战斗在一线, 但迁移学习的文章较少. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:3","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Sinno Jialin Pan 杨强老师的学生, 现在为老师, 比较年轻, 在2017年有部分迁移学习的文章. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:4","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Lixin Duan 2012年发表过关于迁移学习的文章, 现在在亚马逊任职, 主要做视觉方面的研究. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:5","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Mingsheng Long 挺年轻的一位学者, 坚持在迁移学习的一线. 推荐! ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:6","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Judy Hoffman 16, 17年有几篇迁移学习相关的论文, 主要做Domain Adaption, 包含对抗学习. 推荐! ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:7","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["paper"],"content":"Fuzhen Zhuang 庄福振 主要做Multi-Task的. ","date":"2018-10-31","objectID":"/posts/old/transfer-learning/transfer-scholar.html:3:8","tags":["paper","transfer-learning"],"title":"迁移学习资源整理","uri":"/posts/old/transfer-learning/transfer-scholar.html"},{"categories":["dl"],"content":"深入浅出：GAN原理与应用入门介绍 是一类在无监督学习中使用的神经网络 致力于通过学习恒等函数 f（x）= x 从数据中提取特征，且都依赖马尔可夫链来训练或生成样本。 相似的无监督学习方法还包含 玻尔兹曼机（Geoffrey Hinton 和 Terry Sejnowski，1985） 自动解码器（Dana H. Ballard，1987） ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:1:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"什么是GAN 想要学习生成器的分布，应该定义数据 $x$ 的参数 $p_g$，以及输入噪声变量 $p_z（z）$的分布。然后 $G（z，θ_g）$将 $z$ 从潜在空间 $Z$ 映射到数据空间，$D（x，θ_d）$输出单个标量——一个 $x$ 来自真实数据而不是 $p_g$ 的概率。 训练判别器以最大化正确标注实际数据和生成样本的概率。训练生成器用于最小化 $log（1-D（G（z）））$。换句话说，尽量减少判别器得出正确答案的概率。 可以将这样的训练任务看作具有值函数 $V（G，D）$的极大极小博弈： 换句话说，生成器努力生成判别器难以辨认的图像，判别器也愈加聪明，以免被生成器欺骗。 「对抗训练是继切片面包之后最酷的事情。」- Yann LeCun 当判别器不能区分 $p_g$ 和 $p_data$，即 $D（x，θ_d）= 1/2$ 时，训练过程停止。达成生成器与判别器之间判定误差的平衡。 ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:2:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"例如 我们应该获得每个标记的特征表示，但是应用常规机器学习和深度学习方法（包括卷积神经网络）存在一些问题： 它们需要大量标注图像； 商标没有标注； 标记无法从数据集分割出去。 这种新方法显示了如何使用 GAN 从商标的图像中提取和学习特征。在学习每个标记的表征之后，就可以在扫描文档上按图形搜索。 ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:3:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"生成模型具有模拟真实数据样本的性能 ","date":"2018-10-26","objectID":"/posts/old/deep-learning/gans.html:4:0","tags":["gan","dl"],"title":"survey-of-gans","uri":"/posts/old/deep-learning/gans.html"},{"categories":["dl"],"content":"tutorial 地址: pytorch: Training a Classifier. 当使用新的数据集进行测试时, 出现的问题及解决的方法. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:0:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 1 error: RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 484 and 549 in dimension 2 at /pytorch/aten/src/TH/generic/THTensorMath.cpp:3616 location: images, labels = data_iter.next() solution: 数据集中的图像大小不一致. 需要使用`transforms.Resize([height, width])`把所有图像缩放到同一大小. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:1:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 2 error: RuntimeError: invalid argument 2: size '[-1 x 400]' is invalid for input with 719104 elements at /pytorch/aten/src/TH/THStorage.cpp:80 location: x = x.view(-1, 16 * 5 * 5) solution: `Tensor.view()` 相当于 `numpy.reshape()` 方法, 即重塑形状. 其中`-1`表示依据其他维度进行推理得出的维度. 这里的参数需要计算得出, 不同的输入尺寸需要计算对应的参数! --- 我们来计算一下, 计算公式见下面的图片. --- input size = 3*32*32 - class Net_t1(nn.Module): def __init__(self): super(Net_t1, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = self.pool(x) x = F.relu(self.conv2(x)) x = self.pool(x) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x - 1 torch.Size([4, 6, 28, 28]), conv1, (32 - 5) / 1 + 1 = 28, padding=0 2 torch.Size([4, 6, 14, 14]), pool, 28 / 2 = 14 3 torch.Size([4, 16, 10, 10]), conv2, (14 - 5) / 1 + 1 = 10, padding=0 4 torch.Size([4, 16, 5, 5]), pool, 10 / 2 = 5 5 torch.Size([4, 400]), view, 16 * 5 * 5 = 400 6 torch.Size([4, 120]), full_connect 7 torch.Size([4, 84]), full_connect 8 torch.Size([4, 10]), full_connect --- input size = 3*224*224 - class Net_t2(nn.Module): def __init__(self): super(Net_t2, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 53 * 53, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 31) def forward(self, x): x = F.relu(self.conv1(x)) x = self.pool(x) x = F.relu(self.conv2(x)) x = self.pool(x) x = x.view(-1, 16 * 53 * 53) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x - 1 torch.Size([4, 6, 220, 220]), conv1, (224 - 5) / 1 + 1 = 220, padding=0 2 torch.Size([4, 6, 110, 110]), pool, 220 / 2 = 110 3 torch.Size([4, 16, 106, 106]), conv2, (110 - 5) / 1 + 1 = 106, padding=0 4 torch.Size([4, 16, 53, 53]), pool, 106 / 2 = 53 5 torch.Size([4, 44944]), view, 16 * 53 * 53 = 44944 6 torch.Size([4, 120]), full_connect 7 torch.Size([4, 84]), full_connect 8 torch.Size([4, 31]), full_connect ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:2:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 3 error: RuntimeError: Assertion `cur_target \u003e= 0 \u0026\u0026 cur_target \u003c n_classes' failed. at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93 location: loss = criterion(outputs, labels) solution: 预测的标签向量和实际的标签向量维度不一致! 设置输出层(最后一层)神经元个数为真实的标签个数. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:3:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Problem 4 error: RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight' location: net = Net().to(device) solution: 在做计算时, 需要把所有需要计算的量都放在`device`上面. 因此不仅网络需要放在`device上面`, `inputs`和`labels`也要放在`device`上面. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:4:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"其他 torchvision.datasets.ImageFolder()会自动加载标签信息. 可以通过上述语句返回的对象调用len(dataset)返回样本个数, 调用dataset.classes返回标签集合. 2018-10-30 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:5:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"pytorch中的可训练性设置 在代码中看到两种设置 # method 1 for param in base_network.parameters(): param.requires_grad = False # method 2 base_network.train(False) 字面意思都是不训练base_network, 但是两个训练的结果不同. ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"requires_grad 是pytorch中变量自动求导的一个属性[AUTOGRAD MECHANICS]. 当设置为False时, 反向传播时不使用梯度更新变量. 他的作用是用来冻结模型中的部分(freeze part of your model). ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:1","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Module.train(mode) pytorch doc: Module.train(mode) 针对于特有模型的特有表现, 比如Dropout, BathNorm等模型中, 不是需要梯度更新的参数(Dropout: mean, std). Even the parameters are the same, it doesn’t mean the inferences are the same. For dropout, when train(True), it does dropout; when train(False) it doesn’t do dropout (identitical output). And for batchnorm, train(True) uses batch mean and batch var; and train(False) use running mean and running var. [link] For dropout (there’s even no parameter in dropout), the dropout position is changing when train is True. For BatchNorm, the train(True) will use the batch norm instead of running_mean and running_var and also running_mean and running_var will also change. [link] A layer doesn’t have requires_grad, only Variables have. running_mean and running_var are buffers, and are updated during forwarding. I assume train(True) will still use the batch mean and batch var. [link] ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:2","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"如何固定预训练的ResNet I am wondering whether to set .eval() for those frozen layers since it may still update its running mean and running var. [link] Setting .requires_grad = False should work for convolution and FC layers. But how about networks that have instanceNormalization? Is setting .requires_grad = False enough for normalization layers too? [link] 当需要固定要预训练的ResNet, 相当于只做预测任务. 因此只需把模型的状态设置为.eval()即可. for k in range(200): # Make a prediction based on the current network weights net.train() # Set to training mode pred_tr = net.forward(z_tr) # Pass in input loss_tr = lossfn(pred_tr, y_tr) # Compute error between prediction and target # Optimize optimizer.zero_grad() # zero the gradient buffers loss_tr.backward() # Run a backward pass through the network optimizer.step() # Update your network parameters # Display loss \u0026 results on test data net.eval() # Set to eval mode pred_te = net.forward(z_te) loss_te = lossfn(pred_te, y_te) # Compute error between prediction and target print('Iter: {}, Training loss: {}, Test loss: {}'.format(k, loss_tr.data[0], loss_te.data[0])) # ref: https://courses.cs.washington.edu/courses/cse490r/18wi/lecture_slides/02_16/pytorch-tutorial.py refs: https://stackoverflow.com/a/48270921/6494418 https://stackoverflow.com/questions/50233272/pytorch-forward-pass-changes-each-time https://discuss.pytorch.org/t/batchnorm-eval-cause-worst-result/15948/6 https://github.com/bethgelab/foolbox/issues/74 https://courses.cs.washington.edu/courses/cse490r/18wi/lecture_slides/02_16/pytorch-tutorial.py pytorch save model 2018-10-31 当数据为图片时, 并且图片的标签是按照文件夹表示的, 使用torchvision.datasets.ImageFolder()读取数据后, 使用torch.utils.data.DataLoader()配置数据时, 一定要加入参数shuffle=True, 不然网络无法训练! 因为一个批量数据中可能就只有一个类别, 无法反向传播, 致使参数不下降, 或者为nan. 若使用GPU进行训练, 在读取DataLoader时, 把数据加载到GPU, 而不是在iteration时加入GPU, 将大大提升运行时间! 2018-11-13 tensor() 是不能直接和int, 等非tensor类型计算的, 计算结果会成0 2018-11-16 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:6:3","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"官网教程 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"pytorch是什么? 基于python的科学计算工具包: 基于GPU计算的numpy的替代物 深度学习研究平台 tensor 就是numpy的ndarray, 不同之处在于基于GPU的tensor能加速计算. torch.Tensor is the central class of the package. If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call .backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad attribute. Tensor是核心的数据结构 .requires_grad用来追踪Tensor是否需要计算每个算子的梯度 .backward()用来计算梯度 function Tensor和Function是相互联系的, 构成了一个非循环图, 它编码了完整的计算历史. autograd pytorch中所有神经网络的核心是autograd. gradient 反向传播(backprop)阶段, 损失是一个标量(scalar) 因为损失函数也是计算图中的一部分(最上层部分), 然后通过梯度分布在各个label上 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:1","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"neural networks A typical training procedure for a neural network is as follows: Define the neural network that has some learnable parameters (or weights) Iterate over a dataset of inputs Process input through the network Compute the loss (how far is the output from being correct) Propagate gradients back into the network’s parameters Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate* gradient define the networks You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function. basic classes Recap: torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor. nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc. nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to aModule. autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensoroperation, creates at least a single Function node, that connects to functions that created a Tensor and encodes its history. backprop To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients. Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward. net.zero_grad() # zeroes the gradient buffers of all parameters loss.backward() # backprop, calculate gradients optimizer.step() # Does the update the weight ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:2","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"training a classifier 代码框架: loading and normalizing data define the neural network define loss function and optimizer train the network test the network ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:7:3","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Github: pytorch-tutorial 2018-12-10 对于tensor.detach()的理解. pytorch想做gpu加速版的numpy，取代numpy在python中科学计算的地位。 pytorch的python前端在竭力从语法、命名规则、函数功能上与numpy统一，加持的自动微分和gpu加速功能尽可能地在吸引更大范围内的python用户人群。 [Link] 因此, 在使用pytorch的时候, 仅需要注意自动微分就行了! 而tensor.detach()就是解决禁用自动微分的方法[Link]. (与tensor.clone()区别, tensor.clone()保持了源tensor的requires_grad) 简单理解, 就是把计算图中的一部分拆解下来, 而这部分不需要自动微分. update 作用: 利用detach截断梯度流[Link] 返回一个新变量，与当前计算图分离。结果将永远不需要改变。 如果输入是易失的，输出也将变得不稳定。 返回的 Variable 永远不会需要梯度。 参考: https://discuss.pytorch.org/t/clone-and-detach-in-v0-4-0/16861/2 https://blog.csdn.net/u012436149/article/details/76714349 https://blog.csdn.net/Hungryof/article/details/78035332 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:8:0","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"torch.Tensor.register_hook[link] register_hook(hook)[SOURCE] Registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -\u003e Tensor or None The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. 登记一个钩子, 在反向传播是调用! refs: https://discuss.pytorch.org/t/solved-reverse-gradients-in-backward-pass/3589 https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94 2018-12-13 比赛心得和pytorch（等）踩得坑[Link] 2018-12-14 20:35:33 在使用某个工具之前, 一定要先看看别人已经踩过那些坑. 比如说使用github上面的开源代码, 先要看看issue里面别人踩过的坑, 然后自己尽量避免, 或者早有准备. 知乎上有一个问题, 里面的回答也非常有建设性: PyTorch 有哪些坑/bug？ 里面的一些回答也非常的不错, 比如: 总结一个代码模板 2019-1-7 22:08:19 又找到一个不错的教程 https://github.com/chenyuntc/pytorch-book 作者陈云, 北邮的研究生, 著有\u003c深度学习框架PyTorch：入门与实践\u003e, 热爱分享, 知乎和github都有不错的干货. 2019-1-9 17:14:17 https://mp.weixin.qq.com/s/mPmFOm32-ipbiIp8mPSd-A 黄海广老师对官网1.0版本教程的翻译 2019-1-13 15:31:59 在XXXLoss的前面不要加softmax? 有些损失需要加, 有些损失已经包含了softmax的计算. 具体来讲 nn.BCELoss前面需要加nn.Sigmoid(), 并且输出一维向量 nn.BCEWithLogitsLoss相当于(nn.Sigmoid() + nn.BCELoss), 因为损失函数包含了归一化 nn.CrossEntropyLoss不需要加nn.Softmax(dim=1), 因为损失函数里面包含了归一化 参考: pytorch loss function 总结 ","date":"2018-10-24","objectID":"/posts/old/deep-learning/pytorch_tutorial.html:8:1","tags":["dl","pytorch"],"title":"pytorch tutorial","uri":"/posts/old/deep-learning/pytorch_tutorial.html"},{"categories":["dl"],"content":"Nvidia https://www.nvidia.cn/object/what-is-gpu-computing-cn.html ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:1:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"GPU 与 CPU 性能比较 理解 GPU 和 CPU 之间区别的一种简单方式是比较它们如何处理任务。CPU 由专为顺序串行处理而优化的几个核心组成，而 GPU 则拥有一个由数以千计的更小、更高效的核心（专为同时处理多重任务而设计）组成的大规模并行计算架构。 ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:2:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"CPU串行处理 VS GPU并行处理 总的来说在于核心数上面的优势, CPU一般核心数是较少了(Intel Core i9-7980XE @ 2.60GHz, 18core), 而GPU核心数较多(NVIDIA TESLA V100/CUDA 5120) ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:3:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"参考 http://itianti.sinaapp.com/index.php/cpu http://itianti.sinaapp.com/index.php/gpu https://ark.intel.com/compare/126699,120496 https://www.nvidia.com/zh-cn/deep-learning-ai/ https://item.jd.com/31346484884.html [京东NVIDIA TESLA V100介绍] ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:4:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"cuda CUDA 是 NVIDIA 发明的一种并行计算平台和编程模型。它通过利用图形处理器 (GPU) 的处理能力，可大幅提升计算性能。 https://www.geforce.cn/hardware/technology/cuda ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:5:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"tensor core https://www.nvidia.com/en-us/data-center/tensorcore/ ","date":"2018-10-23","objectID":"/posts/old/deep-learning/how_gpu_accelerate_compute.html:6:0","tags":["dl"],"title":"GPU是如何加速计算的?","uri":"/posts/old/deep-learning/how_gpu_accelerate_compute.html"},{"categories":["dl"],"content":"传说BERT牛皮得不行, 好奇看了看. 里面用到了Transformer Block, 这是什么结构? 其实也就是Attention as all you need的Transformer. 之前读Attention as all you need 也是云里雾里的, 今天又再看了看这个Transformer的结构. 首先说说attention 的原理: 将query 和key-value 对的集合 映射到输出 (将query 和key 计算出一个关于value 的weight (也就是attention), 然后输出) Transformer也就是attention的具体实现, 总的有两种结构: Scaled Dot-Product Attention Multi-Head Attention ","date":"2018-10-18","objectID":"/posts/old/deep-learning/attention.html:0:0","tags":["dl","attention"],"title":"Attention","uri":"/posts/old/deep-learning/attention.html"},{"categories":["dl"],"content":"Scaled Dot-Product Attention 右边的结构图中, 每个方框对应一个算子, 可以和公式中的每个计算对应 weight 就是attention. 就是对于value有不同的权重(attention) ","date":"2018-10-18","objectID":"/posts/old/deep-learning/attention.html:0:1","tags":["dl","attention"],"title":"Attention","uri":"/posts/old/deep-learning/attention.html"},{"categories":["dl"],"content":"Multi-Head Attention multi-head 就是将原始的向量拆分为多个子向量(或者做不同的映射成多个向量) 然后每个子向量分别做scale dot-product attetion 然后把计算的结果进行连接, 再做一次映射作为输出 // multi-head 相当于拆分后子空间的scale dot-product attention的集成(stacking), // 能学习到更多的参数, 支持并行 ","date":"2018-10-18","objectID":"/posts/old/deep-learning/attention.html:0:2","tags":["dl","attention"],"title":"Attention","uri":"/posts/old/deep-learning/attention.html"},{"categories":["reading"],"content":"2018-11-20 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:1:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"数据标注|人工智能背后的人 之前有了解过ImageNet有过巨大的标记成本, 找了些资料看了看, 这篇文章的信息量还是非常巨大的, 无论是从人工智能的技术角度, 还是社会角度, 都有所涉及. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:2:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"人工智能技术 人工智能技术还存在着许多的痛点与挑战, 这篇文章就从数据标注这个角度讲了讲. 数据标注的成本巨大 数据标注的质量参差不齐 因此, 也引发了如何解决数据标注的问题 小标记数据如何学习 迁移学习 半监督学习 无监督学习 数据增强 对于标注质量的参差不齐如何提升模型的性能 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:2:1","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"社会角度 数据标注的工作人员一般为生活在社会的底层, 然而这部分工作也是付出了血汗, 但是反馈并不高的行业, 如何解决这样的社会问题, 是需要投入更多教育, 让弱者也能更轻松的工作. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:2:2","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-10-30 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:3:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"Five Interview Questions to Predict a Good Data Scientist 想找数据科学的相关岗位, 看看招聘者是如何总结的. 要么有较高的文聘, 要么有特定领域有多年的经验. 有较好的编码能力. 对机器学习的基础知识有较好的掌握. 对数据科学的热情, 是否参加过比赛, 会议发言, 写书或文章. 方法论的形成, 失败后的总结. 对新问题的数据建模问题. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:4:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-10-26 https://weibo.com/1402400261/GEmL0cQoc 𝒎𝒊𝒏 𝑮 𝒎𝒂𝒙 𝑫 𝔼𝒙 [𝒍𝒐𝒈 𝑫 (𝒙))] + 𝔼𝒛 [𝒍𝒐𝒈(𝟏 − 𝑫(𝑮(𝒛)))] GANs: 最小的生成误差, 最大的判别误差 awesome GANs!!!😄 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:5:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-10-18 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:6:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"博士五年之后五年的总结（其一） $技术, 感悟 经验的积累, 方法的总结至关重要. 作为毕业五年的优秀毕业生, 他都有些什么感悟呢? 读什么东西，就成为什么样的人。 在日积月累之下，做或者不做这些，会让每个人最终成为不一样的人。 所以多看点动脑的内容，就不会让大脑生锈。做科研一个比较好的地方是工作本身并不重复，而是一直在开拓边界，这样自然会有更多的动脑时间。在闲暇时间，我经常会多看知乎上做数学和物理的同学们的回答，最好有几个不懂的名词需要自己去查去想想，手机上有个刷Arxiv的app经常看看，看一看一些优秀的github代码，也会动手刷刷题。 作者从多个角度总结了自己的经验 选方向？先要控制自己阅读的入口 如何选方向 如何抓紧时间 如何坚持一个长期的方向 作者的文笔是相当不存的, 简单而充实. 另外, 还有其他的后续的几篇, 值得阅读 远东轶事 ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:7:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["reading"],"content":"2018-11-15 [LINK] 曾经买了一本精要主义, 当然这本书中很多方法都是有效的, 不过主要还是在于吸收与执行. 如果只是读过而没有实践, 那么一切都是徒劳的大白话而已. ","date":"2018-10-18","objectID":"/posts/old/reading-web/web.html:8:0","tags":["reading"],"title":"web reading","uri":"/posts/old/reading-web/web.html"},{"categories":["随笔"],"content":"一 前些天又准备开始写那个计划了很久的小软件。虽然是一个小软件，但是对于我这种技术还没到家的人也显得有些困难，而且时间有限，写得一点也不舒服。好不容易在网上找了一个，功能完全能满足我的需要。但是因为是歪果仁写的，很多功能需要依赖Google Services（你要知道，在天朝是不允许的）另外还有一些界面不是很满意的地方，然后就想尽办法想去改变这些。为了达到自己心中的状态，花费了一天时间，也没有达到自己满意的状态（虽然有一些改观）。 ","date":"2016-07-08","objectID":"/posts/old/essay/stay-silence.html:1:0","tags":["感受"],"title":"别闹心","uri":"/posts/old/essay/stay-silence.html"},{"categories":["随笔"],"content":"二 需要一本书，但是已经绝版了。某宝有一些卖的，大多数都是二手或者盗版。看上了一家旧货店（明显标注正版），耐心询问（正版且比较干净整洁），店家也给了肯定答复。于是决定购入，还顺便买了一本其他的书。今天收到书了，XXX一本是盗版，印刷质量实在不敢恭维，另一本也说不上整洁（十分讨厌书上无规则的线条）。 ","date":"2016-07-08","objectID":"/posts/old/essay/stay-silence.html:2:0","tags":["感受"],"title":"别闹心","uri":"/posts/old/essay/stay-silence.html"},{"categories":["随笔"],"content":"总结 可以改变的，一定要努力做到更好 不能改变的，也要舒舒服服的接受 弄得自己身心俱累，真是愚笨 ","date":"2016-07-08","objectID":"/posts/old/essay/stay-silence.html:3:0","tags":["感受"],"title":"别闹心","uri":"/posts/old/essay/stay-silence.html"},{"categories":["机器学习"],"content":"k-NN(k-Nearest Neighbors) k-近邻算法 概述 k-近邻算法采用测量不同的特征值之间的距离方法进行分类 k-近邻算法的一般流程 收集数据：可以使用任何方法 准备数据：距离计算所需要的数值，最好是结构化的数据格式 分析数据：可以使用任何方法 训练算法：此步骤不适用于k-近邻算法 测试算法：计算错误率 使用算法：首先需要输入样本数据和结构化的输出结果，然后使用k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理 对未知类别属性的数据集中的每个点依次执行以下操作 计算已知类别数据集中的点与当前点的距离 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在类别的出现频率 返回前k个点出现频率最高的类别作为当前点的预测分类 ","date":"2016-05-14","objectID":"/posts/old/machine-learning/knn-in-c.html:1:0","tags":["算法","机器学习"],"title":"KNN算法的C语言实现","uri":"/posts/old/machine-learning/knn-in-c.html"},{"categories":["机器学习"],"content":"按照上述步骤，可以实现k-近邻算法 k-近邻算法的C语言实现 #include \u003cstdlib.h\u003e#include \u003cstdio.h\u003e#include \u003cmath.h\u003e #define SIZE_ATTR 3 /* 属性维度 */#define SIZE_TRAIN 500 /* 训练集大小 */#define SIZE_TEST 500 /* 测试集大小 */#define K 7 /* 所选k值 */ #define FILE_TRAIN \"train.txt\" /* 记录所构成的结构体变量 */ typedef struct _DataVector { int id; /* 标号 */ float attr[SIZE_ATTR]; /* 属性 */ int label; /* 类别 */ } DataVector; /* 把记录中的属性换成距离后的结构体变量 */ typedef struct _DistanceVector { int id; /* 标号 */ int label; /* 类别 */ float distance; /* 距离 */ } DistanceVector; /* 属性的结构体变量 可以先对属性值做一个分析，再做下一步针对性处理（如归一化特征值处理） */ typedef struct _AttrValue { float max; /* 属性的最大值 */ float min; /* 属性的最小值 */ float length; /* 属性的长度 */ } AttrValue; /* 定义全局变量 */ DataVector trainSet[SIZE_TRAIN]; /* 训练集 */ DataVector testSet[SIZE_TEST]; /* 测试集 */ DistanceVector knn[SIZE_TRAIN]; /* 距离存储 */ AttrValue av[SIZE_ATTR]; /* 属性的属性 */ /* 从文件中加载数据到内存 */ void loadDataFromFile(FILE *fp, char *fileName, DataVector *dv, int length) { int i, j; if ((fp = fopen(fileName, \"r\")) == NULL) { printf(\"open \\\"%s\\\"failured!/n\", fileName); exit(1); } for (i = 0; i \u003c length; ++i) { for (j = 0; j \u003c SIZE_ATTR; ++j) { fscanf(fp, \"%f \", \u0026dv[i].attr[j]); } fscanf(fp, \"%d\\n\", \u0026dv[i].label); } fclose(fp); } /* 准备数据 */ void loadData() { FILE *fp = NULL; loadDataFromFile(fp, FILE_TRAIN, trainSet, SIZE_TRAIN); loadDataFromFile(fp, FILE_TRAIN, testSet, SIZE_TEST); printf(\"loading data success!\\n\"); } /* 数据分析（预处理） 计算每个属性长度，为归一化特征值准备 */ void preProcess() { int i, j; /* 初始化 */ for (i = 0; i \u003c SIZE_ATTR; ++i) { av[i].max = trainSet[0].attr[i]; av[i].min = trainSet[0].attr[i]; } /* 计算属性最大最小值 */ for (i = 0; i \u003c SIZE_TRAIN; ++i) { for (j = 0; j \u003c SIZE_ATTR; ++j) { if (trainSet[i].attr[j] \u003e av[j].max) { av[j].max = trainSet[i].attr[j]; } else if (trainSet[i].attr[j] \u003c av[j].min) { av[j].min = trainSet[i].attr[j]; } } } /* 计算属性长度 */ for (i = 0; i \u003c SIZE_ATTR; ++i) { av[i].length = av[i].max - av[i].min; } } /* 归一化特征值 公式：newValue = (oldValue - min) / (max - min) */ float autoNorm(float oldValue, AttrValue *av) { return (oldValue - (av-\u003emin)) / (av-\u003elength); } /* 距离计算 这里计算的是欧式距离 */ float calcDistance(DataVector d1, DataVector d2) { float sum = 0.0; float newValue; int i; for (i = 0; i \u003c SIZE_ATTR; ++i) { newValue = autoNorm((d1.attr[i] - d2.attr[i]), av+i); sum += newValue * newValue; } return (float) sqrt(sum); } /* 把每个数据的属性向量转化为距离 */ void transDistance(DataVector dv) { int i; for (i = 0; i \u003c SIZE_TRAIN; ++i) { /* 对距离进行赋值 */ knn[i].id = i; knn[i].label = trainSet[i].label; knn[i].distance = calcDistance(trainSet[i], dv); } } /* 对所有距离进行排序，选取距离最小的k个数据向量（此处使用直接选择排序） */ void knnSort() { int i, j, k; DistanceVector temp; for (i = 0; i \u003c K; ++i) { k = i; /* 从无序序列中挑出一个最小的元素 */ for (j = i + 1; j \u003c= SIZE_TRAIN; ++j) { if (knn[k].distance \u003e knn[j].distance) { k = j; } } temp = knn[i]; knn[i] = knn[k]; knn[k] = temp; } } /* 预测分类 */ int forecastClassification() { int freq[K] = {0}; int maxFreq = 0; int i, j, k = 0; /* 确定前k个点所在类别出现的概率 这里有点欠妥，因为分类最多能出现k个，出现了重复类别重复计算*/ for (i = 0; i \u003c K; ++i) { for (j = 0; j \u003c K; ++j) { if (knn[j].label == knn[i].label) { freq[i]++; } } } /* 找到最大频率 */ for (i = 0; i \u003c K; ++i) { if (freq[i] \u003e maxFreq) { maxFreq = freq[i]; k = i; } } /* 得到最大频率的类别 */ return knn[k].label; } /* 对测试数据进行测试 */ void test() { int i; int k = 0; loadData(); preProcess(); /* 对每一条测试数据进行计算 */ for (i = 0; i \u003c SIZE_TEST; ++i) { transDistance(testSet[i]); knnSort(); if (testSet[i].label == forecastClassification()) { printf(\"1\"); } else { printf(\"0\"); ++k; } } printf(\"\\nTest end, wrong time is %d, the correct rate is %.2f%%\\n\", k, (float) (SIZE_TEST - k)/SIZE_TEST*100); } void main() { test(); system(\"pause\"); } 参考资料 机器学习实战. Peter Harrington 测试材料 机器学习实战源代码/Ch02/datingTestSet2.txt 另外，代码还有许多可以改进之处，比如当所取K值为偶数时，预测概率为50%时的下一步处理等 ","date":"2016-05-14","objectID":"/posts/old/machine-learning/knn-in-c.html:1:1","tags":["算法","机器学习"],"title":"KNN算法的C语言实现","uri":"/posts/old/machine-learning/knn-in-c.html"},{"categories":["涉猎"],"content":"经过好多天的努力，博客基本上也达到了自己满意的样子。博客以Bootstrap-Blog为蓝本，在此基础上增增减减，修修改改。因为自己并不是很懂网页，大多时候都是摸索和调试，所以费时还是挺多的。现在终于能踏踏实实的写文章了。 之前我也用过很多博客，但是自己却很少认认真真的写过文章。原因之一就是自己文笔不好，思考半天都写不出来一句话。从小学到大学，作文一直就是我的痛点，时间费得最多，效果还不是很好，这也是我可提高之处吧！现在，我想我得尽量多写吧，至少搭建这个博客我还是挺费心的。其实很多时候不必思考过多，随心所欲，应该也是很不错的。 写这篇文章的目的是对我的Hexo之旅做一个小结，也算对这次博客搭建告一段落。 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:0:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"所参考的一些链接 当然，最主要的还是Hexo官网上的资源，包括教程、主题等等。 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:1:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"详细教程 如何搭建一个独立博客——简明Github Pages与Hexo教程 这篇文章十分详细，作者在后面给了很多其他教程的链接。 图灵社区：Hexo合集 这篇文章对于Hexo的使用更加详细，适合想要深度折腾的同学。 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:2:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"增增减减 Hexo，添加返回顶部按钮，如果打不开，请点击这里 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:3:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"一些问题 npm国内被墙的解决方法，应该前两步就能解决。 连接挂掉, 补(http://blog.csdn.net/qq_23329541/article/details/68927747) npm config set strict-ssl false npm config set registry \"http://registry.npmjs.org/\" 被墙的另一个解决方法。 使用taobao npm镜像 npm install -g cnpm --registry=https://registry.npm.taobao.org 搭建 hexo，在执行 hexo deploy 后,出现 error deployer not found:github 的错误 OK, 剩下的就是好好的写文章吧！ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:4:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"160622更新 设置sublime_text为默认编辑器 git config --global core.editor \"'D:/Program Files (x86)/Sublime Text/sublime_text.exe' -w\" 设置Notepad++为默认编辑器 git config --global core.editor \"'D:/Program Files (x86)/Notepad++/notepad++.exe' -multiInst -nosession\" 直接使用命令subl编辑文本 把sublime_text.exe的所在目录添加至环境变量的path中 在Git下直接使用命令 subl filename.txt 就可以直接使用sublime text编辑文本文件了 参考链接: https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:5:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"161104更新 使用上述教程部署网站出现错误 $ hexo d ... nothing to commit, working tree clean bash: /dev/tty: No such device or address error: failed to execute prompt script (exit code 1) fatal: could not read Username for 'https://github.com': Invalid argument FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html Error: bash: /dev/tty: No such device or address error: failed to execute prompt script (exit code 1) fatal: could not read Username for 'https://github.com': Invalid argument ... 解决方法： 修改根目录文件：_config.yml 原有内容： repository: https://github.com/username/username.github.io.git 修改为： repository: git@github.com:username/username.github.io.git 参考链接：http://www.jianshu.com/p/d1fc64c445ce ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:6:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"update for detail steps download git download node.js // gen ssh-key ssh-keygen -t rsa -C \"wangkest@qq.com\" # -C comment 你的ssh名称(本地) // add ssh-key to github // https://github.com/settings/keys (http://blog.csdn.net/keyboardota/article/details/7603630) // verify ssh -T git@github.com // config git config --global user.name \"username\" git config --global user.email \"username@gmail.com\" npm install -g cnpm --registry=https://registry.npm.taobao.org cnpm install -g hexo cnpm install hexo-deployer-git --save // or use taobao mirror // npm config set strict-ssl false // npm config set registry \"http://registry.npmjs.org/\" // npm install -g hexo // npm install hexo-deployer-git --save cd your_hexo_dir hexo init ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:7:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"2017-9-6 update add quick generate_deploy.bat @echo off \u0026 rem not print commond title Hexo \u0026 rem setting title rem cd D:/github/hexo \"D:\\Program Files\\Git\\bin\\sh.exe\" --login -i -c \"cd /d/github/hexo \u0026\u0026 hexo generate --deploy\" pause exit ref: https://stackoverflow.com/questions/5203723/how-do-i-write-a-batch-file-which-opens-the-gitbash-shell-and-runs-a-command-in https://segmentfault.com/q/1010000000263597 https://www.zhihu.com/question/38962022 http://t.cn/RA4BPda update ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:8:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"代码块空行问题 change hexo util file highlight.js file path: your_hexo_dir/node_modules/hexo-util/lib/highlight.js update code for (var i = 0, len = lines.length; i \u003c len; i++) { line = lines[i]; if (tab) line = replaceTabs(line, tab); numbers += '\u003cspan class=\"line\"\u003e' + (firstLine + i) + '\u003c/span\u003e'; content += '\u003cspan class=\"line'; content += (mark.indexOf(firstLine + i) !== -1) ? ' marked' : ''; content += '\"\u003e' + line + '\u003c/span\u003e'; } to code for (var i = 0, len = lines.length; i \u003c len; i++) { line = lines[i]; if (tab) line = replaceTabs(line, tab); // numbers += '\u003cspan class=\"line\"\u003e' + (firstLine + i) + '\u003c/span\u003e'; // content += '\u003cspan class=\"line'; // content += (mark.indexOf(firstLine + i) !== -1) ? ' marked' : ''; // content += '\"\u003e' + line + '\u003c/span\u003e'; numbers += '\u003cspan class=\"line\"\u003e' + (i + firstLine) + '\u003c/span\u003e\\n'; content += '\u003cspan class=\"line\"\u003e' + line + '\u003c/span\u003e\\n'; } ref: http://blog.csdn.net/tobacco5648/article/details/42584653 http://xingwu.me/2014/11/08/Hexo-Code-Block-Bugs-Comments-Style-and-Empty-Lines/ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:9:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"permalink config https://clearsky.me/hexo-permalinks.html http://www.wuliaole.com/post/permalink_and_internal_link_in_hexo/ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:10:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"2018-10-13 update ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加公式 hexo-math已经被抛弃了, 所以使用hexo-math并无任何效果, 使用hexo-renderer-mathjax进行替代 如果你已经安装了hexo-math, 需要先卸载 $ npm uninstall hexo-math --save 然后, 安装hexo-renderer-mathjax $ npm install hexo-renderer-mathjax --save 在主题的配置文件_config.yml中添加下面属性, 配置mathjax # MathJax Support mathjax: enable: true per_page: true cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML OK, 下面是一些测试 行內公式 $\\sin ^{ 2 }{ \\theta +\\cos ^{ 2 }{ \\theta =1 } }$ 行內公式 行外公式 $$\\frac { dy }{ dx } =\\frac { { e }^{ x } }{ 3{ y }^{ 2 } }$$ 行內公式 $\\sin ^{ 2 }{ \\theta +\\cos ^{ 2 }{ \\theta =1 } }$ 行內公式 行外公式 $$\\frac { dy }{ dx } =\\frac { { e }^{ x } }{ 3{ y }^{ 2 } }$$ 参考链接: https://nathaniel.blog/tutorials/make-hexo-support-math-again/ 另外, 推荐一个主题next ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:1","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"mathjax Mixed Content导致不能渲染公式 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:2","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"或者(chrome出现一个拦截图标, 不安全连接) 前面的两个错误Uncaught Reference Error 加载js的顺序不对, 需要先加载jquery, 然后再加载后两个js 第三个问题, 也就是渲染公式 修改[hexo_root]\\node_modules\\hexo-renderer-mathjax\\mathjax.html: 把最后一行 \u003cscript type=\"text/x-mathjax-config\"\u003e MathJax.Hub.Config({ tex2jax: { inlineMath: [ [\"$\",\"$\"], [\"\\\\(\",\"\\\\)\"] ], skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], processEscapes: true } }); MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(); for (var i = 0; i \u003c all.length; ++i) all[i].SourceElement().parentNode.className += ' has-jax'; }); \u003c/script\u003e \u003cscript src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"\u003e\u003c/script\u003e 替换为 \u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML\"\u003e\u003c/script\u003e 即修改了渲染js的地址, 地址来源于官网: https://docs.mathjax.org/en/latest/start.html 参考: https://github.com/hexojs/hexo/issues/3279 ! 尝试了修改hexo下的配置和主题下的配置添加下面内容都无效 mathjax: cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:11:3","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加新页面 https://github.com/hexojs/hexo/issues/1453 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:12:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"hexo template https://hexo.io/docs/templates.html https://hexo.io/zh-cn/docs/helpers.html ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:13:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加目录 article.ejs中添加 \u003c!-- TOC --\u003e \u003c% var mtoc = toc(post.content, {list_number: false}); %\u003e \u003c% if (mtoc) { %\u003e \u003cdiv class=\"toc\"\u003e \u003ch3\u003eContent\u003c/h3\u003e \u003c%- mtoc %\u003e \u003c/div\u003e \u003c% } %\u003e custom.css中添加 .toc { background-color: #eee; padding-left: 25px; } 参考: http://izhaoyi.top/2017/05/30/my-blog/#%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:14:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"windows图床工具 https://jverson.com/2017/05/28/qiniu-image-v2/ 七牛云不支持测试域名了, 需要自己的域名, 后面就没测试了 awesome themes: https://github.com/geekplux/hexo-theme-typing ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:14:1","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"添加 readme https://stackoverflow.com/questions/25258660/how-do-i-add-a-readme-md-file-into-the-root-directory-of-the-generated-blog-by-h/31051913#31051913 注意, 上面很多方法都是不可行的 包含 https://stackoverflow.com/questions/25258660/how-do-i-add-a-readme-md-file-into-the-root-directory-of-the-generated-blog-by-h https://github.com/hexojs/hexo/issues/3158 https://github.com/hexojs/hexo/issues/3248 http://xchb.work/2017/04/08/hexo%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6-skip-render-%E9%85%8D%E7%BD%AE/ https://xuanwo.org/2014/08/14/hexo-usual-problem/ 正确的解决方法 https://hexo.io/docs/configuration#Directory 即README.md文件必须要在source目录下!!! ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:14:2","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"永久链接 https://clearsky.me/hexo-permalinks.html 参考链接 https://juejin.im/entry/5a8079a85188257a6e402c17 ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:15:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"站内搜索 https://hackfun.org/2017/10/04/%E7%BB%99hexo%E6%B7%BB%E5%8A%A0%E6%9C%AC%E5%9C%B0%E6%90%9C%E7%B4%A2%E7%AB%99%E5%86%85%E5%8A%9F%E8%83%BD/ ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:16:0","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["涉猎"],"content":"图片及code linenumber https://gitee.com/wfcrgt/hexo-blog/blob/master/themes/bootstrap/layout/_partial/archive.ejs \u003cdiv class=\"post-img-preview\"\u003e \u003cdiv class=\"post-img-preview-content\"\u003e \u003c%- truncate(strip_html(post.content.replace(/\u003cbr\\\u003e|([\u003cspan\\\u003e]\\d+[\u003c/span\\\u003e])|\u003c\\/.*?\u003e/gi, \" \")), {length: 150, omission: '...'}) %\u003e \u003c/div\u003e \u003c% var img_matched = post.content.match(/\u003cimg [^\u003e]*src=['\"]([^'\"]+)[^\u003e]*\u003e/gi) %\u003e \u003c% if (img_matched) { %\u003e \u003cdiv class='post-img-preview-img'\u003e \u003cimg src=\"\u003c%- \"/p/\" + img_matched[0].split(\"\\\"\")[1] %\u003e\" height=100% width=100% \u003e \u003c/div\u003e \u003c% } %\u003e \u003c/div\u003e [TOC] ","date":"2016-05-14","objectID":"/posts/old/dabble/hexo-note.html:16:1","tags":["hexo","博客搭建"],"title":"Hexo之旅","uri":"/posts/old/dabble/hexo-note.html"},{"categories":["column"],"content":"前方漫漫 愿我成风 欢迎关注 新浪微博: 王虚极 github: colinwke 最后更新：2016-05-13 ","date":"2016-05-05","objectID":"/columns/page-about.html:0:0","tags":["column"],"title":"关于","uri":"/columns/page-about.html"},{"categories":["column"],"content":"友情推荐 ","date":"2016-05-05","objectID":"/columns/good-blog.html:1:0","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"charmby https://charmbyforever.blogspot.com/ 无详细介绍! 还有一个马甲地址charmby ","date":"2016-05-05","objectID":"/columns/good-blog.html:1:1","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"知乎 https://www.zhihu.com/people/iamkissg https://www.zhihu.com/people/rtb_susan https://www.zhihu.com/people/charmby ","date":"2016-05-05","objectID":"/columns/good-blog.html:1:2","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"计算广告 ","date":"2016-05-05","objectID":"/columns/good-blog.html:2:0","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"王喆的机器学习笔记 https://zhuanlan.zhihu.com/wangzhenotes 关注计算广告，推荐系统等机器学习领域前沿知识 广告/推荐 Sr. Research SDE@Hulu ","date":"2016-05-05","objectID":"/columns/good-blog.html:2:1","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"误入机器学习的码农(吴海波) https://zhuanlan.zhihu.com/c_65697563 https://www.zhihu.com/people/wu-hai-bo 机器学习应用感悟. 写作即反思，我见我思. 花名吾加，蘑菇街搜索、推荐排序算法owner，大规模机器学习，自然语言处理 ","date":"2016-05-05","objectID":"/columns/good-blog.html:2:2","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"算法工程师的自我修养(杨旭东) https://zhuanlan.zhihu.com/yangxudong https://www.zhihu.com/people/yang-xu-dong-6 关注互联网领域算法工程师专业能力、综合能力成长。 关注机器学习、人工智能、推荐系统、信息检索、在线广告等领域的经典和前沿技术分享。 ","date":"2016-05-05","objectID":"/columns/good-blog.html:2:3","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"小潄 https://www.zhihu.com/people/xiao-da-xiao-nao-12/posts 阿里妈妈广告算法 ","date":"2016-05-05","objectID":"/columns/good-blog.html:2:4","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"其他 ","date":"2016-05-05","objectID":"/columns/good-blog.html:3:0","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"苏建林 | 科学空间 https://kexue.fm/ 里面很多干货, 主要偏NLP方向 作者本来是学数学的, 后面对NLP方向有较高的兴趣, 所以数学机器非常的棒 作者博客中涉猎特别广, 基本什么都写, 内容精良, 专研精神可贵, 值得关注! ","date":"2016-05-05","objectID":"/columns/good-blog.html:3:1","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"Mingsheng Long http://ise.thss.tsinghua.edu.cn/~mlong/ 清华大学博士, 副教授 transfer learning ","date":"2016-05-05","objectID":"/columns/good-blog.html:3:2","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"LancelotHolmes(懒死骆驼) http://izhaoyi.top/ 机器学习与数据挖掘 机器学习面试 https://github.com/LancelotHolmes/ ","date":"2016-05-05","objectID":"/columns/good-blog.html:3:3","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"lanbing http://lanbing510.info/ 处女座，理工男，博士僧，爱折腾 喜欢编程、阅读、思考、创作 热衷于计算机视觉、机器学习、计算机技术领域。 https://github.com/lanbing510 ","date":"2016-05-05","objectID":"/columns/good-blog.html:3:4","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"Charles Xiao https://www.zhihu.com/people/jian-xiao-69/activities DeepFM [TOC] ","date":"2016-05-05","objectID":"/columns/good-blog.html:3:5","tags":["column"],"title":"博客推荐","uri":"/columns/good-blog.html"},{"categories":["column"],"content":"2021-05 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:1:0","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"李沐: 工作五年反思 李沐大神这篇文章对工作中的价值进行了剖析, 也是工作五年的反思总结, 其中最难为可贵为对自己价值和对他人的价值, 对自己的价值很好理解, 但是对他人的价值才更为重要, 影响的人的数量与影响的程度, 可以是工作产出, 也可以是其他的形式. 专注于最有价值的事情, 在工作和生活中多选择的场景下, 是一个不错的指导方针. 大惊呼, 听君一席话, 胜读十年书; 大惊呼, 书到用时方恨少, 只恨看得少, 听得少, 想得少(雷军: 对校招生的建议); 老板曾对我讲, 要注重个人产出, 我并以产出作为我工作的指导思想; 老板曾又对我讲, 要注重个人发展, 我也为此开始践行. 这也算是对我不同阶段的两点指导建议. 回过头看, 还是因为我的经验不足, 难以支撑自己, 更是需要多看, 多听, 多学, 多想. 在这些大佬的指引下, 希望能够修养自己, 成就更大的价值. 来自遥远的分割线 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:1:1","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"2016-05 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:2:0","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"程序人生 这是雷军1996年在BBS上写的一篇帖子，现在读起来，也别有一番韵味。尤其是那一句“编程不仅仅是技术，还是艺术”更是说出了编程的真谛，我想也是现在的编程者思考的问题。 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:2:1","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"KISS principle KISS即keep it simple， stupid的缩写，它是解决复杂代码的法宝，也是优秀代码的准则。 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:2:2","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"Digital Carpenter Evolves To Geoscientist 这篇文章来自android weekly网站创始人Martin Gauer博客中的一篇文章，主要讲诉了他作为网站前端开发的一个老手转变为地球科学家的过程。这其中是对自己的逐渐的认识，敢于做出改变，然后下定决心和付出行动。 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:2:3","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"Java的第20年：Java和我的故事 一篇跨越20年的回忆史，通过Java的故事线回忆着作者自己的人生。 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:2:4","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"2017-10 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:0","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"Discover Feature Engineering, How to Engineer Features and How to Get Good at It 特征工程是数据挖掘中至关重要的一环. 甚至, 在业界中传有, 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已. 这篇文章中, 不仅详实的叙述了特征工程的概念, 还发散性的引导构建特征的思维, 但谨记, 特征工程即使存在一个模式, 也要努力的跳出这个模式. 我想, 特征工程也许就是一个思维发散的过程. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:1","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"进程与线程的一个简单解释 工厂的例子很形象, 插图也非常的不错! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:2","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"关于 KaiMing He 的两篇论文 https://www.zhihu.com/question/67119841 https://www.zhihu.com/question/57403701 周博磊评论“从现象和问题出发追溯本质的思想” 现今深度学习在机器学习领域的研究和应用中, 处于一个十分火热的状态, 甚至对统计学习呈一种打压的景象. 所以不禁会问, 继续学习统计学习还有必要吗? 可以从周博磊的评论中能得到一点启发: 从现象和问题出发追溯本质的思想. 深度学习的应用主要在cv, speech recognition和nlp问题上, 这类问题一般为表达结构较为复杂的问题. 深度学习简化了提取特征的这个步骤, 而对于一些结构化的数据, 统计学习依然能够, 或者胜任深度学习. 另一方面, 在KDD领域, 统计学习依然占有一席之地, 而KDD更偏重于使用机器学习算法解决实际的问题, 也更注重于特征工程. 最后, 对于统计学习和深度学习的讨论可以参见周志华老师机器学习序言的问题三. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:3","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"裴健当选SIGKDD主席, 研究被引超7万次，他还有一个遗憾 | 专访 数据挖掘的核心是对数据和业务的理解能力和对算法的构建能力。 是不是数据永远是越大越好？对于研究者来说，怎样才算是适合的数据？ 一般来说，数据是越多越好。 深度学习需要大量的数据来产生可以generalize的模型。 在实际应用中，数据往往是有成本的。 有很多应用场景不容易获取大量的高质量数据。 所以说我们需要针对具体问题，获取合适的数据。 在这方面，统计学对数据的采集评价有一系列的方法和原则，值得深入学习。 另一个方面，要很好利用大量的数据，通常需要比较复杂的模型，对计算的要求也相应地比较高，所以我们要根据数据量和应用来选择合适的模型。 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:4","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"数据挖掘最强华人齐聚加拿大，干货满满亮点不断 | KDD 2017 杨强教授提到, 特征比模型更为重要(Big Data is useless unless it can deliver big feature space). 对于Data Minming来讲, 实际也是这么回事, 但是也不能否认模型的重要性, 只不过大多数模型的算法已经成熟了, 而特征则更需要对业务的分析与理解, 所以, 特征才表现出了更加的重要. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:5","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"机器学习该怎么入门？ ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。 一同门一直在哔哔, 现在理解不深, 可能以后会加深理解. (边笑边逃) ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:6","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"Yann LeCun创造的新词“预测学习”将要落脚于GANs？ 如果你沉浸在传统机器学习领域, 又对深度学习了解很少, 那么你可以看看这一篇文章. 首先是Yann LeCun的\"蛋糕比喻\", 形象的描述了人工智能的三大领域: 强化学习, 监督学习和无监督学习. 其次是对于GANs(Generative Adversarail Networks)的描述. GANs是无监督学习的一个方法, 如LeCun所描述的那样: 对抗性网络是“20年来机器学习领域最酷的想法”, GANs的概念充满了想象力, 也让读者有一些引申的思考. 最后, 通过这篇文章也会增进你对深度学习的理解. 最后, 看看GANs生成的图片, 是不是很新奇呢? ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:7","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"清华大学刘知远：在 NLP 领域「做事」兼「发声」 如果你想了解一个人, 你就和他多接触一下吧, 听听他的故事. 文章通过两个项目, 以及为学生, 为老师三方面介绍了刘知远博士, 大神的练成并不简单, 更多的是努力和智谋. 顺便文章还介绍了一些NLP的项目, 值得收藏. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:8","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"KDD 2017最佳论文得主叶艳芳专访：AI时代的互联网安全 – 攻与防的黑白博弈 又一篇关于或学者或导师的访谈. 在研究方面, 机遇与挑战共存. 在导师方面, 站的够高, 看的更远. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:9","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"2017 NBA 选秀有哪些值得关注的新秀？ 走向成功的道路并不平坦, 但是他们胸怀勇气, 披荆斩棘, 坚持不懈, 最后得到了成功. 每个人都有着他们不同寻常的故事, 也造就了他们的成就! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:3:10","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"2017-12 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:4:0","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"很傻很天真的贝叶斯定理 贝叶斯定理是一种思维上的推理方式. 一般的推理是建立在因果关系上的, 对于执果索因, 就需要使用贝叶斯定理. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:4:1","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"F1 比赛中最严重的事故是哪次？ 一些事故让人感到沮丧, 但是, 他们以更坚强的方式活着! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:4:2","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"文本情感分类与深度学习模型 共三篇关于NLP的文章, 每次都是对上次内容的更正. 其中第三篇为一年后的更正, 实在难为可贵! http://kexue.fm/archives/3360/ http://spaces.ac.cn/archives/3414/ http://kexue.fm/archives/3863/ 详细的讲述了作者做情感分析的过程及想法, 内容详实, 对于刚上手NLP分类有很好的引导作用. 本以为是团体运维的博客, 但就是个人博客. 博客内容精良, 专研精神可贵, 涉猎广泛, 值得关注! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:4:3","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"2018-01 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:5:0","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"How to do machine learning efficiently 10秒原则 时间是最贵重的成本. 永远不允许计算超过10s的问题(抽样). 急于成功 更专注一个问题. 由简至繁的构建. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:5:1","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"2018-11 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:0","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"Ask Me Anything session with a Kaggle Grandmaster 作者是一位物理专业毕业的博士生, 后面转向数据科学(DS). 过程中也遇到很多困难, 坚持可热情, 还有总结沉淀下来的方法让他成为了Kaggle Grandmaster. 在这个过程中, 作者是付出了很多精力的. 在全职的工作上要保持比赛的强度, 是工作也比赛的权衡. 当然, 作者也会发费时间在运动, 旅游上面. 但是更多的闲暇时间用在了比赛和学习上. 当最后成为Kaggle Grandmaster时, 作者认为一切都是值得的. 里面有很多比赛方法上的经验, 在打比赛之前和之后都可以看看! 千里之行始于足下, 最困难的一步也就是第一步. 来自于爱可可老师的推荐 “你在Kaggle学到的技能只是你在工业或学术界工作时所需技能的一小部分，那些Kaggle不涉及的技术领域基础教育可能是至关重要的” “要加速磁盘jpeg图像I/O，不应该用PIL，Skimage甚至OpenCV，而是用libjpeg-turbo或PyVips。” “Kaggle技能是我从学术界和其他知识来源获得的一系列技能的有力补充” “高估你的专业、大学等在该行业找工作中的影响是不明智的。一家公司雇用你、愿意付钱给你是希望解决他们面临的问题。学位和专业只是评估能力的参考……” “机器学习领域的论文、竞赛、博客和书籍实在太多，根本看不过来。实际上，当我遇到问题，会专注于查看最近结果并深入研究。完成后，再切换到下个问题，只是掌握缺少实践经验领域的高级知识……NIPS、CVPR等会议，可以很好地代表在目前研究阶段我们能做什么和不能做什么” 附上机器之心翻译[link]版本. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:1","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"从「深度学习」到「深度」学习 | 龙明盛老师专访 近期读了龙明盛老师的几篇论文, 感觉其是一位很nice的年轻老师, 学者. 偶然在网上看到这篇专访, 讲述了龙明盛老师的为师者, 为学者, 为人者的态度和践行. 一位优秀的往往能启发更优秀的学生, 值得学习! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:2","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 吴海波 花名吾加，蘑菇街搜索、推荐排序算法owner，大规模机器学习从业者. 醍醐灌顶! 前辈走出的路, 总结的经验一定要好好的看, 尤其是如此优秀的文章! 首先文章以算法工程师面试最为引子, 算法工程师首先需要能解决工程问题, 因此代码能力是必不可少的, 要求也是严格的, 其次是机器学习理论和实践上的能力. 同时具有上面两个能力才能称作是一个合格的算法工程师. 但是作者并没有局限在于算法工程师上面, 而是在于职业的操守. 理论 + 实践, KPI, ROI, 这些才是算法工程师要面对的问题. 而为了有产出, 必须要对算法进行一些探索, 工程化. 最后是作者也是深耕于搜索, 推荐排序的算法工程师, 而且乐于分享自己的想法, 推动行业的前进, 值得关注! 作者关于此问题写了两篇 其一: 论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 其二: 算法工程师又不只是工程师 另外还有作者还引用了多篇质量非常高的关于推荐排序的博文和专栏, 包含了杨旭东的几篇文章, 之前也有看过. 另外还引用了很多领域的论文. 最后, 附上吴海波的专栏误入机器学习的码农, 以及杨旭东的专栏算法工程师的自我修养. ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:3","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"爬虫租房, 是一个不错的注意 另外还有一个开源的代码 https://github.com/lorien/awesome-web-scraping/blob/master/python.md 又发现了一个 https://github.com/XuefengHuang/lianjia-scrawler ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:4","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"一份写给NLP研究者的编程指南 开发经验非常的重要! 快速原型开发!!!!!!!!!然后再重构与模块化!!!!!!!!!! 写安全的工程代码 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:5","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"贫富差距是如何逐渐扩大的 总结一下: 先投资自己 出发点是基于长期的回报而不是即时的满足 多种收入来源 不断的投资自己 从不指责, 对自己的失败承担全部责任 不睡懒觉 清晰的愿景和目标 进步后娱乐 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:6","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"学习Git, 这里有个非常耐撕的工具 demo hands on ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:7","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"善用资源，找书，看书，而不是囤书 依然是那句话, 你拥有的资源多少并不重要, 如果你不知道利用的话, 一切都是无用的. 深度强化学习综述: https://weibo.com/1402400261/H4rJvt4Fk 机器学习在线课程: https://weibo.com/1402400261/H4rH3EhXb 机器学习相关学习资料: https://github.com/iamalotaibi/Machine-Learning ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:8","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"我的八年博士生涯 – 王赟 Maigo 学术篇 娱乐篇 在知乎上, 感觉王赟是一个非常平易近人的人, 他通过两个方面总结了他的博士生涯, 看上去也非常的评议近人. 在学术上, 做的研究和项目好像也没有很大的高大上, 但是却踏踏实实, 虽然走了很多弯路, 但是也有一些不错的收获. 在完成博士论文可谓惊险, 正是这种惊险也使得博士生涯更具有一番味道. 在娱乐上, 可能是比较玩得开的人, 也愿意玩的人, 让整个博士生涯变得不那么的单调, 很好! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:9","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"斯坦福机器学习课程“CS 229 - Machine Learning”速查表(中文版) 全面, 简洁, 整理得很好! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:10","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"图文详解如何利用Git+Github进行团队协作开发 Git在版本控制中是非常重要的工具, 这篇文章把多分支开发说的很详细! ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:11","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"张小龙总结微信8年 https://mp.weixin.qq.com/s/F1LWh9UkeClSgZjE0O5A8Q http://wx2.sinaimg.cn/large/6106a4f0ly1fz1cqvtg3xj21bf2s3n3o.jpg ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:12","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"高效学习 https://weibo.com/1707613190/HbgL0n5E0 ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:13","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/516385585 这是一个系列视频, 指的get [TOC] ","date":"2016-05-05","objectID":"/columns/good-post-2021.html:6:14","tags":["column"],"title":"阅读推荐2021","uri":"/columns/good-post-2021.html"},{"categories":["column"],"content":"2023-04 人类的悲欢并不相通，我只觉得他们吵闹 – 鲁迅 https://www.bilibili.com/video/BV14t411f7X7 天上梦龙, 地上酷玩 Queue: 5 ImagineDragons: 5 ColdPlay: 4 TwoStepsFromHell: 3 FallOutBoy: 3 OneRepuplic: 1 Beyond: 16 许巍: 9 ","date":"2016-05-05","objectID":"/columns/good-except.html:1:0","tags":["column"],"title":"阅读摘抄","uri":"/columns/good-except.html"},{"categories":["column"],"content":"2023-02 https://so.gushiwen.cn/mingju/juv_0678a4494431.aspx 眼看他起朱楼，眼看他宴宾客，眼看他楼塌了。 摘自《桃花扇·续四十出·余韵》 解释：看着人家建起那栋美丽的楼阁，看着人家在楼阁上宴请宾客，一直看到那楼阁倒塌了。 赏析：这句话表达了世事沧桑，世道多变，时过境迁、荣华富贵，功名利禄不过过眼烟云的感慨和情怀。 https://zhuanlan.zhihu.com/p/599134248 把时间分给睡眠，分给书籍，分给运动，分给花鸟树木和山川湖海，分给你对这个世界的热爱，而不是将自己浪费在无聊的人和事上。当你开始做时间的主人，你会感受到平淡生活中喷涌而出的平静的力量，至于那些焦虑与不安，自然烟消云散。 ","date":"2016-05-05","objectID":"/columns/good-except.html:2:0","tags":["column"],"title":"阅读摘抄","uri":"/columns/good-except.html"},{"categories":["column"],"content":"2019-01 具有远大抱负，不但应该有改变自己的意愿，更加应该具备改变世界的情怀。 ","date":"2016-05-05","objectID":"/columns/good-except.html:3:0","tags":["column"],"title":"阅读摘抄","uri":"/columns/good-except.html"},{"categories":["column"],"content":"2018-11 我是个俗气至顶的人. 见山是山, 见海是海, 见花便是花. 唯独见了你. 云海开始翻涌, 江潮开始澎湃, 昆虫的小触须挠着全世界的痒. 你无需开口, 我和天地万物便通通奔向你. – 你听过哪些让人心呯呯跳的情话亦或者诗句 ","date":"2016-05-05","objectID":"/columns/good-except.html:4:0","tags":["column"],"title":"阅读摘抄","uri":"/columns/good-except.html"},{"categories":["column"],"content":"2016-05 被克服的困难就是胜利的契机（契诃夫） 于千万人之中，遇见你要遇见的人。于千万年之中，时间无涯的荒野里，没有早一步，也没有迟一步，遇上了也只能轻轻地说一句：你也在这里吗？（张爱玲） 如果你不调戏女人，她说你不是一个男人；如果你调戏她，她说你不是一个上等人。（张爱玲） [TOC] ","date":"2016-05-05","objectID":"/columns/good-except.html:5:0","tags":["column"],"title":"阅读摘抄","uri":"/columns/good-except.html"},{"categories":["column"],"content":"2021-05 真感觉太崇拜雷军了, 简直快成为了他的教徒了! 今天偶人看了两个视频: ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:1:0","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"许知远vs雷军《灵魂对话》 这里很打动的两个点是 要适当跳出自己的舒适圈, 说服自己与培养兴趣, 只有深入调研, 才有发言权. 创业, 要有信仰, 自己相信, 还要别人相信. 所有, 就有了说服自己和说服别人, 要让别人不仅能和你站在同一艘船上, 而且要心甘情愿, 信心满满的, 全力以赴的支持你. 不仅想到了在公司低谷的时候, 老大是如何的所谓别人口中的画饼, 现在也慢慢理解了, 如果不能说服你认真努力工作, 那leader用来干嘛呢? 所以, 不仅是创业, 到后面带领团队也一样, 一定要有一个很好的说服能力, 能够很好的组织工作, 高效工作. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:1:1","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"雷军：创业第一课 内容很多, 干货也很多, 没有看完, 创业必看! 我是一个不同时代的分界线 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:1:2","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"2016-05 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:2:0","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"程序人生 这是雷军1996年在BBS上写的一篇帖子，现在读起来，也别有一番韵味。尤其是那一句“编程不仅仅是技术，还是艺术”更是说出了编程的真谛，我想也是现在的编程者思考的问题。 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:2:1","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"KISS principle KISS即keep it simple， stupid的缩写，它是解决复杂代码的法宝，也是优秀代码的准则。 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:2:2","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"Digital Carpenter Evolves To Geoscientist 这篇文章来自android weekly网站创始人Martin Gauer博客中的一篇文章，主要讲诉了他作为网站前端开发的一个老手转变为地球科学家的过程。这其中是对自己的逐渐的认识，敢于做出改变，然后下定决心和付出行动。 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:2:3","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"Java的第20年：Java和我的故事 一篇跨越20年的回忆史，通过Java的故事线回忆着作者自己的人生。 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:2:4","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"2017-10 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:0","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"Discover Feature Engineering, How to Engineer Features and How to Get Good at It 特征工程是数据挖掘中至关重要的一环. 甚至, 在业界中传有, 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已. 这篇文章中, 不仅详实的叙述了特征工程的概念, 还发散性的引导构建特征的思维, 但谨记, 特征工程即使存在一个模式, 也要努力的跳出这个模式. 我想, 特征工程也许就是一个思维发散的过程. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:1","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"进程与线程的一个简单解释 工厂的例子很形象, 插图也非常的不错! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:2","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"关于 KaiMing He 的两篇论文 https://www.zhihu.com/question/67119841 https://www.zhihu.com/question/57403701 周博磊评论“从现象和问题出发追溯本质的思想” 现今深度学习在机器学习领域的研究和应用中, 处于一个十分火热的状态, 甚至对统计学习呈一种打压的景象. 所以不禁会问, 继续学习统计学习还有必要吗? 可以从周博磊的评论中能得到一点启发: 从现象和问题出发追溯本质的思想. 深度学习的应用主要在cv, speech recognition和nlp问题上, 这类问题一般为表达结构较为复杂的问题. 深度学习简化了提取特征的这个步骤, 而对于一些结构化的数据, 统计学习依然能够, 或者胜任深度学习. 另一方面, 在KDD领域, 统计学习依然占有一席之地, 而KDD更偏重于使用机器学习算法解决实际的问题, 也更注重于特征工程. 最后, 对于统计学习和深度学习的讨论可以参见周志华老师机器学习序言的问题三. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:3","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"裴健当选SIGKDD主席, 研究被引超7万次，他还有一个遗憾 | 专访 数据挖掘的核心是对数据和业务的理解能力和对算法的构建能力。 是不是数据永远是越大越好？对于研究者来说，怎样才算是适合的数据？ 一般来说，数据是越多越好。 深度学习需要大量的数据来产生可以generalize的模型。 在实际应用中，数据往往是有成本的。 有很多应用场景不容易获取大量的高质量数据。 所以说我们需要针对具体问题，获取合适的数据。 在这方面，统计学对数据的采集评价有一系列的方法和原则，值得深入学习。 另一个方面，要很好利用大量的数据，通常需要比较复杂的模型，对计算的要求也相应地比较高，所以我们要根据数据量和应用来选择合适的模型。 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:4","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"数据挖掘最强华人齐聚加拿大，干货满满亮点不断 | KDD 2017 杨强教授提到, 特征比模型更为重要(Big Data is useless unless it can deliver big feature space). 对于Data Minming来讲, 实际也是这么回事, 但是也不能否认模型的重要性, 只不过大多数模型的算法已经成熟了, 而特征则更需要对业务的分析与理解, 所以, 特征才表现出了更加的重要. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:5","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"机器学习该怎么入门？ ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。 一同门一直在哔哔, 现在理解不深, 可能以后会加深理解. (边笑边逃) ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:6","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"Yann LeCun创造的新词“预测学习”将要落脚于GANs？ 如果你沉浸在传统机器学习领域, 又对深度学习了解很少, 那么你可以看看这一篇文章. 首先是Yann LeCun的\"蛋糕比喻\", 形象的描述了人工智能的三大领域: 强化学习, 监督学习和无监督学习. 其次是对于GANs(Generative Adversarail Networks)的描述. GANs是无监督学习的一个方法, 如LeCun所描述的那样: 对抗性网络是“20年来机器学习领域最酷的想法”, GANs的概念充满了想象力, 也让读者有一些引申的思考. 最后, 通过这篇文章也会增进你对深度学习的理解. 最后, 看看GANs生成的图片, 是不是很新奇呢? ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:7","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"清华大学刘知远：在 NLP 领域「做事」兼「发声」 如果你想了解一个人, 你就和他多接触一下吧, 听听他的故事. 文章通过两个项目, 以及为学生, 为老师三方面介绍了刘知远博士, 大神的练成并不简单, 更多的是努力和智谋. 顺便文章还介绍了一些NLP的项目, 值得收藏. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:8","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"KDD 2017最佳论文得主叶艳芳专访：AI时代的互联网安全 – 攻与防的黑白博弈 又一篇关于或学者或导师的访谈. 在研究方面, 机遇与挑战共存. 在导师方面, 站的够高, 看的更远. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:9","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"2017 NBA 选秀有哪些值得关注的新秀？ 走向成功的道路并不平坦, 但是他们胸怀勇气, 披荆斩棘, 坚持不懈, 最后得到了成功. 每个人都有着他们不同寻常的故事, 也造就了他们的成就! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:3:10","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"2017-12 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:4:0","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"很傻很天真的贝叶斯定理 贝叶斯定理是一种思维上的推理方式. 一般的推理是建立在因果关系上的, 对于执果索因, 就需要使用贝叶斯定理. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:4:1","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"F1 比赛中最严重的事故是哪次？ 一些事故让人感到沮丧, 但是, 他们以更坚强的方式活着! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:4:2","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"文本情感分类与深度学习模型 共三篇关于NLP的文章, 每次都是对上次内容的更正. 其中第三篇为一年后的更正, 实在难为可贵! http://kexue.fm/archives/3360/ http://spaces.ac.cn/archives/3414/ http://kexue.fm/archives/3863/ 详细的讲述了作者做情感分析的过程及想法, 内容详实, 对于刚上手NLP分类有很好的引导作用. 本以为是团体运维的博客, 但就是个人博客. 博客内容精良, 专研精神可贵, 涉猎广泛, 值得关注! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:4:3","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"2018-01 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:5:0","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"How to do machine learning efficiently 10秒原则 时间是最贵重的成本. 永远不允许计算超过10s的问题(抽样). 急于成功 更专注一个问题. 由简至繁的构建. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:5:1","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"2018-11 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:0","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"Ask Me Anything session with a Kaggle Grandmaster 作者是一位物理专业毕业的博士生, 后面转向数据科学(DS). 过程中也遇到很多困难, 坚持可热情, 还有总结沉淀下来的方法让他成为了Kaggle Grandmaster. 在这个过程中, 作者是付出了很多精力的. 在全职的工作上要保持比赛的强度, 是工作也比赛的权衡. 当然, 作者也会发费时间在运动, 旅游上面. 但是更多的闲暇时间用在了比赛和学习上. 当最后成为Kaggle Grandmaster时, 作者认为一切都是值得的. 里面有很多比赛方法上的经验, 在打比赛之前和之后都可以看看! 千里之行始于足下, 最困难的一步也就是第一步. 来自于爱可可老师的推荐 “你在Kaggle学到的技能只是你在工业或学术界工作时所需技能的一小部分，那些Kaggle不涉及的技术领域基础教育可能是至关重要的” “要加速磁盘jpeg图像I/O，不应该用PIL，Skimage甚至OpenCV，而是用libjpeg-turbo或PyVips。” “Kaggle技能是我从学术界和其他知识来源获得的一系列技能的有力补充” “高估你的专业、大学等在该行业找工作中的影响是不明智的。一家公司雇用你、愿意付钱给你是希望解决他们面临的问题。学位和专业只是评估能力的参考……” “机器学习领域的论文、竞赛、博客和书籍实在太多，根本看不过来。实际上，当我遇到问题，会专注于查看最近结果并深入研究。完成后，再切换到下个问题，只是掌握缺少实践经验领域的高级知识……NIPS、CVPR等会议，可以很好地代表在目前研究阶段我们能做什么和不能做什么” 附上机器之心翻译[link]版本. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:1","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"从「深度学习」到「深度」学习 | 龙明盛老师专访 近期读了龙明盛老师的几篇论文, 感觉其是一位很nice的年轻老师, 学者. 偶然在网上看到这篇专访, 讲述了龙明盛老师的为师者, 为学者, 为人者的态度和践行. 一位优秀的往往能启发更优秀的学生, 值得学习! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:2","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 吴海波 花名吾加，蘑菇街搜索、推荐排序算法owner，大规模机器学习从业者. 醍醐灌顶! 前辈走出的路, 总结的经验一定要好好的看, 尤其是如此优秀的文章! 首先文章以算法工程师面试最为引子, 算法工程师首先需要能解决工程问题, 因此代码能力是必不可少的, 要求也是严格的, 其次是机器学习理论和实践上的能力. 同时具有上面两个能力才能称作是一个合格的算法工程师. 但是作者并没有局限在于算法工程师上面, 而是在于职业的操守. 理论 + 实践, KPI, ROI, 这些才是算法工程师要面对的问题. 而为了有产出, 必须要对算法进行一些探索, 工程化. 最后是作者也是深耕于搜索, 推荐排序的算法工程师, 而且乐于分享自己的想法, 推动行业的前进, 值得关注! 作者关于此问题写了两篇 其一: 论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 其二: 算法工程师又不只是工程师 另外还有作者还引用了多篇质量非常高的关于推荐排序的博文和专栏, 包含了杨旭东的几篇文章, 之前也有看过. 另外还引用了很多领域的论文. 最后, 附上吴海波的专栏误入机器学习的码农, 以及杨旭东的专栏算法工程师的自我修养. ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:3","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"爬虫租房, 是一个不错的注意 另外还有一个开源的代码 https://github.com/lorien/awesome-web-scraping/blob/master/python.md 又发现了一个 https://github.com/XuefengHuang/lianjia-scrawler ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:4","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"一份写给NLP研究者的编程指南 开发经验非常的重要! 快速原型开发!!!!!!!!!然后再重构与模块化!!!!!!!!!! 写安全的工程代码 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:5","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"贫富差距是如何逐渐扩大的 总结一下: 先投资自己 出发点是基于长期的回报而不是即时的满足 多种收入来源 不断的投资自己 从不指责, 对自己的失败承担全部责任 不睡懒觉 清晰的愿景和目标 进步后娱乐 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:6","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"学习Git, 这里有个非常耐撕的工具 demo hands on ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:7","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"善用资源，找书，看书，而不是囤书 依然是那句话, 你拥有的资源多少并不重要, 如果你不知道利用的话, 一切都是无用的. 深度强化学习综述: https://weibo.com/1402400261/H4rJvt4Fk 机器学习在线课程: https://weibo.com/1402400261/H4rH3EhXb 机器学习相关学习资料: https://github.com/iamalotaibi/Machine-Learning ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:8","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"我的八年博士生涯 – 王赟 Maigo 学术篇 娱乐篇 在知乎上, 感觉王赟是一个非常平易近人的人, 他通过两个方面总结了他的博士生涯, 看上去也非常的评议近人. 在学术上, 做的研究和项目好像也没有很大的高大上, 但是却踏踏实实, 虽然走了很多弯路, 但是也有一些不错的收获. 在完成博士论文可谓惊险, 正是这种惊险也使得博士生涯更具有一番味道. 在娱乐上, 可能是比较玩得开的人, 也愿意玩的人, 让整个博士生涯变得不那么的单调, 很好! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:9","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"斯坦福机器学习课程“CS 229 - Machine Learning”速查表(中文版) 全面, 简洁, 整理得很好! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:10","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"图文详解如何利用Git+Github进行团队协作开发 Git在版本控制中是非常重要的工具, 这篇文章把多分支开发说的很详细! ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:11","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"张小龙总结微信8年 https://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==\u0026mid=2691354540\u0026idx=2\u0026sn=fa42b5fea385f75a0afe2ad06ca92f08 http://wx2.sinaimg.cn/large/6106a4f0ly1fz1cqvtg3xj21bf2s3n3o.jpg ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:12","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"高效学习 https://weibo.com/1707613190/HbgL0n5E0 ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:13","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["column"],"content":"矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/516385585 这是一个系列视频, 指的get [TOC] ","date":"2016-05-05","objectID":"/columns/reading-note-2019.html:6:14","tags":["column"],"title":"阅读笔记2019","uri":"/columns/reading-note-2019.html"},{"categories":["随笔"],"content":"很多时候发现自己过于浮躁，一些简单的事都不能好好的完成。总是一会看看这里，一会又做做那里，最后什么也没有做。或者是整天都循环的、重复的看一些无用的内容，没有认真的去思考，也没有深入的去理解。这个时候，我想自己更应该静下心来，好好做一个规划，然后认真的实行。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:0:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"学会工作 工作应该全身心的去投入，不要让其他的事情干扰你。 坚持去克服存在的问题，不要逃避，勇敢面对。 工作应总结经验，学会使工作更有效率。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:1:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"学会生活 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"阅读 坚持阅读，阅读时间坚持1.5h以上。 阅读来源主要是简书等上的优质文章和其他技术性、科技资讯等文章。 读文章一定要仔细品读，认真思考，切勿囫囵吞枣。 积极参与评论，做好阅读笔记。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:1","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"写作 发表自己的看法和观点，积极参与其中。 收集一些自己认为好的文章，加以评论。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:2","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"学习 制定好学习计划，循序渐进，阶段性学习。 做好学习笔记。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:3","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"能力培养 积极尝试新的事物，迎接新的挑战。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:4","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"运动 身体是革命的本钱！ ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:2:5","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":["随笔"],"content":"最后几点忠告 善于用脑。 计划和规划。 平心静气，不骄不躁。 相信你会明白，工作是一种状态，学习是另一种状态。 ","date":"2016-04-03","objectID":"/posts/old/essay/work-and-another.html:3:0","tags":["生活"],"title":"学会工作，学会生活","uri":"/posts/old/essay/work-and-another.html"},{"categories":null,"content":"摄影 https://www.bilibili.com/video/BV1t24y1k7Ye https://www.bilibili.com/video/BV1Qu411p7Jj https://www.bilibili.com/video/BV16j411z7MF 虽然别人家的女朋友很漂亮, 但是在人从众中拍人像, 还是很有启发的 https://www.bilibili.com/video/BV19W4y1o7bh 旅游, 芒市, 梁河 ","date":"0001-01-01","objectID":"/columns/funny.html:1:0","tags":null,"title":"","uri":"/columns/funny.html"}]